# ğŸ¤– Ejercicios PrÃ¡cticos con IA - Writing Tools for AI Agents

Este documento contiene ejercicios diseÃ±ados para practicar el diseÃ±o de tools **usando Claude como asistente**. El objetivo es aprender a iterar con IA para mejorar tus tools.

---

## ğŸ“‹ MetodologÃ­a: TDD con IA

**Test-Driven Design con IA**:
1. **Describe** el tool que necesitas (prompt a Claude)
2. **Claude genera** el cÃ³digo inicial
3. **TÃº auditas** usando la checklist de best practices
4. **Claude refactoriza** basÃ¡ndose en tu feedback
5. **TÃº validas** con tests

---

## Ejercicio 1: DiseÃ±ar un Tool de Inventario (45 min)

### Parte 1: GeneraciÃ³n con Claude (15 min)

**Tu rol**: Product Owner que define requisitos

**Prompt inicial para Claude**:

```markdown
Necesito diseÃ±ar un tool para un agente IA que gestiona un sistema de inventario.

**Requisitos funcionales**:
1. Buscar productos por nombre, categorÃ­a o SKU
2. Retornar stock disponible
3. Alertar si stock estÃ¡ bajo (< 10 unidades)
4. Sugerir productos relacionados si el buscado no tiene stock

**Requisitos tÃ©cnicos**:
- Nombre del tool claro y descriptivo
- Description completa siguiendo best practices de Anthropic
- Schema de input con Pydantic (validaciÃ³n estricta)
- Schema de output bien estructurado
- Casos de error con mensajes accionables
- 3 ejemplos de uso
- ParÃ¡metro response_format (detailed/concise)

DiseÃ±a el tool en Python con type hints completos.
```

**AcciÃ³n**: Copia la respuesta de Claude a un archivo `inventory_tool.py`

### Parte 2: AuditorÃ­a Manual (15 min)

**Tu rol**: Security & Quality Reviewer

**Checklist de auditorÃ­a** (mÃ¡rcalas mientras revisas):

```markdown
## Nombre del Tool
- [ ] Â¿Es descriptivo y sin ambigÃ¼edad?
- [ ] Â¿Usa verbos de acciÃ³n (search, get, create)?
- [ ] Â¿No overlappea con otros tools potenciales?

## Description
- [ ] Â¿Explica cuÃ¡ndo usar el tool?
- [ ] Â¿Explica cuÃ¡ndo NO usarlo?
- [ ] Â¿Incluye ejemplos de uso concretos?
- [ ] Â¿Documenta relaciones con otros tools?
- [ ] Â¿Menciona consideraciones de performance?

## Input Schema
- [ ] Â¿Usa Pydantic para validaciÃ³n?
- [ ] Â¿ParÃ¡metros tienen nombres especÃ­ficos (no ambiguos)?
- [ ] Â¿Incluye valores default razonables?
- [ ] Â¿Rangos de valores estÃ¡n validados?
- [ ] Â¿Hay field_validators para lÃ³gica custom?

## Output Schema
- [ ] Â¿Retorna solo informaciÃ³n relevante?
- [ ] Â¿Usa identificadores semÃ¡nticos (no UUIDs)?
- [ ] Â¿Formato es parseable (JSON, Markdown)?
- [ ] Â¿Incluye metadata Ãºtil (count, timestamp)?

## Error Handling
- [ ] Â¿Usa Result type (Success/Error)?
- [ ] Â¿Errores son accionables (dicen cÃ³mo corregir)?
- [ ] Â¿Evita stack traces tÃ©cnicos?
- [ ] Â¿Sugiere alternativas cuando falla?
- [ ] Â¿Tiene error_type categorizado?

## Security
- [ ] Â¿Valida TODOS los inputs?
- [ ] Â¿Previene injection attacks?
- [ ] Â¿No expone secrets en outputs?
- [ ] Â¿Limita tamaÃ±o de respuestas?
- [ ] Â¿Tiene rate limiting (si aplica)?
```

**Documenta** los âŒ que encontraste:

```markdown
## Issues Encontrados

1. [CrÃ­tico/Alto/Medio/Bajo] DescripciÃ³n del issue
   - **Problema**: [QuÃ© estÃ¡ mal]
   - **Impacto**: [Por quÃ© es un problema]
   - **Fix recomendado**: [CÃ³mo corregirlo]

2. ...
```

### Parte 3: IteraciÃ³n con Claude (15 min)

**Tu rol**: Tech Lead que da feedback constructivo

**Prompt de mejora para Claude**:

```markdown
AuditÃ© el tool que diseÃ±aste. AquÃ­ estÃ¡ el feedback:

## Issues CrÃ­ticos
[Pega tu lista de issues crÃ­ticos]

## Issues de Alta Prioridad
[Pega tu lista de issues altos]

## Mejoras Sugeridas
[Pega tu lista de mejoras]

**Refactoriza el tool para**:
1. Corregir todos los issues crÃ­ticos y altos
2. Mejorar la description para ser mÃ¡s clara sobre cuÃ¡ndo NO usar el tool
3. AÃ±adir el parÃ¡metro `response_format` (detailed/concise)
4. Implementar rate limiting con decorator
5. AÃ±adir ejemplos de error handling en los docstrings

Muestra el cÃ³digo refactorizado completo.
```

**Valida** que Claude corrigiÃ³ los issues. Si no, itera de nuevo con prompts mÃ¡s especÃ­ficos.

---

## Ejercicio 2: Auditar Tool Mal DiseÃ±ado (30 min)

### Escenario

Tu compaÃ±ero de equipo (Claude ğŸ˜‰) diseÃ±Ã³ un tool, pero los agentes no lo usan correctamente. Tu misiÃ³n: identificar quÃ© estÃ¡ mal.

**Prompt para Claude**:

```markdown
ActÃºa como un desarrollador junior que NO conoce las best practices de Anthropic.

DiseÃ±a un tool llamado `get_data` que:
- Accede a una base de datos PostgreSQL
- Busca registros de usuarios
- Retorna informaciÃ³n del usuario

Hazlo **deliberadamente mal** siguiendo estos antipatrones:
1. Nombre ambiguo
2. Description vaga
3. Sin validaciÃ³n de inputs
4. Expone informaciÃ³n sensible (passwords)
5. Usa UUIDs en lugar de identificadores semÃ¡nticos
6. Errores crÃ­pticos sin sugerencias
7. SQL injection posible

Dame solo el cÃ³digo (sin explicaciones), para que practique auditarlo.
```

### Tu Tarea

1. **Lee el cÃ³digo** que Claude generÃ³
2. **Identifica 7+ antipatrones** usando la checklist
3. **Documenta cada issue** con:
   - Severidad (Critical/High/Medium/Low)
   - LÃ­nea de cÃ³digo
   - Anti-patrÃ³n especÃ­fico
   - Impacto en el agente
   - Fix recomendado

4. **Pide a Claude que lo refactorice**:

```markdown
IdentifiquÃ© estos antipatrones en tu cÃ³digo:

[Tu anÃ¡lisis completo]

Refactoriza el tool siguiendo TODAS las best practices de Anthropic.
El nuevo tool debe:
- Tener un nombre descriptivo
- Description completa con ejemplos
- ValidaciÃ³n Pydantic estricta
- Prevenir SQL injection (prepared statements)
- No exponer secrets
- Errores accionables
- Usar email en lugar de UUID para buscar usuarios

Muestra el cÃ³digo refactorizado.
```

5. **Compara** el cÃ³digo before/after. Â¿QuÃ© tan diferente es?

---

## Ejercicio 3: Tool Composition (45 min)

### Objetivo

DiseÃ±ar un tool de alto nivel que orquesta mÃºltiples tools de bajo nivel.

**Prompt para Claude (Parte 1)**:

```markdown
DiseÃ±a 3 tools bÃ¡sicos para un sistema de anÃ¡lisis de API:

1. `search_logs` - Busca en logs del servidor
   - Input: query (regex), time_range (Ãºltimas N horas)
   - Output: LÃ­neas de logs relevantes

2. `calculate_percentiles` - Calcula percentiles de response times
   - Input: response_times (list[float])
   - Output: p50, p95, p99

3. `get_error_rate` - Calcula tasa de errores
   - Input: logs (list[str])
   - Output: error_rate (float), common_errors (list[str])

Sigue best practices. Usa Pydantic.
```

**Prompt para Claude (Parte 2)**:

```markdown
Ahora diseÃ±a un tool de ALTO NIVEL llamado `analyze_api_endpoint` que:

**Funcionalidad**:
1. Internamente llama a los 3 tools que diseÃ±aste antes
2. Busca logs del endpoint especificado
3. Extrae response times de los logs
4. Calcula percentiles (p50, p95, p99)
5. Calcula error rate
6. Genera recomendaciÃ³n basada en las mÃ©tricas

**Input**:
- endpoint: str (e.g., "/api/tasks")
- time_window_hours: int (default: 24)

**Output consolidado**:
{
    "endpoint": "/api/tasks",
    "requests_analyzed": 1000,
    "avg_response_time_ms": 45.3,
    "p95_response_time_ms": 120.5,
    "p99_response_time_ms": 250.8,
    "error_rate_percent": 0.5,
    "common_errors": ["Timeout", "ValidationError"],
    "recommendation": "Consider adding caching"
}

Implementa la composiciÃ³n de tools. Sigue best practices.
```

### Tu Tarea

1. **Revisa** los 4 tools generados por Claude
2. **Verifica** que `analyze_api_endpoint`:
   - Llama a los 3 tools internamente
   - Maneja errores de cada tool
   - Consolida resultados de forma Ãºtil
   - Retorna recomendaciones accionables

3. **PregÃºntate**:
   - Â¿El agente prefiere usar 1 tool consolidado o 4 tools separados?
   - Â¿En quÃ© casos es mejor consolidar?
   - Â¿En quÃ© casos es mejor separar?

4. **Documenta** tu anÃ¡lisis:

```markdown
## AnÃ¡lisis: ConsolidaciÃ³n vs SeparaciÃ³n

### Ventajas de Consolidar
1. ...
2. ...

### Desventajas de Consolidar
1. ...
2. ...

### ConclusiÃ³n
Para este caso especÃ­fico, [consolidar/separar] es mejor porque...
```

---

## Ejercicio 4: Response Format Optimization (30 min)

### Objetivo

Optimizar tokens comparando respuestas `concise` vs `detailed`.

**Prompt para Claude**:

```markdown
DiseÃ±a un tool `search_slack_messages` con:

**Input**:
- query: str
- channel: str (opcional)
- limit: int (default: 10)
- response_format: "concise" | "detailed"

**Output (concise)**:
Solo informaciÃ³n mÃ­nima: text, author, timestamp

**Output (detailed)**:
InformaciÃ³n completa: text, author, timestamp, thread_ts, channel_id, user_id, reactions, attachments

Implementa ambos formatos. Usa Pydantic.
```

### Tu Tarea

1. **Genera** 2 respuestas ejemplo:
   - Una respuesta `concise` con 10 mensajes
   - Una respuesta `detailed` con 10 mensajes

2. **Cuenta tokens** de cada respuesta:
   - Usa Claude: "Â¿CuÃ¡ntos tokens tiene esta respuesta?"
   - Compara: Â¿CuÃ¡l consume mÃ¡s tokens?

3. **Analiza casos de uso**:

```markdown
## CuÃ¡ndo usar Concise
- [Caso 1]
- [Caso 2]

## CuÃ¡ndo usar Detailed
- [Caso 1]
- [Caso 2]

## Ahorro de Tokens
- Concise: X tokens
- Detailed: Y tokens
- Ahorro: (Y-X)/Y * 100 = Z%
```

4. **Pregunta a Claude**:

```markdown
BasÃ¡ndome en el anÃ¡lisis de tokens, Â¿cuÃ¡l deberÃ­a ser el default para `response_format`?

Contexto:
- Concise: {X} tokens
- Detailed: {Y} tokens
- El agente tÃ­picamente necesita hacer llamadas downstream en 30% de los casos

Â¿QuÃ© recomiendas y por quÃ©?
```

---

## Ejercicio 5: Debugging Tool Issues (60 min)

### Escenario Real

Un agente NO estÃ¡ usando tu tool correctamente. Tienes estos logs:

```
[LOG] Tools available: ['search_code', 'read_file', 'execute_command']
[LOG] User: "Busca la funciÃ³n process_payment en el cÃ³digo"
[LOG] Agent: "No encuentro esa informaciÃ³n. Â¿Puedes darme mÃ¡s contexto?"
[LOG] Agent did NOT call: search_code
```

### Parte 1: DiagnÃ³stico con Claude

**Prompt**:

```markdown
Tengo un problema: Mi agente NO estÃ¡ usando el tool `search_code` cuando deberÃ­a.

**Tool actual**:
```python
def search_code(query: str) -> list:
    """Searches code."""
    # ... implementation
```

**Logs del agente**:
[Pega los logs de arriba]

**Pregunta**: Â¿Por quÃ© el agente no usÃ³ el tool? Â¿QuÃ© estÃ¡ mal en el diseÃ±o?

Analiza:
1. Nombre del tool
2. Description
3. Schema de input
4. Casos donde deberÃ­a vs no deberÃ­a usarse
```

### Parte 2: Fix Iterativo

1. **Claude propone** un fix
2. **TÃº lo implementas**
3. **Simulas** llamar al agente de nuevo (prompt):

```markdown
Imagina que eres Claude (el agente) y tienes estos tools disponibles:

[Tool refactorizado]

El usuario dice: "Busca la funciÃ³n process_payment en el cÃ³digo"

Â¿QuÃ© tool usarÃ­as? Â¿Por quÃ©?
```

4. **Itera** hasta que Claude (actuando como agente) diga que usarÃ­a el tool correctamente

---

## Ejercicio 6: Security Audit (30 min)

### Objetivo

Practicar security review de tools usando Claude como auditor.

**Prompt para Claude**:

```markdown
ActÃºa como un Security Engineer experto.

Audita este tool para vulnerabilidades de seguridad:

```python
def execute_command(cmd: str) -> str:
    """Runs a command."""
    import subprocess
    result = subprocess.run(cmd, shell=True, capture_output=True)
    return result.stdout.decode()
```

Identifica:
1. Vulnerabilidades crÃ­ticas (con ejemplos de exploit)
2. Vulnerabilidades altas
3. Vulnerabilidades medias
4. Mejoras de seguridad recomendadas

Para cada issue:
- Severity (Critical/High/Medium/Low)
- OWASP Category (si aplica)
- Exploit ejemplo
- Fix recomendado con cÃ³digo
```

### Tu Tarea

1. **Lee** el anÃ¡lisis de Claude
2. **Implementa** los fixes recomendados
3. **Pide re-audit**:

```markdown
ImplementÃ© tus recomendaciones. Audita esta nueva versiÃ³n:

```python
[Tu cÃ³digo refactorizado]
```

Â¿Quedan vulnerabilidades? Â¿Es seguro para producciÃ³n?
```

4. **Itera** hasta que Claude diga "Aprobado para producciÃ³n"

---

## Ejercicio 7: Real-World Tool (90 min)

### Proyecto Final

DiseÃ±a un tool REAL que necesitas en tu trabajo diario.

**Workflow con Claude**:

1. **Define requisitos** (15 min):

```markdown
Necesito un tool para [tu caso de uso real].

**Contexto**:
- [Describe tu workflow actual]
- [QuÃ© hace manualmente un humano]
- [QuÃ© deberÃ­a hacer el agente]

**Requisitos funcionales**:
1. ...
2. ...

**Requisitos no funcionales**:
- Performance: [respuesta < X segundos]
- Security: [quÃ© datos sensibles maneja]
- Reliability: [quÃ© pasa si falla]
```

2. **Claude diseÃ±a** (20 min)
3. **TÃº auditas** (20 min) - Usa checklist completa
4. **Claude refactoriza** (15 min)
5. **TÃº implementas tests** (20 min)

**Prompt para tests**:

```markdown
Genera tests unitarios para este tool usando pytest.

Incluye:
1. Test happy path
2. Test de cada caso de error
3. Test de validaciÃ³n de inputs
4. Property-based test con Hypothesis
5. Mock de dependencias externas

Coverage objetivo: 90%+
```

---

## ğŸ“Š Rubrica de EvaluaciÃ³n

| Criterio | Peso | EvaluaciÃ³n |
|----------|------|------------|
| **Tool Design** | 30% | Â¿Sigue best practices de Anthropic? |
| **Security** | 25% | Â¿Previene vulnerabilidades comunes? |
| **Error Handling** | 20% | Â¿Errores son accionables? |
| **Testing** | 15% | Â¿Coverage >80%? Â¿Tests significativos? |
| **Documentation** | 10% | Â¿Description completa con ejemplos? |

---

## ğŸ“ ReflexiÃ³n Final

DespuÃ©s de completar los ejercicios, responde:

1. **Â¿QuÃ© diferencia clave notaste entre tools mal diseÃ±ados y bien diseÃ±ados?**

2. **Â¿CuÃ¡l fue el antipatrÃ³n mÃ¡s comÃºn que encontraste?**

3. **Â¿CÃ³mo cambiÃ³ tu forma de diseÃ±ar tools despuÃ©s de estos ejercicios?**

4. **Â¿En quÃ© casos consolidarÃ­as tools vs mantenerlos separados?**

5. **Â¿QuÃ© checklist mental usarÃ¡s en el futuro al diseÃ±ar tools?**

---

## ğŸ“š Recursos Adicionales

- [Anthropic Tool Use Guide](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)
- [Pydantic Documentation](https://docs.pydantic.dev/)
- [OWASP Top 10](https://owasp.org/www-project-top-ten/)

---

**Nota**: Estos ejercicios estÃ¡n diseÃ±ados para usar Claude como **asistente de aprendizaje**. El objetivo NO es que Claude haga todo el trabajo, sino que **tÃº aprendas a iterar con IA** para diseÃ±ar mejores tools.
