# Clase 4 â€“ Testing ampliado y primeros principios SOLID

### Escenario inicial

Ya tienes un CLI que:

- Agrega tareas.
- Lista tareas.
- Marca tareas como completadas.
- Tiene **tests unitarios bÃ¡sicos** (agregar, completar, listar).

Hasta aquÃ­, bien.

Pero en el mundo real, la historia no se detiene. Te piden una **nueva feature**: *â€œnecesitamos prioridades en las tareas (alta, media, baja) y poder filtrarlasâ€*.

Tu reacciÃ³n natural como dev novato podrÃ­a ser:

*â€œBueno, meto un nuevo campo `prioridad` en el diccionario y apaÃ±o las funciones para que lo imprimanâ€*.

### El problema

Si haces eso sin cuidado:

- Cambias la estructura de datos â†’ **rompes funciones** sin darte cuenta.
- Los tests que escribiste antes quizÃ¡s fallen, o peor: no fallen aunque el programa ya tenga inconsistencias.
- El archivo empieza a tener funciones que hacen demasiadas cosas (olor a spaghetti).

AquÃ­ es donde se juntan dos ideas:

1. **Tests mÃ¡s fuertes** â†’ te avisan de quÃ© rompiste al tocar la estructura.
2. **Principio de responsabilidad Ãºnica (SRP de SOLID)** â†’ cada funciÃ³n o mÃ³dulo debe tener un motivo Ãºnico para cambiar.

---

ğŸ‘‰ Primer mini-ejercicio:

Mira tu funciÃ³n `agregar_tarea` actual. Si ahora necesita guardar tambiÃ©n `prioridad`, Â¿quÃ© cambiarÃ­as exactamente en esa funciÃ³n y en sus tests?

(Haz un apunte mental o anÃ³talo en tu repo. No lo codifiques aÃºn. Queremos ver dÃ³nde empieza a doler antes de sacar la â€œcuraâ€).

Â¿Se te ha ocurrido esto?

En la clase:

```
# ================================
# 1.1. TIPOS DE DATOS
# ================================
class Tarea(TypedDict):
    id: int
    nombre: str
    completada: bool
    prioridad: str 
```

En agregar tarea

```sql
def agregar_tarea(ruta: str, nombre: str, prioridad: str) -> Tarea:
    """Crea una nueva tarea no completada y la persiste."""

    tareas = cargar_tareas(ruta)
    tarea: Tarea = {"id": nuevo_id(tareas), "nombre": nombre, "completada": False, "prioridad": prioridad}
    tareas.append(tarea)
    guardar_tareas(ruta, tareas)
    return tarea  # devolver la entidad ayuda en tests 
```

Exacto, ese es el movimiento natural: aÃ±adir un campo nuevo y extender la funciÃ³n.

Lo interesante es que al hacerloâ€¦ ya has creado **un punto de dolor**:

1. Todos los lugares donde se llama a `agregar_tarea` ahora **rompen**, porque piden solo `(ruta, nombre)` y tÃº exiges `(ruta, nombre, prioridad)`.
2. Tus **tests anteriores** tampoco compilan ni pasan, porque no saben quÃ© es `prioridad`.
3. Si alguien se olvida de pasar la prioridad, se lÃ­a.

Y aquÃ­ entra la magia de lo que estamos aprendiendo hoy:

- **Los tests te sirven de alarma**: si tu cambio rompe algo, lo detectas antes de usar el programa.
- **SOLID te da brÃºjula**: â€œcada funciÃ³n deberÃ­a tener un Ãºnico motivo para cambiarâ€. Tu funciÃ³n `agregar_tarea` estÃ¡ asumiendo varias responsabilidades (crear estructura, decidir ID, meter la prioridad, guardar en disco).

---

Lo que vamos a hacer ahora es **reforzar un test existente** para incluir la prioridad, antes incluso de tocar mÃ¡s funciones. AsÃ­ practicamos cÃ³mo los tests te dan red de seguridad.

Por ejemplo, tu test de agregar podrÃ­a ampliarse asÃ­:

```python
def test_agregar_tarea_con_prioridad(self):
    t = agregar_tarea(self.tmp, "Estudiar IA", "alta")
    tareas = cargar_tareas(self.tmp)
    self.assertEqual(len(tareas), 1)
    self.assertEqual(tareas[0]["nombre"], "Estudiar IA")
    self.assertEqual(tareas[0]["prioridad"], "alta")
    self.assertFalse(tareas[0]["completada"])

```

Si corres este test ahora, Â¿quÃ© pasarÃ¡?

- Si ya cambiaste la funciÃ³n como escribiste, pasarÃ¡ âœ….
- Si alguien sigue usando la versiÃ³n vieja de `agregar_tarea`, fallarÃ¡ âŒ.

Â¿Que sucedio? Â¿Te ha ocurrido este error?

```sql
======================================================================
ERROR: test_completar_tarea (__main__.TestTareas.test_completar_tarea)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "e:\master-ia-manu\Modulo 1 â€“ Fundamentos Dev y pensamiento\Clase 4 â€“ Testing ampliado y primeros principios SOLID\cli-tareas\test_tareas.py", line 41, in test_completar_tarea
    tarea = agregar_tarea(self.tmp, "Repasar Git")
TypeError: agregar_tarea() missing 1 required positional argument: 'prioridad'

======================================================================
ERROR: test_listar_con_tareas (__main__.TestTareas.test_listar_con_tareas)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "e:\master-ia-manu\Modulo 1 â€“ Fundamentos Dev y pensamiento\Clase 4 â€“ Testing ampliado y primeros principios SOLID\cli-tareas\test_tareas.py", line 63, in test_listar_con_tareas
    agregar_tarea(self.tmp, "Estudiar IA")
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: agregar_tarea() missing 1 required positional argument: 'prioridad'

----------------------------------------------------------------------
Ran 5 tests in 0.009s

FAILED (errors=2)
```

Perfecto: tus tests han fallado **como deben**. Has cambiado la firma de `agregar_tarea` y los tests antiguos no pasan la nueva `prioridad`. Antes de ponernos finos con SOLID, vamos a **desbloquear la suite** sin romper a nadie.

## Paso 1 â€” Cambio no rompedor (retrocompatibilidad)

Haz que `agregar_tarea` acepte una prioridad **por defecto**. AsÃ­ los tests antiguos (que no pasan prioridad) vuelven a funcionar y podemos migrarlos con calma.

```python
from typing import Literal

PRIORIDADES = ("alta", "media", "baja")

def _normaliza_prioridad(p: str) -> str:
    p = (p or "").lower()
    return p if p in PRIORIDADES else "media"

def agregar_tarea(ruta_archivo: str, nombre_tarea: str, prioridad: str = "media") -> dict:
    """Crea una tarea (por defecto prioridad='media'), la guarda y la devuelve."""
    if prioridad not in PRIORIDADES_VALIDAS:
        prioridad = "media"

    lista_tareas = cargar_tareas(ruta_archivo)

    nueva_tarea = {
        "id": nuevo_id(lista_tareas),
        "nombre": nombre_tarea,
        "completada": False,
        "prioridad": prioridad,
    }

    lista_tareas.append(nueva_tarea)
    guardar_tareas(ruta_archivo, lista_tareas)
    return nueva_tarea

```

Antes de cambiar nada, he colado un detallito que seguro que habras visto en los codigos que te ha dado la IA.

una variable llamada p.

Quizas, cuando lo escribe la IA o lo escribes tu, lo entiendes del tirÃ³n. Peroâ€¦ Â¿QuÃ© ocurrirÃ¡ dentro de 6 meses? Â¿Y si le das este codigo a otro desarrollador?

Va a perder horas trazando el codigo para descubrir que querias decir con â€œpâ€

Esto es algo que la IA hace con muchisima frecuencia. Y si te dedicas a copiar y pegar, luego no entenderÃ¡s nada de lo que estÃ¡ haciendo.

Un dev humano que lea `t["prioridad"]` o una variable llamada `t` se queda pensando: â€œÂ¿t de quÃ©? Â¿trabajo? Â¿task? Â¿taza de cafÃ©?â€

Esto conecta directamente con **Clean Code** y con el primer principio de **SOLID (Single Responsibility Principle)**: cada parte del cÃ³digo deberÃ­a contar su historia de forma clara y tener un Ãºnico motivo para cambiar. Si encima usas nombres crÃ­pticos, la historia se convierte en jeroglÃ­fico.

la forma correcta serÃ­a:

```sql
PRIORIDADES = ("alta", "media", "baja")

def _normaliza_prioridad(prioridad: str) -> str:
    """Devuelve la prioridad normalizada o 'media' si es invÃ¡lida."""
    prioridad_normalizada = (prioridad or "").lower()
    return prioridad_normalizada if prioridad_normalizada in PRIORIDADES else "media"
```

Siempre hay que preocuparse de dejar cÃ³digo legible.

## Paso 2 â€” Backfill suave al leer (por si hay JSON viejo)

Si ya tienes `tareas.json` (porque has ejecutado el programa) sin el campo, aÃ±adimos la prioridad al vuelo al cargar para que nada explote.

```python
def cargar_tareas(ruta_archivo: str) -> list[dict]:
    if not os.path.exists(ruta_archivo):
        return []

    try:
        with open(ruta_archivo, "r", encoding="utf-8") as fichero:
            contenido = fichero.read().strip()
            lista_tareas = json.loads(contenido) if contenido else []
    except json.JSONDecodeError:
        return []

    # Aseguramos que cada tarea tenga 'prioridad'
    for tarea in lista_tareas:
        if "prioridad" not in tarea or tarea["prioridad"] not in PRIORIDADES_VALIDAS:
            tarea["prioridad"] = "media"

    return lista_tareas

```

Con esto, tus dos tests que llamaban `agregar_tarea(self.tmp, "â€¦")` **dejan de petar** porque la prioridad tiene valor por defecto.

## Paso 3 â€” AÃ±ade 1 test nuevo (cubrir la nueva feature)

Ahora sÃ­, sumamos una prueba mÃ­nima para la prioridad (sin reescribir todo tu suite):

```python
def test_agregar_tarea_prioridad_por_defecto(self):
    nueva = agregar_tarea(self.tmp, "Repasar SOLID")  # sin prioridad
    self.assertEqual(nueva["prioridad"], "media")

def test_agregar_tarea_con_prioridad_alta(self):
    nueva = agregar_tarea(self.tmp, "Estudiar IA", "alta")
    self.assertEqual(nueva["prioridad"], "alta")
```

## Mini check de diseÃ±o (SRP en miniatura)

- **Antes** `agregar_tarea` rompÃ­a llamadas antiguas â†’ mÃºltiples motivos de cambio a la vez.
- **Ahora** aislamos el cambio con un **valor por defecto** + **normalizaciÃ³n** + **backfill en carga**. Una cosa a la vez; tests verdes; cabeza frÃ­a.

## QuÃ© sigue

Cuando confirmes que **tu suite estÃ¡ en verde**, atacamos **filtrar por prioridad** en `listar_tareas` con el mismo estilo:

1. escribimos **un test** sÃºper claro,
2. implementamos **lo mÃ­nimo** para pasarlo,
3. nombres largos y cero magia.

Dime cuando tengas los tests verdes y seguimos con el filtro.

## Antes de continuar

Â¿Te has dado cuenta de que hemos hecho una funciÃ³n que empieza por _? Â¿Es magia?

En Python, cuando ves una funciÃ³n o variable con **guion bajo delante** (`_normaliza_prioridad`, `_variable`), significa:

- **ConvenciÃ³n.**
    
    No cambia cÃ³mo funciona Python. Es simplemente una forma de decir: *â€œesto es interno, no lo llames desde fuera del mÃ³duloâ€*.
    
- Ejemplo en castellano:
    - `agregar_tarea()` serÃ­a la â€œpuerta principalâ€ del restaurante: la funciÃ³n que cualquier cliente puede usar.
    - `_normaliza_prioridad()` serÃ­a la â€œcocina internaâ€: existe para ayudar, pero no estÃ¡ pensada para que el cliente meta mano ahÃ­.
- El guion bajo es un **aviso para humanos y para linters** (programas que revisan tu estilo de cÃ³digo):
    
    *â€œEsto es privado. Si lo usas fuera, bajo tu responsabilidadâ€*.
    

En tu caso, quizÃ¡s **yo metÃ­ `_normaliza_prioridad()` demasiado pronto**, y te di un dolor de cabeza extra cuando todavÃ­a estÃ¡s asentando cosas.

Vamos a dejarlo asÃ­ para evitar complicarlo, simplemente debes entender que:

- Si empieza por _ , es una funcion interna (una funcion que ayuda a otra funciÃ³n que si puede ser llamada desde fuera.

Â¿Esto provoca que no puedas llamarla desde fuera?

NO, tan solo es una forma de escribirlo para que los humanos y programas automaticos lo entiendan. Escribir con buenas convenciones ayudarÃ¡ tambien a la IA a entender.

## Â¿seguimos?

Hasta ahora en la clase hemos hecho:

- **Problema real**: â€œtu CLI empieza a crecer, el jefe pide prioridad en las tareasâ€.
- **Primer choque**: al aÃ±adir `prioridad` rompimos los tests â†’ vimos cÃ³mo los tests son tu red de seguridad.
- **SoluciÃ³n didÃ¡ctica**: aÃ±adimos `prioridad` con valor por defecto, explicamos la convenciÃ³n del `_` y dejamos el cÃ³digo **claro y legible**.

Vale, quiero dejar algo claro. Es posible que ahora mismo digas:

- No entiendo los codigos de los tests
- No entiendo muchas partes del codigo python que se esta escribiendo
- Me estoy aturullando de tanta informaciÃ³n de golpe

Si esto es asÃ­, necesitas refrescar conceptos.

1. Esto es un master para desarrollar con IA, el objetivo no es que sepas todo el codigo de memoria, sino que lo entiendas al leerlo y que tengas los conceptos de la metodologÃ­a que quieres seguir.
2. En Internet hay muchos cursos de Python bÃ¡sicos. Antes de continuar, te recomiendo que te los mires, aunque sea rapidamente, sin pararte a memorizar todo.
3. Si quieres aprender a hacer test con profundidad, hay muchos cursos de testing en programaciÃ³n.

MI objetivo ahora mismo es montar un esquema mental, que te indique como se debe de programar, crear proyectos y hacer aplicaciones escalables. No es tanto que seas un experto en testing o en python. Para eso ya hay muchos cursos gratuitos que cubren esos aspectos.

Po tanto, lo que quiero que ahora mismo tengas claro en tu cabeza es:

1. Un proyecto empieza con un repo de git, para permitirnos volver atras si nos cargamos el proyecto entero
2. Antes de realiza cualquier cambio en la aplicacion, hacemos una rama (branch) y ahÃ­ hacemos los cambios. Si todo estÃ¡ correcto, hacemos pull request para escribir los cambios en la rama principal (o en la rama dev, como veremos mas adelante)
3. Si queremos usar TDD (Y con IA es lo mejor), primero se hacen los test y luego se programan las funciones
4. Los test verifican casos de uso (historias de usuario), y comprueban que la salida devuelva lo que debe devolver.
5. Para modificar funciones, primero debemos modificar los test y luego modificar la funcion.
6. A la IA se le pide codigo con una estructura, contexto y en pasos pequeÃ±os:
    1. Rol
    2. Tarea
    3. Objetivo
7. Siempre dividimos el problema es funciones con una sola responsabilidad, y a la IA le iremos dando estas tareas.
8. La IA escribe abreviaturas, pidele que no lo haga y siempre haz codigo limpio y legible. Si tu no lo entiendes, estÃ¡ mal.
9. Antes de hacer PR los test deben pasar.
10. Los test deben cubrir todas las posibilidades que puedan ocurrirle a un usuario. (Esto antes para un programador era practicamente imposible, debido a los presupuestos, ahora con la IA es posible hacerlo, y es necesario.

Vale, ahora que tienes en la cabeza este esquema de trabajo, vamos a aÃ±adir una **nueva historia de usuario**: listar tareas filtradas por prioridad.

Lo atacamos con TDD: primero un test que exprese esa historia, luego el cÃ³digo mÃ­nimo para que pase.

### La historia continÃºa

Imagina que tu CLI de tareas ya lo usas en tu dÃ­a a dÃ­a. Has agregado varias tareas con distinta prioridad:

- Estudiar IA â†’ prioridad alta
- Repasar Git â†’ prioridad baja
- Llamar a tu madre â†’ prioridad media

Hasta aquÃ­ todo bien.

Pero un dÃ­a, con la lista llena de cosas, piensas:

*"Necesito ver solo lo que es **urgente**. No quiero que se me pierda entre todo lo demÃ¡s."*

Ese es el **nuevo problema** que enfrentamos: **filtrar tareas por prioridad**.

Y es perfecto para practicar **TDD + SRP**: escribir primero el test, luego el cÃ³digo mÃ­nimo, y asegurarnos de que la funciÃ³n hace **una sola cosa clara**.

### Paso 1 â€” Escribir el test (el contrato)

Antes de tocar cÃ³digo, describimos la historia como prueba automatizada:

```python
def test_listar_solo_prioridad_alta(self):
    agregar_tarea(self.tmp, "Estudiar IA", "alta")
    agregar_tarea(self.tmp, "Repasar Git", "baja")
    agregar_tarea(self.tmp, "Llamar a mamÃ¡", "media")

    tareas_alta = listar_tareas(self.tmp, prioridad="alta")

    # Debe haber exactamente 1 tarea
    self.assertEqual(len(tareas_alta), 1)
    
    # Esa tarea debe ser la de "Estudiar IA"
    self.assertEqual(tareas_alta[0]["nombre"], "Estudiar IA")
    self.assertEqual(tareas_alta[0]["prioridad"], "alta")

```

Este test cuenta la historia: *â€œcuando filtro por prioridad alta, solo me devuelve esas tareasâ€*.

### Paso 2 â€” Implementar lo mÃ­nimo

Ahora sÃ­, ampliamos `listar_tareas` para aceptar un filtro opcional:

```python
def listar_tareas(ruta_archivo: str, prioridad: str | None = None) -> list[dict]:
    """Devuelve la lista de tareas. Si se pasa prioridad, filtra por ella."""
    todas = cargar_tareas(ruta_archivo)

    if prioridad is None:
        return todas

    return [tarea for tarea in todas if tarea.get("prioridad") == prioridad]

```

- **Si no pasas prioridad** â†’ devuelve todas (como hasta ahora).
- **Si pasas prioridad** â†’ devuelve solo las que coincidan.

### Paso 3 â€” RelaciÃ³n con SRP (Single Responsibility Principle)

FÃ­jate quÃ© hemos hecho aquÃ­:

- `listar_tareas` tiene **una responsabilidad clara**: devolver tareas.
- Si ademÃ¡s le pides filtrar, lo hace, pero sigue siendo **coherente con su propÃ³sito**.
- No imprime, no escribe en archivos, no cambia datos. Solo **lee y selecciona**.

Esto es SRP en miniatura: una funciÃ³n que tiene **un Ãºnico motivo para cambiar** â†’ si maÃ±ana cambia cÃ³mo se guardan las prioridades, tocarÃ­as aquÃ­; pero si cambia cÃ³mo se imprimen en el CLI, eso va en otro lado.

---

## ğŸ¤– Workflow TDD con IA: RED â†’ GREEN â†’ REFACTOR

Hasta ahora has visto el ciclo TDD bÃ¡sico. Ahora vamos a potenciarlo con **Test Coverage Strategist** y **Python Best Practices Coach** para hacer TDD profesional.

### RED: Escribir tests que fallen (Con ayuda de IA)

**Problema**: Cuando aÃ±ades una feature nueva (ej. prioridades), Â¿quÃ© casos debes testear?

**SoluciÃ³n**: Pedir al Test Coverage Strategist una lista COMPLETA de casos.

#### Paso RED.1: Define la feature (Manual)

```markdown
Feature: Sistema de prioridades (alta/media/baja)
- agregar_tarea debe aceptar parÃ¡metro prioridad
- listar_tareas debe filtrar por prioridad
```

#### Paso RED.2: Pide casos de prueba a IA

**Prompt al Test Coverage Strategist**:

```
Rol: Test Coverage Strategist
Feature nueva: Sistema de prioridades en tareas (alta/media/baja)

Funcionalidad:
- agregar_tarea(ruta, nombre, prioridad="media")
- listar_tareas(ruta, prioridad=None)

Objetivo: Lista de casos de prueba para TDD.
Categoriza por criticidad (Alta/Media/Baja).
```

**IA responde**:

```markdown
### Criticidad ALTA:
1. Agregar con prioridad explÃ­cita "alta"
2. Agregar sin prioridad (default "media")
3. Prioridad invÃ¡lida usa "media"
4. Filtrar por prioridad "alta"
5. Listar sin filtro devuelve todas
6. Backward compatibility: tareas antiguas sin prioridad

### Criticidad MEDIA:
7. Filtrar prioridad sin resultados â†’ []
8. Case-insensitive ("ALTA" â†’ "alta")

### Criticidad BAJA:
9. Todas las prioridades (parametrizaciÃ³n)
```

#### Paso RED.3: Escribir tests que FALLEN (Manual)

**âš ï¸ IMPORTANTE**: TÃš escribes los tests, la IA solo sugiriÃ³ QUÃ‰ testear.

```python
# test_tareas_pytest_prioridades.py

def test_agregar_tarea_prioridad_alta(archivo_temporal):
    """Test RED: Este fallarÃ¡ porque feature no existe."""
    tarea = agregar_tarea(archivo_temporal, "Urgente", prioridad="alta")
    assert tarea["prioridad"] == "alta"
    # âŒ TypeError: agregar_tarea() got unexpected keyword 'prioridad'


def test_listar_solo_prioridad_alta(archivo_temporal):
    """Test RED: Filtrado no implementado aÃºn."""
    agregar_tarea(archivo_temporal, "Urgente", prioridad="alta")
    agregar_tarea(archivo_temporal, "Normal", prioridad="media")

    altas = listar_tareas(archivo_temporal, prioridad="alta")

    assert len(altas) == 1
    assert altas[0]["nombre"] == "Urgente"
    # âŒ TypeError: listar_tareas() got unexpected keyword 'prioridad'
```

**Ejecuta tests** (deben fallar âŒ):
```bash
pytest test_tareas_pytest_prioridades.py -v
```

**âœ… Fase RED completa**: Tienes tests que describen la feature pero fallan.

---

### GREEN: ImplementaciÃ³n mÃ­nima (Con ayuda de IA)

**Objetivo**: Hacer que los tests pasen con el cÃ³digo MÃN IMO necesario.

#### Paso GREEN.1: Pedir implementaciÃ³n a IA

**Prompt**:

```
Rol: Python developer
Contexto: Tengo tests RED que fallan (feature prioridades no existe).

Tests que deben pasar:
[pega los tests de arriba]

CÃ³digo actual:
[pega agregar_tarea y listar_tareas]

Objetivo: Modifica funciones para que pasen tests.
Requisitos:
- ParÃ¡metro opcional prioridad="media"
- Validar prioridades vÃ¡lidas (alta/media/baja)
- Filtrado opcional en listar_tareas
- Type hints
```

**IA genera**:

```python
PRIORIDADES_VALIDAS = ("alta", "media", "baja")

def agregar_tarea(ruta: str, nombre: str, prioridad: str = "media") -> dict:
    if prioridad not in PRIORIDADES_VALIDAS:
        prioridad = "media"

    tareas = cargar_tareas(ruta)
    nueva = {
        "id": nuevo_id(tareas),
        "nombre": nombre,
        "completada": False,
        "prioridad": prioridad
    }
    tareas.append(nueva)
    guardar_tareas(ruta, tareas)
    return nueva


def listar_tareas(ruta: str, prioridad: str | None = None) -> list[dict]:
    todas = cargar_tareas(ruta)
    if prioridad is None:
        return todas
    return [t for t in todas if t.get("prioridad") == prioridad]
```

#### Paso GREEN.2: Revisar cÃ³digo (Manual)

**TÃš decides**:
- Â¿Entiendes cada lÃ­nea?
- Â¿Sigue SRP?
- Â¿Es el mÃ­nimo o aÃ±ade complejidad innecesaria?

**Si no entiendes algo**, pregunta a la IA:
```
Explica esta lÃ­nea: return [t for t in todas if t.get("prioridad") == prioridad]
```

#### Paso GREEN.3: Ejecutar tests (deben pasar âœ…)

```bash
pytest test_tareas_pytest_prioridades.py -v
```

**Resultado esperado**:
```
test_agregar_tarea_prioridad_alta PASSED
test_listar_solo_prioridad_alta PASSED
```

**âœ… Fase GREEN completa**: Feature implementada, tests pasan.

---

### REFACTOR: Limpieza con IA (Python Best Practices Coach)

**Objetivo**: CÃ³digo funciona, ahora lo hacemos PYTHONIC.

#### Paso REFACTOR.1: Pedir revisiÃ³n al Coach

**Prompt al Python Best Practices Coach**:

```
Rol: Python Best Practices Coach
Contexto: ImplementÃ© prioridades. Tests pasan âœ….

CÃ³digo:
[pega agregar_tarea y listar_tareas]

Objetivo: Sugiere mejoras Pythonic (NO cambies funcionalidad).
- Anti-patterns detectados
- Type hints mÃ¡s especÃ­ficos
- CÃ³digo mÃ¡s idiomÃ¡tico
```

**Coach sugiere**:

```markdown
## Mejoras Pythonic

### 1. Type hints especÃ­ficos con Literal
âŒ Antes: `prioridad: str = "media"`
âœ… DespuÃ©s: `prioridad: Literal["alta", "media", "baja"] = "media"`

**Ventaja**: IDE detecta typos ("medio" darÃ­a error)

### 2. Extraer validaciÃ³n a funciÃ³n helper
âŒ Antes: `if prioridad not in PRIORIDADES_VALIDAS:`
âœ… DespuÃ©s: `_normalizar_prioridad(prioridad: str) -> Literal[...]`

**Ventaja**: CÃ³digo mÃ¡s limpio, funciÃ³n tiene un solo propÃ³sito

### 3. List comprehension estÃ¡ OK âœ…
Tu cÃ³digo `[t for t in todas if t.get("prioridad") == prioridad]` es Pythonic.
```

#### Paso REFACTOR.2: Aplicar mejoras (Manual)

```python
from typing import Literal

Prioridad = Literal["alta", "media", "baja"]

def _normalizar_prioridad(prioridad: str) -> Prioridad:
    """Normaliza prioridad o usa 'media' si invÃ¡lida."""
    prioridad_lower = prioridad.lower()
    return prioridad_lower if prioridad_lower in ("alta", "media", "baja") else "media"


def agregar_tarea(ruta: str, nombre: str, prioridad: Prioridad = "media") -> dict:
    prioridad = _normalizar_prioridad(prioridad)
    # ... resto del cÃ³digo
```

#### Paso REFACTOR.3: Tests deben seguir pasando

```bash
pytest test_tareas_pytest_prioridades.py -v
```

**Si fallan**, revertir cambio. **Si pasan** âœ…, refactor exitoso.

**âœ… Fase REFACTOR completa**: CÃ³digo limpio, tests pasan, SRP mantenido.

---

## ğŸ¯ Proyecto: Alcanzar 90%+ Coverage con Test Coverage Strategist

**Objetivo**: Superar el mÃ­nimo (80%) y llegar a cobertura de excelencia (90%+).

### Paso 1: Coverage actual

```bash
pytest --cov=. --cov-report=term-missing
```

**Ejemplo de output**:
```
Name          Stmts   Miss  Cover   Missing
-------------------------------------------
tareas.py        45      5    89%   23-25, 67, 89
test_...py       32      0   100%
-------------------------------------------
TOTAL            77      5    93%
```

EstÃ¡s en **89%**. Necesitas **90%+**.

### Paso 2: Invocar Test Coverage Strategist

**Prompt**:

```
Rol: Test Coverage Strategist
Contexto: Estoy en 89% coverage, quiero 90%+.

Coverage report:
[pega output pytest --cov]

CÃ³digo: tareas.py completo

Objetivo: Plan priorizado de tests para alcanzar 90%+.
Identifica gaps y quÃ© tests necesito.
```

**IA responde**:

```markdown
## Gaps de Coverage Detectados

### LÃ­neas sin cubrir:

**LÃ­nea 23-25** (tareas.py):
```python
if "prioridad" not in tarea or tarea["prioridad"] not in PRIORIDADES_VALIDAS:
    tarea["prioridad"] = "media"
```
**Criticidad**: ALTA (backward compatibility)
**Test necesario**: Cargar tarea legacy sin prioridad

**LÃ­nea 67** (tareas.py):
```python
return False  # Tarea no encontrada
```
**Criticidad**: ALTA (error handling)
**Test necesario**: Completar ID inexistente

**LÃ­nea 89** (tareas.py):
```python
return []  # JSON corrupto
```
**Criticidad**: MEDIA
**Test necesario**: Archivo JSON invÃ¡lido

### Plan para 90%+:

**Test 1** (cubre lÃ­neas 23-25):
```python
def test_backfill_prioridad_tareas_legacy(archivo_temporal):
    # Simular tarea sin prioridad
    legacy = [{"id": 1, "nombre": "Antigua", "completada": False}]
    guardar_tareas(archivo_temporal, legacy)

    tareas = cargar_tareas(archivo_temporal)
    assert tareas[0]["prioridad"] == "media"
```

**Test 2** (cubre lÃ­nea 67):
```python
def test_completar_tarea_id_inexistente(archivo_temporal):
    ok = completar_tarea(archivo_temporal, 999)
    assert ok == False
```

**Impacto**: +3 lÃ­neas cubiertas â†’ **92% coverage** âœ…
```

### Paso 3: Implementar tests del plan (Manual)

Escribe TÃš los tests. Verifica que cubren las lÃ­neas indicadas.

### Paso 4: ParametrizaciÃ³n para reducir duplicaciÃ³n

**Detecta duplicaciÃ³n**:

```python
def test_prioridad_alta(...): ...
def test_prioridad_media(...): ...
def test_prioridad_baja(...): ...
```

**Refactoriza con pytest.mark.parametrize**:

```python
@pytest.mark.parametrize("prioridad", ["alta", "media", "baja"])
def test_prioridades_validas(archivo_temporal, prioridad):
    tarea = agregar_tarea(archivo_temporal, f"Tarea {prioridad}", prioridad=prioridad)
    assert tarea["prioridad"] == prioridad
```

**Beneficio**: 1 test en vez de 3, mÃ¡s mantenible.

### Paso 5: ValidaciÃ³n final (90%+ alcanzado)

```bash
pytest --cov=. --cov-report=html --cov-fail-under=90 -v
```

**Resultado esperado**:
```
======================== 12 passed in 0.52s ========================
Coverage: 92%
```

Abre `htmlcov/index.html` para visualizar cobertura lÃ­nea por lÃ­nea.

**âœ… Ã‰XITO**: 90%+ coverage con tests significativos (no tests inÃºtiles).

---

## ğŸ“š Ejercicio Completo: TDD con IA (90%+ Coverage)

**Consulta**: `ejercicio_clase4_ai_avanzado.md` en esta carpeta.

**Fases del ejercicio**:
1. **RED** (20 min): Tests que fallan con lista de IA
2. **GREEN** (20 min): ImplementaciÃ³n mÃ­nima con ayuda IA
3. **REFACTOR** (15 min): Limpieza con Python Best Practices Coach
4. **COVERAGE** (30 min): Plan con Test Coverage Strategist â†’ 90%+

**Entregables**:
- `test_tareas_pytest_prioridades.py` con 10+ tests
- `tareas.py` refactorizado y Pythonic
- `notes.md` documentando workflow RED-GREEN-REFACTOR
- Coverage 90%+ âœ…

**Regla de oro**:
- IA sugiere QUÃ‰ testear â†’ TÃš escribes el cÃ³digo
- IA genera plantilla â†’ TÃš entiendes cada lÃ­nea
- IA refactoriza â†’ TÃš validas con tests

---

### Pausa de respiraciÃ³n

Â¿Te das cuenta del patrÃ³n?

1. Empieza un **problema real** (quiero ver solo lo urgente).
2. Lo transformamos en un **test** (un contrato claro).
3. Escribimos el **cÃ³digo mÃ­nimo** para que pase.
4. Confirmamos que cada funciÃ³n mantiene una **Ãºnica responsabilidad**.

Ya no estamos improvisando ni escribiendo jeroglÃ­ficos: estamos siguiendo una metodologÃ­a que escala.

ğŸ‘‰ Ejercicio de la clase:

- Crea rama `feature/cli-prioridades`.
- AÃ±ade el test de filtro + el cambio en `listar_tareas`.
- Corre todos los tests.
- Si todo estÃ¡ verde, haz PR.
- Documenta en `notes.md`: *â€œAÃ±adÃ­ prioridad a tareas. Al principio se rompieron los tests. Los arreglÃ© con valor por defecto y ahora tambiÃ©n tengo filtrado por prioridad.â€*

# âœ… Checklist de la Clase 4 â€“ Testing ampliado + primer principio SOLID

### Conceptos que deben quedarte claros

- Los **tests** son tu red de seguridad: cuando cambiaste la funciÃ³n `agregar_tarea` para aÃ±adir `prioridad`, los tests fallaron y te avisaron del problema.
- Un cambio grande debe empezar por el **test**: escribes lo que quieres comprobar, luego ajustas el cÃ³digo para que lo cumpla.
- El guion bajo `_` delante de una funciÃ³n o variable es solo **una convenciÃ³n**: indica que es interna, pero no te impide usarla.
- **SRP (Single Responsibility Principle)** en miniatura: cada funciÃ³n debe tener un motivo Ãºnico para cambiar.
    - `agregar_tarea` crea y guarda.
    - `listar_tareas` devuelve y filtra.
    - No mezclan impresiÃ³n ni argumentos del CLI.

### Resultado tangible del PR

- Rama nueva: `feature/cli-prioridades`.
- `agregar_tarea` ahora admite un campo `prioridad` con valor por defecto `"media"`.
- `cargar_tareas` rellena el campo si el archivo viejo no lo tiene.
- `listar_tareas` acepta un argumento opcional `prioridad` para filtrar.
- Tests aÃ±adidos:
    - Agregar con prioridad por defecto.
    - Agregar con prioridad explÃ­cita.
    - Listar solo las tareas de una prioridad.
- Todos los tests deben estar en verde antes de hacer el PR.

### Meta-mensaje de la clase

No necesitas ser experto en Python o unittest. Lo importante es que interiorices la **metodologÃ­a**:

1. Rama â†’ test â†’ cÃ³digo mÃ­nimo â†’ PR.
2. Funciones pequeÃ±as y con responsabilidad clara.
3. Convenciones que ayudan a humanos, IA y al â€œyo del futuroâ€ a entender el cÃ³digo.

---

ğŸ‘‰ Con esto, tu CLI ya empieza a parecerse a un mini-proyecto real: tiene nuevas features, tests que lo protegen y una base de diseÃ±o que evita que se convierta en spaghetti.

---

# ğŸ¯ Proyecto Final del MÃ³dulo 1

Esta Clase 4 ES tu **proyecto final del MÃ³dulo 1**. No es solo un ejercicio mÃ¡s: es donde demuestras que dominas los fundamentos de desarrollo + asistencia de IA.

## Objetivos del Proyecto Final

Al completar este proyecto, habrÃ¡s demostrado:

1. âœ… **Git workflow profesional**: Branches, commits, Pull Requests
2. âœ… **TDD con IA**: RED â†’ GREEN â†’ REFACTOR usando agentes
3. âœ… **SOLID bÃ¡sico**: Single Responsibility Principle aplicado
4. âœ… **Coverage de excelencia**: 90%+ con tests significativos
5. âœ… **ValidaciÃ³n de cÃ³digo IA**: Verificar que el cÃ³digo generado es correcto

---

## ğŸ¤– Workflow Multi-Agente del Proyecto

Este proyecto debes completarlo usando un **workflow estructurado de agentes**. No se trata de "pedirle todo a la IA", sino de usar **el agente correcto en cada fase**.

### Fase 1: RED (DiseÃ±o de Tests) â†’ Test Coverage Strategist

**CuÃ¡ndo usarlo**: Cuando necesites identificar QUÃ‰ casos de prueba escribir.

**CÃ³mo usarlo**:

```markdown
Prompt al Test Coverage Strategist:

Rol: Test Coverage Strategist
Feature nueva: Sistema de prioridades en tareas (alta/media/baja)

Funcionalidad:
- agregar_tarea(ruta, nombre, prioridad="media")
- listar_tareas(ruta, prioridad=None)

Objetivo: Lista completa de casos de prueba para TDD.
Categoriza por criticidad (Alta/Media/Baja).
```

**QuÃ© hace el agente**:
- âœ… Sugiere casos de prueba (happy path, edge cases, validaciÃ³n)
- âœ… Prioriza por criticidad (quÃ© testear primero)
- âœ… Identifica casos que NO se te habÃ­an ocurrido

**QUÃ‰ HACES TÃš** (NO delegar al agente):
- âŒ NO copies los tests automÃ¡ticamente
- âœ… ESCRIBE TÃš los tests basÃ¡ndote en las sugerencias
- âœ… ENTIENDE por quÃ© cada test es necesario

---

### Fase 2: GREEN (ImplementaciÃ³n) â†’ Python Best Practices Coach

**CuÃ¡ndo usarlo**: DespuÃ©s de implementar el cÃ³digo (cuando tests ya pasan).

**CÃ³mo usarlo**:

```markdown
Prompt al Python Best Practices Coach:

Rol: Python Best Practices Coach
Contexto: ImplementÃ© prioridades en tareas. Tests pasan âœ….

CÃ³digo:
[pega tu cÃ³digo]

Objetivo: Revisa y sugiere mejoras Pythonic.
- Â¿Hay anti-patterns?
- Â¿EstÃ¡n bien los type hints?
- Â¿CÃ³digo legible y mantenible?
```

**QuÃ© hace el agente**:
- âœ… Detecta anti-patterns (variables `p`, `t`, cÃ³digo crÃ­ptico)
- âœ… Sugiere mejoras Pythonic (f-strings, comprehensions)
- âœ… Valida type hints y documentaciÃ³n

**QUÃ‰ HACES TÃš**:
- âœ… REVISA cada sugerencia crÃ­ticamente
- âœ… APLICA solo mejoras que entiendes
- âœ… EJECUTA tests despuÃ©s de cada cambio

---

### Fase 3: REFACTOR (Arquitectura) â†’ Clean Architecture Enforcer

**CuÃ¡ndo usarlo**: Cuando hayas completado la feature y quieras validar diseÃ±o.

**CÃ³mo usarlo**:

```markdown
Prompt al Clean Architecture Enforcer:

Rol: Clean Architecture Enforcer
Contexto: CLI de tareas con prioridades. Â¿Sigue SRP?

CÃ³digo completo:
[pega tareas.py]

Objetivo: Valida si funciones tienen responsabilidad Ãºnica.
- Â¿Alguna funciÃ³n hace demasiadas cosas?
- Â¿Mezclo concerns (IO + lÃ³gica de negocio)?
```

**QuÃ© hace el agente**:
- âœ… Valida Single Responsibility Principle
- âœ… Detecta "god functions" (funciones que hacen demasiado)
- âœ… Sugiere refactorings si es necesario

**QUÃ‰ HACES TÃš**:
- âœ… EVALÃšA si los refactorings son necesarios (no siempre lo son)
- âœ… REFACTORIZA con tests pasando como red de seguridad
- âœ… DOCUMENTA las decisiones arquitectÃ³nicas

---

### Fase 4: COVERAGE (ValidaciÃ³n Final) â†’ Test Coverage Strategist

**CuÃ¡ndo usarlo**: Antes del Pull Request, para validar coverage.

**CÃ³mo usarlo**:

```bash
# Ejecuta coverage
pytest --cov=. --cov-report=term-missing
```

```markdown
Prompt al Test Coverage Strategist:

Rol: Test Coverage Strategist
Contexto: Estoy en X% coverage, objetivo 90%+.

Coverage report:
[pega output de pytest --cov]

CÃ³digo: tareas.py

Objetivo: Plan priorizado de tests para 90%+.
```

**QuÃ© hace el agente**:
- âœ… Identifica lÃ­neas sin cubrir
- âœ… Explica POR QUÃ‰ son importantes
- âœ… Sugiere tests especÃ­ficos

**QUÃ‰ HACES TÃš**:
- âœ… ESCRIBES los tests sugeridos
- âœ… VALIDAS que coverage sube
- âœ… VERIFICAS que tests son significativos (no "test por test")

---

## ğŸ“ DocumentaciÃ³n del Uso de IA (OBLIGATORIO)

Parte del proyecto es **documentar quÃ© hiciste con IA y quÃ© hiciste manualmente**. Esto es crÃ­tico para:

1. **Transparencia**: Saber quÃ© aprendiste vs quÃ© delegaste
2. **AuditorÃ­a**: Demostrar que entiendes el cÃ³digo
3. **Aprendizaje**: Reflexionar sobre el proceso

### Plantilla: `ai_workflow_log.md`

Crea este archivo en tu proyecto y complÃ©talo:

```markdown
# AI Workflow Log - Proyecto Final MÃ³dulo 1

**Estudiante**: [Tu nombre]
**Fecha**: [Fecha de inicio - Fecha fin]
**Feature**: Sistema de prioridades en tareas

---

## ğŸ¤– Uso de Agentes IA

### Test Coverage Strategist

**CuÃ¡ndo usÃ©**: Fase RED (diseÃ±o de tests)

**Prompt usado**:
\```
Rol: Test Coverage Strategist
Feature nueva: Sistema de prioridades...
[pega prompt completo]
\```

**QuÃ© sugiriÃ³ el agente**:
- Test 1: Agregar con prioridad explÃ­cita "alta"
- Test 2: Agregar sin prioridad (default "media")
- Test 3: Prioridad invÃ¡lida usa "media"
- Test 4: Filtrar por prioridad "alta"
- Test 5: Backward compatibility

**QuÃ© casos NO se me habÃ­an ocurrido**:
- âŒ Backward compatibility (tareas antiguas sin prioridad)
- âŒ Case-insensitive ("ALTA" â†’ "alta")

**QuÃ© hice yo manualmente**:
- âœ… EscribÃ­ TODOS los tests yo mismo (no copiÃ© cÃ³digo del agente)
- âœ… EntendÃ­ por quÃ© cada test era necesario
- âœ… AÃ±adÃ­ un test extra que el agente no sugiriÃ³: [descripciÃ³n]

---

### Python Best Practices Coach

**CuÃ¡ndo usÃ©**: Fase GREEN (refactoring de implementaciÃ³n)

**QuÃ© sugiriÃ³**:
- Usar `Literal["alta", "media", "baja"]` en vez de `str`
- Extraer validaciÃ³n a funciÃ³n `_normalizar_prioridad()`
- Usar f-strings en mensajes de error

**QuÃ© cambios apliquÃ©**:
- âœ… Type hints con Literal (entendÃ­ que mejora autocomplete)
- âœ… FunciÃ³n helper `_normalizar_prioridad()` (separaciÃ³n de concerns)
- âŒ NO usÃ© f-strings porque [razÃ³n]

**Tests despuÃ©s de refactor**: âœ… Todos pasando

---

### Clean Architecture Enforcer

**CuÃ¡ndo usÃ©**: Fase REFACTOR (validaciÃ³n arquitectÃ³nica)

**Violaciones detectadas**:
- [Ninguna / DescripciÃ³n de violaciÃ³n]

**Refactorings aplicados**:
- [DescripciÃ³n de cambios]

---

## âœï¸ CÃ³digo Escrito Manualmente vs IA

### Escritura Manual (sin asistencia IA)

- âœ… **Todos los tests**: EscribÃ­ cada `assert`, cada caso
- âœ… **Estructura del programa**: DecidÃ­ funciones, flujo
- âœ… **Validaciones de negocio**: Reglas de prioridades

### Con Asistencia IA (prompt â†’ revisar â†’ modificar)

- ğŸ¤– **ImplementaciÃ³n inicial de `agregar_tarea`**: IA generÃ³ esqueleto, yo ajustÃ©
- ğŸ¤– **List comprehension para filtrado**: IA sugiriÃ³, yo entendÃ­ y adaptÃ©
- ğŸ¤– **Type hints especÃ­ficos**: IA sugiriÃ³ `Literal`, yo investiguÃ© y apliquÃ©

### CÃ³digo Copiado Directamente de IA (âŒ EVITAR)

- âŒ **Ninguno**: No copiÃ© cÃ³digo sin entender

---

## ğŸ“ Aprendizajes

### Conceptos Nuevos Aprendidos

1. **TDD con IA**: La IA sugiere QUÃ‰ testear, yo escribo CÃ“MO
2. **Type hints avanzados**: `Literal` para strings con valores especÃ­ficos
3. **Coverage estratÃ©gico**: No se trata del %, sino de tests significativos
4. **SRP en prÃ¡ctica**: Funciones pequeÃ±as, una responsabilidad

### Errores Cometidos y Corregidos

1. **Error**: CopiÃ© cÃ³digo de IA sin entender list comprehension
   - **CorrecciÃ³n**: PreguntÃ© a IA "explica esta lÃ­nea", luego reescribÃ­ yo
2. **Error**: AlcancÃ© 90% con tests inÃºtiles (test por test)
   - **CorrecciÃ³n**: EliminÃ© tests duplicados, usÃ© parametrizaciÃ³n

### Decisiones de DiseÃ±o (Justificadas)

1. **Por quÃ© `prioridad` tiene default "media"**:
   - Backward compatibility con cÃ³digo existente
   - No romper tests antiguos

2. **Por quÃ© extraje `_normalizar_prioridad()`**:
   - Reutilizable en mÃºltiples funciones
   - Testeable independientemente
   - Sigue SRP

---

## âœ… Coverage Final

**Coverage alcanzado**: 92%

**LÃ­neas sin cubrir (y por quÃ©)**:
- LÃ­nea X: [RazÃ³n]

**Tests escritos**: 12 tests
- 8 tests crÃ­ticos (alta prioridad)
- 3 tests edge cases (media prioridad)
- 1 test parametrizado (baja, media, alta)

---

## ğŸš€ Siguientes Pasos

**Si tuviera mÃ¡s tiempo, mejorarÃ­a**:
- [ ] AÃ±adir validaciÃ³n de longitud de `nombre`
- [ ] Implementar `eliminar_tarea()`
- [ ] Persistencia en JSON (ahora solo memoria)

**Lo que aplicarÃ© en el prÃ³ximo mÃ³dulo**:
- Workflow RED-GREEN-REFACTOR siempre
- Validar con agentes DESPUÃ‰S de escribir, no antes
- Documentar uso de IA en cada fase
```

---

## ğŸ” ValidaciÃ³n de CÃ³digo Generado por IA

**REGLA DE ORO**: NUNCA confÃ­es ciegamente en cÃ³digo de IA. Siempre valida.

### Checklist de ValidaciÃ³n (ANTES de aceptar cÃ³digo de IA)

#### 1. Â¿Entiendes cada lÃ­nea?

```python
# âŒ CÃ³digo de IA que NO entiendes
return [t for t in todas if t.get("prioridad") == prioridad]

# âœ… ANTES de aceptar, pregunta:
"Explica esta lÃ­nea paso a paso. Â¿QuÃ© hace .get()? Â¿Por quÃ© no usar t['prioridad']?"
```

**AcciÃ³n**: Si no entiendes algo, pregunta a la IA "Explica como si tuviera 10 aÃ±os".

---

#### 2. Â¿Los tests pasan?

```bash
# Ejecuta tests DESPUÃ‰S de pegar cÃ³digo de IA
pytest -v

# âŒ Si fallan: NO aceptes el cÃ³digo, investiga por quÃ©
# âœ… Si pasan: Bien, pero NO es suficiente
```

**AcciÃ³n**: Tests pasando es mÃ­nimo, no garantÃ­a de calidad.

---

#### 3. Â¿Sigue las convenciones del proyecto?

**Checklist**:
- [ ] Type hints en todos los parÃ¡metros
- [ ] Nombres largos y descriptivos (no `p`, `t`, `x`)
- [ ] Docstrings en funciones pÃºblicas
- [ ] Sin cÃ³digo "mÃ¡gico" (nÃºmeros sin constantes, strings hardcodeados)

```python
# âŒ CÃ³digo de IA que viola convenciones
def a(p):
    if p not in ["alta", "media", "baja"]:  # String hardcodeado
        return "media"
    return p

# âœ… Tu versiÃ³n corregida
PRIORIDADES = ("alta", "media", "baja")

def _normalizar_prioridad(prioridad: str) -> str:
    """Normaliza prioridad o usa 'media' si invÃ¡lida."""
    return prioridad if prioridad in PRIORIDADES else "media"
```

---

#### 4. Â¿Es el cÃ³digo MÃNIMO necesario?

La IA tiende a sobre-complicar. Valida:

```python
# âŒ IA aÃ±ade complejidad innecesaria
def agregar_tarea(ruta, nombre, prioridad="media"):
    try:
        with open(ruta, "r") as f:
            tareas = json.load(f)
    except FileNotFoundError:
        tareas = []
    except Exception as e:
        logger.error(f"Error: {e}")
        tareas = []
    # ... 20 lÃ­neas mÃ¡s

# âœ… VersiÃ³n mÃ¡s simple (si ya tienes cargar_tareas())
def agregar_tarea(ruta, nombre, prioridad="media"):
    tareas = cargar_tareas(ruta)  # Reutilizar funciÃ³n existente
    nueva = {"id": nuevo_id(tareas), "nombre": nombre, "prioridad": prioridad}
    tareas.append(nueva)
    guardar_tareas(ruta, tareas)
    return nueva
```

**Pregunta crÃ­tica**: Â¿Hay funciones existentes que puedo reutilizar?

---

#### 5. Â¿Funciona con edge cases?

**Tests obligatorios DESPUÃ‰S de cÃ³digo IA**:

```python
# Test edge case: Prioridad vacÃ­a
def test_prioridad_vacia():
    tarea = agregar_tarea(tmp, "Test", prioridad="")
    assert tarea["prioridad"] == "media"  # Â¿Pasa?

# Test edge case: Prioridad con espacios
def test_prioridad_con_espacios():
    tarea = agregar_tarea(tmp, "Test", prioridad="  alta  ")
    assert tarea["prioridad"] == "alta"  # Â¿Pasa o falla?

# Test edge case: Case-insensitive
def test_prioridad_mayusculas():
    tarea = agregar_tarea(tmp, "Test", prioridad="ALTA")
    assert tarea["prioridad"] == "alta"
```

**Si algÃºn test falla**: El cÃ³digo de IA tiene bugs. ArrÃ©glalo ANTES de continuar.

---

#### 6. Â¿Introduce deuda tÃ©cnica?

**Red flags**:
- âŒ TODO comments: `# TODO: fix this later`
- âŒ Warnings de linter: `ruff check` reporta issues
- âŒ DuplicaciÃ³n: CÃ³digo copiado en mÃºltiples lugares
- âŒ Acoplamiento: FunciÃ³n depende de detalles de implementaciÃ³n

**AcciÃ³n**: Ejecuta linter y corrige ANTES de commit.

```bash
ruff check tareas.py
# Si hay warnings, arregla o justifica por quÃ© no
```

---

### Workflow de ValidaciÃ³n (Paso a Paso)

Cuando la IA te genera cÃ³digo, sigue este proceso:

```markdown
1. [ ] Leo el cÃ³digo lÃ­nea por lÃ­nea
   - Si no entiendo algo â†’ Pregunto "Explica X"
   - Si hay cÃ³digo "mÃ¡gico" â†’ Pido versiÃ³n mÃ¡s simple

2. [ ] Copio el cÃ³digo a mi archivo
   - NO directamente, escribo yo tecleando (refuerza aprendizaje)

3. [ ] Ejecuto tests
   - pytest -v
   - Â¿Todos pasan? â†’ ContinÃºa
   - Â¿Alguno falla? â†’ Investiga, NO copies mÃ¡s cÃ³digo

4. [ ] AÃ±ado tests de edge cases
   - Casos que IA no considerÃ³
   - Si fallan â†’ Arreglo el cÃ³digo

5. [ ] Reviso con linter
   - ruff check
   - Corrijo warnings

6. [ ] Pregunto a Clean Architecture Enforcer
   - Â¿Sigue SRP?
   - Â¿Introduce acoplamiento?

7. [ ] Documento en ai_workflow_log.md
   - QuÃ© generÃ³ IA
   - QuÃ© modifiquÃ© yo
   - Por quÃ©

8. [ ] Commit con mensaje descriptivo
   - feat: aÃ±adir prioridades con validaciÃ³n
   - (NO: "cÃ³digo de IA" o "cambios varios")
```

---

## ğŸ¯ Criterios de Ã‰xito del Proyecto Final

Has completado el Proyecto Final del MÃ³dulo 1 si:

### TÃ©cnico
- [ ] Coverage 90%+ con tests significativos
- [ ] Todos los tests pasan (pytest -v)
- [ ] Sin warnings de linter (ruff check)
- [ ] Feature de prioridades implementada completa
- [ ] Backward compatibility mantenida

### MetodologÃ­a
- [ ] Usaste TDD: RED â†’ GREEN â†’ REFACTOR
- [ ] Aplicaste SRP (validado por Clean Architecture Enforcer)
- [ ] Git workflow: Branch â†’ Commits â†’ PR
- [ ] Tests escritos ANTES de implementaciÃ³n

### IA Workflow
- [ ] Usaste Test Coverage Strategist (fase RED)
- [ ] Usaste Python Best Practices Coach (fase REFACTOR)
- [ ] Usaste Clean Architecture Enforcer (validaciÃ³n)
- [ ] Documentaste en `ai_workflow_log.md` quÃ© hizo IA y quÃ© hiciste tÃº

### ComprensiÃ³n
- [ ] Puedes explicar cada lÃ­nea de cÃ³digo
- [ ] Entiendes POR QUÃ‰ cada test es necesario
- [ ] Sabes cuÃ¡ndo es OK usar IA y cuÃ¡ndo no
- [ ] Reflexionaste en el log sobre aprendizajes

---

## ğŸš« Antipatrones a Evitar

### âŒ Copiar cÃ³digo de IA sin entender

**SeÃ±al de problema**:
```python
# CÃ³digo que pegaste pero no entiendes
return [t for t in todas if t.get("prioridad") == prioridad]
```

**Pregunta clave**: "Si tuviera que explicar esta lÃ­nea a alguien, Â¿podrÃ­a hacerlo?"

**SoluciÃ³n**: Pregunta "Explica X paso a paso" ANTES de aceptar el cÃ³digo.

---

### âŒ Alcanzar 90% con tests inÃºtiles

**SeÃ±al de problema**:
```python
def test_1():
    assert True  # Test que siempre pasa

def test_nombre_no_es_numero():
    tarea = agregar_tarea(tmp, "Test")
    assert isinstance(tarea["nombre"], str)  # Test trivial
```

**SoluciÃ³n**: Pregunta "Â¿Este test falla si hay un bug real?" Si no, bÃ³rralo.

---

### âŒ Usar IA para TODO

**SeÃ±al de problema**: `ai_workflow_log.md` dice "IA generÃ³ todo, yo solo ejecutÃ©".

**SoluciÃ³n**:
- TÃš escribes tests
- TÃš diseÃ±as arquitectura
- IA solo sugiere mejoras

---

### âŒ No validar cÃ³digo de IA

**SeÃ±al de problema**: Pegaste cÃ³digo, tests pasan, hiciste commit inmediato.

**SoluciÃ³n**: Sigue el Workflow de ValidaciÃ³n completo (checklist de 8 pasos).

---

## ğŸ“š Recursos de Apoyo

### Agentes Educativos

- **Test Coverage Strategist**: `.claude/agents/educational/test-coverage-strategist.md`
- **Python Best Practices Coach**: `.claude/agents/educational/python-best-practices-coach.md`
- **Clean Architecture Enforcer**: `.claude/agents/educational/clean-architecture-enforcer.md`

### DocumentaciÃ³n

- **Ejercicio completo**: `ejercicio_clase4_ai_avanzado.md` (en esta carpeta)
- **Glosario**: `Glosario - Clase 4.md` (tÃ©rminos clave)

### Comandos Ãštiles

```bash
# Tests
pytest -v                                    # Ejecutar todos los tests
pytest --cov=. --cov-report=term-missing    # Coverage detallado
pytest -k "prioridad"                        # Solo tests de prioridad

# Linting
ruff check tareas.py                         # Verificar estilo
ruff check --fix tareas.py                   # Auto-corregir

# Git
git status                                   # Ver cambios
git diff                                     # Ver diferencias
git add tareas.py test_*.py                 # AÃ±adir archivos
git commit -m "feat: aÃ±adir prioridades"    # Commit
```

---

## ğŸ“ ReflexiÃ³n Final

Este proyecto final NO es sobre "hacer funcionar el cÃ³digo". Es sobre:

1. **MetodologÃ­a**: TDD, Git workflow, uso estratÃ©gico de IA
2. **Criterio**: Saber CUÃNDO usar IA, CUÃNDO escribir tÃº
3. **ComprensiÃ³n**: Entender cada lÃ­nea, no copiar ciegamente
4. **DocumentaciÃ³n**: Transparencia sobre quÃ© aprendiste

**Pregunta clave para tu reflexiÃ³n**:

> "Si tuviera que explicar este cÃ³digo a un compaÃ±ero SIN mencionar que usÃ© IA, Â¿podrÃ­a hacerlo con confianza?"

Si la respuesta es SÃ â†’ Has completado el proyecto exitosamente.

Si la respuesta es NO â†’ Revisa, pregunta, entiende. Repite hasta que SÃ.

---

**PrÃ³ximo paso**: MÃ³dulo 2 - Arquitectura Limpia con FastAPI + Agentes Especializados ğŸš€