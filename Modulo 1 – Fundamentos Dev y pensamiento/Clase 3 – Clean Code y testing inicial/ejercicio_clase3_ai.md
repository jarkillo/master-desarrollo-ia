# Ejercicio Clase 3: Clean Code y Testing con IA

## ğŸ¯ Objetivos

1. **DiseÃ±ar estructura de proyecto** usando IA como arquitecto (manual vs IA)
2. **Alcanzar 80%+ coverage** en tu CLI de tareas usando el Test Coverage Strategist agent
3. **Aprender cuÃ¡ndo confiar en IA** vs usar criterio propio

---

## ğŸ“‹ Pre-requisitos

Antes de empezar:

1. **CÃ³digo funcional**: Tienes `tareas.py` con funciones:
   - `cargar_tareas(ruta)`
   - `guardar_tareas(ruta, tareas)`
   - `agregar_tarea(ruta, nombre)`
   - `completar_tarea(ruta, id)`
   - `listar_tareas(ruta)`

2. **pytest instalado**:
   ```bash
   pip install pytest pytest-cov
   ```

3. **Tests bÃ¡sicos**: Has escrito al menos `test_agregar_tarea` manualmente

---

## ğŸš€ Parte 1: DiseÃ±o de Estructura de Proyectos con IA

### Tarea 0: DiseÃ±o Manual vs IA (30 min)

**Objetivo**: Comparar tu intuiciÃ³n de estructura vs sugerencias de IA, validadas con Clean Architecture Enforcer.

#### Subtarea 0.1: DiseÃ±o Manual (10 min)

**Sin pedir ayuda a la IA**, diseÃ±a estructura para:

**Proyecto**: Sistema de inventario (CLI)
- Agregar productos (nombre, precio, stock)
- Listar productos
- Actualizar stock
- Buscar por nombre
- Persistencia JSON
- Tests con pytest

**Instrucciones**:

1. Abre `notes.md` y dibuja tu estructura:
   ```
   inventario/
   â”œâ”€â”€ ??? (tus carpetas y archivos)
   â””â”€â”€ ???
   ```

2. Anota:
   - Â¿QuÃ© archivos creaste?
   - Â¿Por quÃ© los separaste asÃ­?
   - Â¿QuÃ© dudas tuviste?

**Ejemplo de reflexiÃ³n**:
```markdown
## Mi diseÃ±o inicial (sin IA)

inventario/
â”œâ”€â”€ inventario.py    # Â¿Todo junto? Â¿O separar?
â”œâ”€â”€ tests.py         # Â¿AquÃ­ los tests?
â””â”€â”€ productos.json

Dudas:
- Â¿Separo lÃ³gica de CLI?
- Â¿DÃ³nde van las funciones de JSON?
- Â¿Tests en archivo separado o carpeta?
```

---

#### Subtarea 0.2: DiseÃ±o con IA (10 min)

Ahora pide estructura a la IA usando un **prompt estructurado**:

**Prompt para Claude Code**:
```
Rol: Arquitecto de proyectos Python
Proyecto: CLI de inventario (productos con nombre, precio, stock)

Operaciones:
- Agregar producto
- Listar productos
- Actualizar stock
- Buscar por nombre
- Persistencia JSON
- Tests con pytest

Dame estructura de carpetas/archivos con explicaciÃ³n.
Considera: testabilidad, escalabilidad moderada, CLI simple.

NO generes cÃ³digo, solo estructura de directorios explicada.
```

**AcciÃ³n TÃš**:

1. **Lee** la estructura que te dio la IA
2. **Compara** con tu diseÃ±o manual en `notes.md`:
   ```markdown
   ## Sugerencia IA

   [Pega aquÃ­ la estructura que te dio]

   ## ComparaciÃ³n

   ### Coincidencias: âœ…
   - [Cosas en las que ambos coincidimos]

   ### IA sugiriÃ³ (yo no): ğŸ¤”
   - [Cosas que la IA propuso y no habÃ­as pensado]

   ### Yo propuse (IA no): ğŸ’¡
   - [Cosas que tÃº pensaste y la IA no mencionÃ³]
   ```

---

#### Subtarea 0.3: ValidaciÃ³n con Clean Architecture Enforcer (10 min)

Ahora valida **ambas estructuras** (tuya y la de IA) usando el agente educativo:

**Prompt para el agente**:
```
Rol: Clean Architecture Enforcer
Contexto: Tengo DOS estructuras propuestas para CLI de inventario.

Estructura 1 (manual):
[Pega tu estructura manual]

Estructura 2 (IA):
[Pega estructura de IA]

Objetivo: Compara ambas. Â¿CuÃ¡l sigue mejor separation of concerns?
Â¿Alguna es demasiado compleja para CLI simple?
Dame feedback educativo sobre ambas.
```

**El agente te dirÃ¡** cuÃ¡l estructura es mejor y por quÃ©.

**AcciÃ³n TÃš**:

1. Lee el feedback del agente
2. **Decide tu estructura final** (hÃ­brido):
   ```markdown
   ## Mi decisiÃ³n final

   [Dibuja la estructura final que usarÃ¡s]

   ## JustificaciÃ³n

   - TomÃ© de mi diseÃ±o: [quÃ© y por quÃ©]
   - TomÃ© de IA: [quÃ© y por quÃ©]
   - RechacÃ© de IA: [quÃ© y por quÃ©]
   - AprendÃ­: [insights del Clean Architecture Enforcer]
   ```

**ValidaciÃ³n**: Has documentado en `notes.md`:
- [ ] Tu diseÃ±o manual inicial
- [ ] Sugerencia de IA
- [ ] Feedback del Clean Architecture Enforcer
- [ ] Tu decisiÃ³n final razonada

---

## ğŸš€ Parte 2: Testing con IA

### Tarea 1: Escribir test happy path (Manual - 15 min)

**Objetivo**: Interiorizar la estructura de un test pytest.

**Instrucciones**:

1. Abre `test_tareas_pytest.py`
2. Escribe MANUALMENTE el test `test_agregar_tarea` (sin copiar del README):

```python
def test_agregar_tarea(archivo_temporal):
    # Arrange: prepara el contexto (fixture ya lo hace)

    # Act: ejecuta la funciÃ³n a testear
    agregar_tarea(archivo_temporal, "Estudiar IA")

    # Assert: verifica el resultado
    tareas = cargar_tareas(archivo_temporal)
    assert len(tareas) == 1
    assert tareas[0]["nombre"] == "Estudiar IA"
```

3. Ejecuta el test:
   ```bash
   pytest test_tareas_pytest.py::test_agregar_tarea -v
   ```

**ValidaciÃ³n**: Test pasa âœ…

---

### Tarea 2: Pedir lista de edge cases a IA (Con IA - 10 min)

**Objetivo**: Aprender a usar el Test Coverage Strategist para descubrir casos que no habÃ­as pensado.

**Prompt para Claude Code**:

```
Rol: Test Coverage Strategist
Contexto: Tengo una funciÃ³n agregar_tarea(ruta, nombre) que guarda tareas en JSON.

CÃ³digo:
[pega la funciÃ³n agregar_tarea de tareas.py]

Objetivo: Lista de edge cases que deberÃ­a testear (NO generes el cÃ³digo aÃºn).
Categoriza por criticidad (Alta/Media/Baja).
```

**Resultado esperado de la IA**:

```markdown
## Edge Cases para agregar_tarea

### Criticidad ALTA:
1. Nombre vacÃ­o ("") - debe rechazar o usar default
2. Nombre con solo espacios ("   ") - debe validar
3. Archivo JSON corrupto - debe manejar error

### Criticidad MEDIA:
4. Nombre muy largo (1000+ caracteres) - puede causar problemas
5. Caracteres especiales en nombre (\n, \t, emojis)

### Criticidad BAJA:
6. Agregar mÃºltiples tareas muy rÃ¡pido (concurrencia)
```

**AcciÃ³n TU**:

1. Lee la lista que te dio la IA
2. Anota en tu `notes.md`:
   - Â¿QuÃ© casos SÃ habÃ­as pensado?
   - Â¿QuÃ© casos NO se te habÃ­an ocurrido?
   - Â¿CuÃ¡les crees que son realmente importantes?

---

### Tarea 3: Implementar edge cases de criticidad ALTA (Manual - 20 min)

**Objetivo**: Escribir TÃš MISMO los tests crÃ­ticos para interiorizar el patrÃ³n.

**Instrucciones**:

Escribe tests para los 2-3 edge cases de criticidad ALTA:

```python
def test_agregar_tarea_nombre_vacio(archivo_temporal):
    """
    Edge case: Nombre vacÃ­o debe ser rechazado o usar default.
    """
    # Â¿QuÃ© deberÃ­a pasar?
    # OpciÃ³n A: Lanzar error
    # OpciÃ³n B: Usar nombre default "Sin tÃ­tulo"

    # Decide TÃš quÃ© comportamiento quieres y escribe el test
    ...


def test_agregar_tarea_json_corrupto(archivo_temporal):
    """
    Edge case: Si el JSON estÃ¡ corrupto, no debe crashear.
    """
    # Escribe JSON invÃ¡lido en el archivo
    with open(archivo_temporal, 'w') as f:
        f.write("{esto no es json vÃ¡lido")

    # Intenta agregar tarea
    # Â¿DeberÃ­a funcionar? Â¿Lanzar error controlado?
    ...
```

**âš ï¸ IMPORTANTE**:
- NO copies el cÃ³digo completo del README
- Escribe lÃ­nea por lÃ­nea, pensando quÃ© hace cada assert
- Si te atascas, pide ayuda a la IA para UN test especÃ­fico

---

### Tarea 4: Pedir ayuda para UN edge case complejo (Con IA - 10 min)

**Objetivo**: Aprender a pedir ayuda especÃ­fica a la IA para casos difÃ­ciles.

**Escenario**: El edge case de "JSON corrupto" es complejo de testear.

**Prompt efectivo**:

```
AyÃºdame a escribir el test para este caso especÃ­fico:

Edge case: Intentar agregar tarea cuando el archivo JSON estÃ¡ corrupto.

Comportamiento esperado: La funciÃ³n cargar_tareas debe devolver lista vacÃ­a (no crashear).

CÃ³digo actual de cargar_tareas:
[pega la funciÃ³n]

Dame el cÃ³digo pytest para este test, explicando cada parte.
```

**AcciÃ³n TÃš**:

1. Lee el cÃ³digo generado lÃ­nea por lÃ­nea
2. Entiende QUÃ‰ hace cada parte
3. ModifÃ­calo si es necesario para que se ajuste a tu cÃ³digo
4. Ejecuta el test y verifica que pasa

---

### Tarea 5: Validar coverage (VerificaciÃ³n - 10 min)

**Objetivo**: Medir cuÃ¡nto coverage has alcanzado.

**Comando**:

```bash
pytest --cov=. --cov-report=term-missing
```

**Resultado esperado**:

```
Name          Stmts   Miss  Cover   Missing
-------------------------------------------
tareas.py        42      8    81%   23-25, 67, 89-91
test_tareas_pytest.py    28      0   100%
-------------------------------------------
TOTAL            70      8    89%
```

**Preguntas para reflexionar** (anota en `notes.md`):

1. Â¿Llegaste a 80%+?
2. Si NO: Â¿QuÃ© lÃ­neas/funciones faltan? (mira columna "Missing")
3. Â¿Son crÃ­ticas esas lÃ­neas o son casos raros?
4. Â¿Vale la pena testear el 10% restante?

---

## ğŸ“ Entregable

Al final de este ejercicio, debes tener:

### Archivos

1. **test_tareas_pytest.py** con 5+ tests:
   - Al menos 2 escritos 100% manualmente
   - Al menos 1 con ayuda de IA (documentado)

2. **notes.md** con reflexiones completas:
   ```markdown
   # Clase 3 - Clean Code y Testing con IA

   ## Parte 1: DiseÃ±o de Estructura (Manual vs IA)

   ### Mi diseÃ±o inicial (sin IA)
   [Tu estructura manual para inventario]

   ### Sugerencia IA
   [Estructura que propuso la IA]

   ### Feedback Clean Architecture Enforcer
   [QuÃ© te dijo el agente sobre ambas estructuras]

   ### Mi decisiÃ³n final
   [Estructura hÃ­brida que elegiste]
   - TomÃ© de mi diseÃ±o: [...]
   - TomÃ© de IA: [...]
   - RechacÃ© de IA: [...]

   ### Aprendizajes sobre estructura
   - La IA me hizo pensar en: [...]
   - Yo tenÃ­a razÃ³n en: [...]
   - PrÃ³xima vez harÃ©: [...]

   ---

   ## Parte 2: Testing con IA

   ### Edge cases descubiertos con IA
   - [Lista de casos que la IA te sugiriÃ³ y no habÃ­as pensado]

   ### Tests escritos
   - `test_agregar_tarea`: Manual âœ…
   - `test_nombre_vacio`: Manual (con sugerencia de IA para assert)
   - `test_json_corrupto`: Con ayuda de IA (entendÃ­ cada lÃ­nea)

   ### Coverage alcanzado
   - X% (objetivo 80%+)

   ### Aprendizajes sobre testing
   - [QuÃ© aprendiste sobre testing con pytest]
   - [QuÃ© aprendiste sobre usar IA como asistente (no como copiador)]
   ```

### ValidaciÃ³n

```bash
# Todos los tests pasan
pytest test_tareas_pytest.py -v

# Coverage 80%+
pytest --cov=. --cov-report=term-missing --cov-fail-under=80
```

---

## âœ… Criterios de Ã‰xito

Has completado el ejercicio si:

**Parte 1: DiseÃ±o de Estructura**
- [ ] DiseÃ±aste estructura manualmente primero (sin IA)
- [ ] Pediste estructura a IA con prompt estructurado
- [ ] Validaste ambas con Clean Architecture Enforcer
- [ ] Decidiste estructura final hÃ­brida con justificaciÃ³n
- [ ] Documentaste comparaciÃ³n y aprendizajes en notes.md

**Parte 2: Testing con IA**
- [ ] Escribiste al menos 2 tests 100% manual (sin copiar)
- [ ] Usaste IA para descubrir edge cases (lista de casos)
- [ ] Implementaste al menos 1 test con ayuda de IA (entendiendo cada lÃ­nea)
- [ ] Alcanzaste 80%+ coverage
- [ ] Documentaste en notes.md quÃ© aprendiste sobre testing

---

## ğŸš« Antipatrones a Evitar

âŒ **NO HAGAS ESTO**:
- Copiar todos los tests del README sin entenderlos
- Pedir a la IA "genera todos los tests" y copiar sin leer
- Alcanzar 80% sin saber quÃ© estÃ¡s testeando

âœ… **HAZ ESTO**:
- Escribe tests manualmente para interiorizar
- Usa IA para DESCUBRIR casos, no para copiar cÃ³digo
- Entiende cada assert que escribes
- Documenta quÃ© aprendiste

---

## ğŸ“ Siguientes Pasos

Cuando completes este ejercicio, estarÃ¡s listo para:

- **Clase 4**: Testing ampliado (90%+ coverage, parametrizaciÃ³n)
- **TDD con IA**: Escribir tests ANTES de implementar funciones
- **Refactoring seguro**: Cambiar cÃ³digo sabiendo que tests te protegen

---

**Recuerda**: El objetivo no es alcanzar un nÃºmero, es **entender quÃ© estÃ¡s testeando y por quÃ©**.
