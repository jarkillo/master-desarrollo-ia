# Clase 3 â€“ Clean Code y testing inicial

Perfecto. Ya tenemos claras la **Clase 1 (pensamiento computacional + CLI mÃ­nimo)** y la **Clase 2 (fundamentos de programaciÃ³n + persistencia JSON + argparse)**.

La **Clase 3** de este MÃ³dulo 1, segÃºn el plan del mÃ¡ster, toca entrar en dos piezas clave:

- **Clean Code**: cÃ³mo escribir cÃ³digo legible, mantenible y fÃ¡cil de refactorizar. AquÃ­ veremos principios bÃ¡sicos (nombres claros, funciones cortas, separar responsabilidades, evitar duplicaciÃ³n).
- **Testing inicial**: empezar con tests unitarios muy simples, para verificar que nuestro CLI funciona igual despuÃ©s de los refactors.

El objetivo es que tu app de tareas pase de â€œfuncionaâ€ a â€œse entiende y se puede comprobar automÃ¡ticamenteâ€.

---

### Esquema de la Clase 3

**Concepto**

- QuÃ© significa Clean Code y por quÃ© es mÃ¡s importante que â€œhacerlo funcionarâ€.
- Primeros pasos con testing: usar `unittest` en Python para probar funciones como `agregar_tarea`, `listar_tareas` o `completar_tarea`.

**AplicaciÃ³n manual**

- Refactorizar el cÃ³digo del CLI (ya sea tu versiÃ³n manual o la de `argparse`).
- Crear un archivo `test_tareas.py` con un par de pruebas unitarias que verifiquen:
    - Que al agregar se incrementa la lista de tareas.
    - Que al completar, el estado de la tarea cambia a `True`.

**AplicaciÃ³n con IA**

- Prompt para pedirle a la IA que te ayude a aplicar Clean Code (por ejemplo: â€œrefactoriza mis funciones para que no tengan efectos secundarios ocultosâ€).
- Prompt para que genere tests unitarios adicionales y explique quÃ© cubren.

Empecemos:

---

### Clase 3 â€“ Clean Code y Testing inicial

Hasta ahora tu app de tareas **funciona**, pero todavÃ­a tiene ese â€œolorâ€ a cÃ³digo de principiante: comentarios que explican lo obvio, funciones con responsabilidades mezcladas, y nada que garantice que el programa siga funcionando despuÃ©s de un cambio.

Lo que toca hoy es **pasar de â€œfuncionaâ€ a â€œse puede mantenerâ€**.

### 1. Clean Code en pequeÃ±ito

No vamos a estudiar un libro entero de Robert C. Martin, pero sÃ­ algunos principios bÃ¡sicos que te sirven ya:

- **Nombres claros**: variables y funciones que se expliquen solas. `cargar_tareas` se entiende, `ct()` no.
- **Funciones cortas**: que hagan **una sola cosa**. Si tu funciÃ³n hace dos o tres cosas, sepÃ¡rala.
- **Eliminar comentarios innecesarios**: si el cÃ³digo se explica solo, no hace falta comentar â€œabrimos el archivo en modo lecturaâ€. El comentario debe aportar contexto, no repetir lo obvio.
- **Evitar duplicaciÃ³n**: si tienes el mismo bloque de cÃ³digo en dos sitios, conviÃ©rtelo en funciÃ³n.
- **Separar capas**:
    - lÃ³gica de negocio (agregar, completar),
    - acceso a datos (leer/escribir JSON),
    - interfaz (CLI).
        
        Esa separaciÃ³n la IA ya la hizo en la versiÃ³n con `argparse`, pero tÃº debes entenderla.
        

La idea es que cualquier dev (incluido tu "yo del futuro") pueda leer tu cÃ³digo y entenderlo sin un mÃ¡ster en jeroglÃ­ficos.

---

### 2. IA para diseÃ±ar estructura de proyectos

Hasta ahora has visto cÃ³digo limpio a nivel de funciones y variables. Pero **Â¿cÃ³mo organizas carpetas y archivos en un proyecto mÃ¡s grande?**

AquÃ­ es donde la IA se convierte en tu **arquitecto de proyectos**.

#### 2.1. Por quÃ© la estructura importa

Imagina tu cÃ³digo como una casa:
- **Casa sin estructura**: Todo en un salÃ³n gigante (1 archivo con 2000 lÃ­neas)
- **Casa bien organizada**: Cocina, baÃ±o, dormitorios separados (carpetas `api/`, `tests/`, `utils/`)

**Beneficios de buena estructura**:
- âœ… Encuentras cÃ³digo rÃ¡pido ("Â¿dÃ³nde estÃ¡ la lÃ³gica de tareas?" â†’ `servicio_tareas.py`)
- âœ… Tests separados del cÃ³digo principal
- âœ… Puedes trabajar en equipo sin pisarte
- âœ… Proyecto escalable (agregar features sin romper todo)

**Problema**: DiseÃ±ar estructura **antes de tener experiencia** es difÃ­cil. Â¿QuÃ© carpetas crear? Â¿QuÃ© archivos van juntos?

**SoluciÃ³n**: Usar IA como mentor de arquitectura, pero **tÃº decides** si sus sugerencias tienen sentido.

---

#### 2.2. Workflow: Manual â†’ IA â†’ DecisiÃ³n CrÃ­tica

**Paso 1: Define tu proyecto (Manual - 5 min)**

Antes de pedir estructura a la IA, necesitas claridad:

**PregÃºntate**:
- Â¿QuÃ© tipo de proyecto es? (CLI, API, script)
- Â¿QuÃ© componentes principales tiene? (lÃ³gica de tareas, persistencia JSON, interfaz CLI)
- Â¿QuÃ© necesito testear?

**Ejemplo: Tu CLI de tareas**
```
Proyecto: CLI de tareas con persistencia JSON
Componentes:
1. Funciones de negocio (agregar, completar, listar)
2. Lectura/escritura JSON
3. Interfaz CLI (argparse)
4. Tests
```

**Anota esto** antes de pedir nada a la IA. Necesitas saber **quÃ© estÃ¡s construyendo**.

---

**Paso 2: Prompt estructurado a la IA (Con IA - 5 min)**

**NO hagas esto** âŒ:
```
Dame una estructura de carpetas para mi proyecto de tareas
```

**Haz esto** âœ…:
```
Rol: Arquitecto de proyectos Python
Contexto: Tengo un CLI de tareas (agregar/listar/completar) con persistencia JSON.

Componentes:
- LÃ³gica de negocio (funciones agregar_tarea, completar_tarea, listar_tareas)
- Acceso a datos (cargar/guardar JSON)
- Interfaz CLI (argparse)
- Tests (pytest)

Objetivo: PropÃ³n estructura de carpetas y archivos para organizar este proyecto.
Considera: separaciÃ³n de capas, testabilidad, escalabilidad.

NO me generes cÃ³digo aÃºn, solo la estructura de directorios con explicaciÃ³n de quÃ© va en cada carpeta.
```

**La IA responderÃ¡** con algo como:

```
proyecto-tareas/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py              # Interfaz CLI con argparse
â”‚   â”œâ”€â”€ servicio_tareas.py  # LÃ³gica de negocio
â”‚   â””â”€â”€ persistencia.py     # Cargar/guardar JSON
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_servicio.py    # Tests de lÃ³gica
â”‚   â””â”€â”€ test_persistencia.py # Tests de JSON
â”œâ”€â”€ tareas.json             # Datos (en .gitignore)
â”œâ”€â”€ requirements.txt        # Dependencias
â””â”€â”€ README.md               # DocumentaciÃ³n

ExplicaciÃ³n:
- src/: CÃ³digo fuente (lÃ³gica separada de interfaz)
- tests/: Tests aislados
- cli.py: Punto de entrada (importa servicio)
- servicio_tareas.py: Business logic (sin CLI, sin JSON)
- persistencia.py: Solo lectura/escritura (sin lÃ³gica)
```

---

**Paso 3: Validar con criterio (Manual + Clean Architecture Enforcer - 10 min)**

**âš ï¸ IMPORTANTE**: La IA puede sugerir estructura innecesariamente compleja para un proyecto pequeÃ±o.

**TU tarea**: Evaluar crÃ­ticamente cada sugerencia.

**Usa el Clean Architecture Enforcer agent** para validar si la estructura sigue principios sÃ³lidos:

**Prompt para el agente**:
```
Rol: Clean Architecture Enforcer
Contexto: Tengo esta estructura sugerida por IA para mi CLI de tareas:

[Pega la estructura que te dio la IA]

Objetivo: Valida si esta estructura sigue separation of concerns correctamente.
SeÃ±ala si es demasiado compleja para un CLI simple o si hay mejoras.
```

**El agente te dirÃ¡**:
```markdown
## âœ… Estructura VÃ¡lida con PequeÃ±as Mejoras

**Positivos**:
- âœ… SeparaciÃ³n clara: CLI â†’ Servicio â†’ Persistencia
- âœ… Tests aislados
- âœ… Business logic independiente de CLI

**Mejoras sugeridas**:
1. Para un CLI pequeÃ±o, carpeta `src/` puede ser overkill
2. Considera estructura mÃ¡s plana si <200 lÃ­neas de cÃ³digo
3. `persistencia.py` y `servicio_tareas.py` podrÃ­an estar en raÃ­z

**DecisiÃ³n**:
- Si proyecto pequeÃ±o (<3 archivos): Estructura plana OK
- Si planeas crecer: Acepta estructura con src/
```

---

**Paso 4: Decidir TÃš (Manual)**

Ahora que tienes:
1. Tu anÃ¡lisis inicial
2. Sugerencia de IA
3. ValidaciÃ³n del Clean Architecture Enforcer

**TÃš decides** quÃ© estructura usar.

**Criterios de decisiÃ³n**:

| Pregunta | Si SÃ â†’ | Si NO â†’ |
|----------|---------|---------|
| Â¿Proyecto >200 lÃ­neas? | Usa carpeta `src/` | Estructura plana OK |
| Â¿TrabajarÃ¡s en equipo? | Separa bien carpetas | Menos importante |
| Â¿Planeas agregar API despuÃ©s? | Prepara estructura escalable | YAGNI (You Ain't Gonna Need It) |
| Â¿MÃºltiples modos (CLI + Web)? | Separa interfaz de lÃ³gica | CLI Ãºnico OK |

**Ejemplo de decisiÃ³n razonada**:

```markdown
## Mi decisiÃ³n: Estructura plana con separaciÃ³n bÃ¡sica

**RazÃ³n**:
- Proyecto pequeÃ±o (~150 lÃ­neas)
- Solo CLI (no web ni API)
- Solo yo trabajando

**Estructura elegida**:
proyecto-tareas/
â”œâ”€â”€ tareas.py              # LÃ³gica + persistencia juntas (cohesiÃ³n)
â”œâ”€â”€ cli.py                 # Solo interfaz CLI (separada)
â”œâ”€â”€ test_tareas.py         # Tests
â””â”€â”€ tareas.json            # Datos

**JustificaciÃ³n vs sugerencia IA**:
- NO uso src/ â†’ Demasiado para proyecto pequeÃ±o
- NO separo persistencia.py â†’ Solo 2 funciones (cargar/guardar), va bien en tareas.py
- SÃ separo cli.py â†’ FÃ¡cil testear lÃ³gica sin CLI

**CuÃ¡ndo refactorizar**:
- Si proyecto crece >300 lÃ­neas â†’ Crear src/
- Si agrego API â†’ Separar persistencia.py
```

---

#### 2.3. Ejercicio prÃ¡ctico: Manual vs IA (20 min)

**Objetivo**: Comparar estructura diseÃ±ada manualmente vs con ayuda de IA.

**Parte A: DiseÃ±o manual (10 min)**

Sin pedir ayuda a la IA, diseÃ±a estructura para:

**Proyecto**: Sistema de inventario (CLI)
- Agregar productos (nombre, precio, stock)
- Listar productos
- Actualizar stock
- Buscar por nombre
- Persistencia JSON
- Tests

**Tu tarea**: Dibuja en papel o `notes.md`:
```
inventario/
â”œâ”€â”€ ??? (tus carpetas y archivos)
```

**Anota**:
- Â¿QuÃ© archivos creaste?
- Â¿Por quÃ© los separaste asÃ­?
- Â¿QuÃ© dudas tuviste?

---

**Parte B: DiseÃ±o con IA (10 min)**

Ahora pide a la IA:

```
Rol: Arquitecto Python
Proyecto: CLI de inventario (productos con nombre, precio, stock)
Operaciones: CRUD + bÃºsqueda + persistencia JSON + tests

Dame estructura de carpetas/archivos con explicaciÃ³n.
Considera: testabilidad, escalabilidad moderada.
```

**Compara**:
```markdown
## ComparaciÃ³n Manual vs IA

### Mi diseÃ±o:
[Pega tu estructura manual]

### Sugerencia IA:
[Pega estructura de IA]

### Diferencias:
1. IA sugiriÃ³ X, yo no lo habÃ­a pensado â†’ Â¿Tiene sentido?
2. Yo puse Y, IA no lo mencionÃ³ â†’ Â¿Es necesario?
3. Ambos coincidimos en Z â†’ âœ… Buena seÃ±al

### Mi decisiÃ³n final:
[HÃ­brido: toma lo mejor de ambos]

### Aprendizajes:
- IA me hizo pensar en: [aspecto que no habÃ­as considerado]
- Yo tenÃ­a razÃ³n en: [decisiÃ³n mejor que IA]
- PrÃ³xima vez: [quÃ© harÃ­as diferente]
```

---

**Parte C: ValidaciÃ³n con Clean Architecture Enforcer (5 min)**

Toma tu diseÃ±o final hÃ­brido y pide al agente:

```
Rol: Clean Architecture Enforcer
Estructura propuesta para CLI inventario:
[Pega tu estructura final]

Â¿Sigue separation of concerns? Â¿Mejoras?
```

**Incorpora feedback** y documenta en `notes.md`:
```markdown
## Feedback del Clean Architecture Enforcer

**Validaciones pasadas**: âœ…
- [Aspectos correctos]

**Mejoras sugeridas**: âš ï¸
- [Sugerencias del agente]

**Aplicadas**: âœ…
- [QuÃ© cambios hiciste]

**Rechazadas**: âŒ
- [QuÃ© no aplicaste y por quÃ©]
```

---

#### 2.4. Antipatrones comunes que la IA detecta

**AntipatrÃ³n 1: Todo en un archivo**

âŒ **Problema**:
```
inventario.py  # 800 lÃ­neas: CLI + lÃ³gica + persistencia + tests
```

âœ… **SoluciÃ³n** (IA te sugiere):
```
inventario/
â”œâ”€â”€ servicio.py     # LÃ³gica
â”œâ”€â”€ persistencia.py # JSON
â”œâ”€â”€ cli.py          # Interfaz
â””â”€â”€ tests/
```

---

**AntipatrÃ³n 2: Nombres genÃ©ricos**

âŒ **Problema**:
```
src/
â”œâ”€â”€ utils.py        # Â¿QuÃ© utils? 500 funciones mezcladas
â”œâ”€â”€ helpers.py      # Â¿Helpers de quÃ©?
â””â”€â”€ manager.py      # Â¿QuÃ© gestiona?
```

âœ… **SoluciÃ³n** (Clean Architecture Enforcer detecta):
```
src/
â”œâ”€â”€ validacion_productos.py  # Nombres especÃ­ficos
â”œâ”€â”€ formato_salida.py
â””â”€â”€ servicio_inventario.py
```

**LecciÃ³n**: Nombres especÃ­ficos > nombres genÃ©ricos.

---

**AntipatrÃ³n 3: Tests mezclados con cÃ³digo**

âŒ **Problema**:
```
src/
â”œâ”€â”€ servicio.py
â”œâ”€â”€ test_servicio.py  # Tests mezclados con cÃ³digo
â””â”€â”€ persistencia.py
```

âœ… **SoluciÃ³n**:
```
src/
â”œâ”€â”€ servicio.py
â””â”€â”€ persistencia.py

tests/              # Tests separados
â”œâ”€â”€ test_servicio.py
â””â”€â”€ test_persistencia.py
```

**RazÃ³n**: Tests son "documentaciÃ³n ejecutable", no parte del cÃ³digo productivo.

---

#### 2.5. CuÃ¡ndo confiar en IA vs tu criterio

**ConfÃ­a en IA cuando**:
âœ… Sugiere separaciÃ³n clara de responsabilidades
âœ… Propone estructura estÃ¡ndar de la industria (src/, tests/, docs/)
âœ… Detecta antipatrones que no conocÃ­as (God Object, duplicaciÃ³n)
âœ… Nombra cosas especÃ­ficamente (servicio_X, repositorio_Y)

**Usa tu criterio cuando**:
âš ï¸ IA sugiere estructura muy compleja para proyecto pequeÃ±o
âš ï¸ Propone carpetas que no necesitas aÃºn (YAGNI)
âš ï¸ Usa patterns avanzados para casos simples (Factory, Strategy en CLI de 100 lÃ­neas)
âš ï¸ No considera tu contexto especÃ­fico (equipo, plazos, experiencia)

**Regla de oro**:
> "La IA te da la estructura ideal. TÃº decides quÃ© parte necesitas HOY y quÃ© DESPUÃ‰S."

---

#### 2.6. Checklist: Â¿Es buena estructura?

Usa este checklist para evaluar cualquier estructura (tuya, de IA, o hÃ­brida):

**SeparaciÃ³n de capas**:
- [ ] Interfaz (CLI) separada de lÃ³gica
- [ ] LÃ³gica de negocio sin dependencias de interfaz
- [ ] Persistencia aislada (cambiable de JSON a DB sin tocar lÃ³gica)

**Testabilidad**:
- [ ] Tests en carpeta separada
- [ ] Puedes testear lÃ³gica sin ejecutar CLI
- [ ] Fixtures/mocks fÃ¡ciles de crear

**Escalabilidad**:
- [ ] FÃ¡cil agregar nueva operaciÃ³n sin tocar todo
- [ ] Nombres claros (sÃ© quÃ© hay en cada archivo sin abrirlo)
- [ ] No God Objects (archivos >300 lÃ­neas son sospechosos)

**Pragmatismo**:
- [ ] No mÃ¡s complejo de lo necesario
- [ ] Estructura justificada para tamaÃ±o del proyecto
- [ ] FÃ¡cil de navegar (no 10 niveles de carpetas)

---

### 3. IntroducciÃ³n al testing

Testing no es magia. En Python, lo mÃ­nimo se hace con `unittest`.

Un **test unitario** es solo una funciÃ³n que comprueba que otra funciÃ³n hace lo que esperamos.

Ejemplo sencillo:

```python
import unittest
from tareas import agregar_tarea, cargar_tareas, guardar_tareas

class TestTareas(unittest.TestCase):

    def setUp(self):
        # Preparamos un archivo temporal vacÃ­o antes de cada test
        self.archivo = "test_tareas.json"
        guardar_tareas(self.archivo, [])

    def test_agregar_tarea(self):
        agregar_tarea(self.archivo, "Estudiar IA")
        tareas = cargar_tareas(self.archivo)
        self.assertEqual(len(tareas), 1)
        self.assertEqual(tareas[0]["nombre"], "Estudiar IA")
        self.assertFalse(tareas[0]["completada"])

if __name__ == "__main__":
    unittest.main()
```

Con esto, cuando corras:

```bash
python -m unittest test_tareas.py
```

Ves si tu funciÃ³n hace lo correcto.

---

### 2.1. Â¿Por quÃ© pytest en vez de unittest?

El ejemplo anterior usa `unittest`, que es la librerÃ­a estÃ¡ndar de Python. Funciona, pero tiene algunas limitaciones:

- **Verboso**: Requiere clases, `self.`, y mÃ©todos con nombres largos (`assertEqual`, `assertTrue`)
- **Setup pesado**: `setUp` y `tearDown` son menos flexibles que fixtures
- **Sin parametrizaciÃ³n**: Tests similares requieren copiar-pegar cÃ³digo

**pytest** es la alternativa moderna que la industria prefiere:

```python
# unittest (viejo)
class TestTareas(unittest.TestCase):
    def setUp(self):
        self.archivo = "test_tareas.json"

    def test_agregar(self):
        self.assertEqual(resultado, esperado)

# pytest (moderno)
@pytest.fixture
def archivo():
    return "test_tareas.json"

def test_agregar(archivo):
    assert resultado == esperado  # MÃ¡s simple!
```

**Ventajas de pytest**:

âœ… **Asserts simples**: `assert x == y` en vez de `self.assertEqual(x, y)`
âœ… **Fixtures reutilizables**: Setup compartido entre tests
âœ… **ParametrizaciÃ³n**: Un test para mÃºltiples casos
âœ… **Mejor output**: Muestra diferencias claras cuando falla
âœ… **Plugins**: pytest-cov, pytest-asyncio, pytest-mock
âœ… **Menos boilerplate**: No necesitas clases ni heredar de TestCase

**InstalaciÃ³n**:
```bash
pip install pytest pytest-cov
```

**EjecuciÃ³n**:
```bash
# Ejecutar todos los tests
pytest

# Ejecutar con verbose
pytest -v

# Con coverage
pytest --cov=. --cov-report=term-missing
```

**DecisiÃ³n pedagÃ³gica**: A partir de ahora usaremos **pytest** porque es:
1. Lo que usan equipos profesionales (Google, Spotify, Netflix)
2. MÃ¡s simple de leer y escribir
3. Mejor integraciÃ³n con herramientas de IA (Claude genera cÃ³digo pytest mÃ¡s limpio)

---

### 2.2. Ejemplo con pytest (migrando desde unittest)

Vamos a reescribir el test anterior usando pytest:

```python
# test_tareas_pytest.py
import os
import tempfile
import pytest
from tareas import agregar_tarea, cargar_tareas, guardar_tareas


@pytest.fixture
def archivo_temporal():
    """
    Fixture que crea un archivo temporal para tests.
    Se ejecuta antes de cada test y limpia despuÃ©s.
    """
    fd, tmp = tempfile.mkstemp(prefix="tareas_", suffix=".json")
    os.close(fd)
    guardar_tareas(tmp, [])  # Empezamos con archivo vacÃ­o
    yield tmp  # El test usa este archivo
    os.remove(tmp)  # Limpieza automÃ¡tica


def test_agregar_tarea(archivo_temporal):
    """Test: Agregar una tarea con nombre vÃ¡lido."""
    agregar_tarea(archivo_temporal, "Estudiar IA")
    tareas = cargar_tareas(archivo_temporal)

    # Asserts simples (no self.assertEqual)
    assert len(tareas) == 1
    assert tareas[0]["nombre"] == "Estudiar IA"
    assert tareas[0]["completada"] == False


def test_completar_tarea(archivo_temporal):
    """Test: Completar una tarea existente."""
    tarea = agregar_tarea(archivo_temporal, "Repasar Git")
    ok = completar_tarea(archivo_temporal, tarea["id"])

    assert ok == True

    tareas = cargar_tareas(archivo_temporal)
    assert tareas[0]["completada"] == True
```

**Diferencias clave**:

| unittest | pytest |
|----------|--------|
| `class TestTareas(unittest.TestCase):` | Funciones sueltas (no clase) |
| `def setUp(self):` | `@pytest.fixture` reutilizable |
| `self.assertEqual(a, b)` | `assert a == b` |
| `self.archivo` (estado en clase) | Fixtures como parÃ¡metros |
| `python -m unittest` | `pytest` |

**ConclusiÃ³n**: pytest es mÃ¡s Pythonic y mÃ¡s fÃ¡cil de usar con IA.

---

### 3. Generar tests con IA (Workflow Manual â†’ IA â†’ ValidaciÃ³n)

**Problema**: Escribir tests desde cero es lento y es fÃ¡cil olvidar casos importantes (edge cases, error handling).

**SoluciÃ³n**: Usar el **Test Coverage Strategist agent** para descubrir quÃ© testear, luego TÃš escribes los tests.

#### Paso 1: Definir quÃ© testear (Manual)

Antes de pedirle nada a la IA, necesitas claridad:

**PregÃºntate**:
- Â¿QuÃ© funciÃ³n voy a testear? (ej. `agregar_tarea`)
- Â¿QuÃ© debe hacer en el caso feliz? (agregar tarea correctamente)
- Â¿QuÃ© puede salir mal? (nombre vacÃ­o, archivo no existe, JSON corrupto)

**Anota tu lista inicial** (la que se te ocurra):
```
- Agregar tarea con nombre vÃ¡lido
- Agregar tarea cuando archivo no existe
- Â¿QuÃ© mÃ¡s...?
```

#### Paso 2: Prompt estructurado a la IA

**NO hagas esto** âŒ:
```
Dame tests para agregar_tarea
```

**Haz esto** âœ…:
```
Rol: Test Coverage Strategist
Contexto: Tengo una funciÃ³n agregar_tarea(ruta, nombre) que guarda tareas en JSON.

CÃ³digo actual:
[pega la funciÃ³n agregar_tarea de tareas.py]

Objetivo: Lista de casos de prueba que deberÃ­a testear (NO el cÃ³digo de tests aÃºn).
Incluye: happy path, edge cases, error handling.
Categoriza por criticidad (Alta/Media/Baja).
```

**La IA responderÃ¡** con una LISTA de casos (no cÃ³digo):

```markdown
## Edge Cases para agregar_tarea

### Criticidad ALTA:
1. Happy path: Agregar tarea con nombre vÃ¡lido
2. Nombre vacÃ­o ("") - debe rechazar o usar default
3. Archivo JSON corrupto - debe manejar error sin crashear

### Criticidad MEDIA:
4. Nombre con solo espacios ("   ")
5. Persistencia: Agregar, cerrar, reabrir archivo
6. MÃºltiples tareas consecutivas (IDs correctos)

### Criticidad BAJA:
7. Nombre muy largo (1000+ caracteres)
8. Caracteres especiales en nombre (\n, \t, emojis)
```

#### Paso 3: Revisar y validar (Manual)

**TÃš decides**:
- âœ… Â¿Tiene sentido cada caso?
- âœ… Â¿Falta alguno crÃ­tico que la IA no mencionÃ³?
- âœ… Â¿Alguno es redundante o innecesario?

**Anota en notes.md**:
```markdown
## Edge cases descubiertos con IA (Clase 3)

Casos que YO habÃ­a pensado:
- Agregar tarea vÃ¡lida
- Archivo no existe

Casos que la IA sugiriÃ³ (NO se me habÃ­an ocurrido):
- JSON corrupto â† CRÃTICO
- Nombre con solo espacios â† Importante
- Caracteres especiales â† Interesante pero baja prioridad
```

#### Paso 4: Pedir cÃ³digo de UN test como plantilla (IA)

Ahora sÃ­, pide el cÃ³digo de **un solo test** como ejemplo:

```
Genera el cÃ³digo pytest para el caso 1: agregar tarea con nombre vÃ¡lido.
Usa fixture temporal para archivo JSON.
Incluye comentarios explicativos.
```

**La IA generarÃ¡**:
```python
def test_agregar_tarea_nombre_valido(archivo_temporal):
    """
    Test happy path: Agregar tarea con nombre vÃ¡lido.
    Verifica que se crea con ID, nombre y estado inicial correcto.
    """
    # Act: Agregar tarea
    tarea = agregar_tarea(archivo_temporal, "Estudiar IA")

    # Assert: Verificar estructura y valores
    assert tarea["id"] == 1
    assert tarea["nombre"] == "Estudiar IA"
    assert tarea["completada"] == False

    # Assert: Verificar persistencia
    tareas = cargar_tareas(archivo_temporal)
    assert len(tareas) == 1
    assert tareas[0] == tarea
```

#### Paso 5: Entender y adaptar (Manual)

**âš ï¸ IMPORTANTE**: NO copies y pegues todo.

**Haz esto**:
1. Lee el cÃ³digo generado **lÃ­nea por lÃ­nea**
2. Entiende quÃ© hace cada assert
3. ModifÃ­calo si es necesario para tu implementaciÃ³n
4. Ejecuta el test: `pytest test_tareas_pytest.py::test_agregar_tarea_nombre_valido -v`

#### Paso 6: Escribir los demÃ¡s tests TÃš MISMO

**Usando la plantilla anterior**, escribe los tests de criticidad Alta:

```python
def test_agregar_tarea_nombre_vacio(archivo_temporal):
    """
    Edge case: Nombre vacÃ­o debe ser rechazado.
    """
    # Â¿QuÃ© deberÃ­a pasar?
    # OpciÃ³n A: Lanzar ValueError
    # OpciÃ³n B: Usar nombre default "Sin tÃ­tulo"

    # TÃš decides el comportamiento esperado
    with pytest.raises(ValueError, match="Nombre no puede estar vacÃ­o"):
        agregar_tarea(archivo_temporal, "")


def test_agregar_tarea_json_corrupto(archivo_temporal):
    """
    Edge case: Si JSON corrupto, no debe crashear.
    cargar_tareas deberÃ­a devolver lista vacÃ­a.
    """
    # Arrange: Escribir JSON invÃ¡lido
    with open(archivo_temporal, 'w') as f:
        f.write("{esto no es json vÃ¡lido")

    # Act: Intentar agregar (internamente llama cargar_tareas)
    tarea = agregar_tarea(archivo_temporal, "Nueva tarea")

    # Assert: DeberÃ­a funcionar (lista empieza desde vacÃ­o)
    assert tarea["id"] == 1
    tareas = cargar_tareas(archivo_temporal)
    assert len(tareas) == 1
```

**Regla de oro**: Escribe al menos **2-3 tests manualmente** para interiorizar el patrÃ³n. Usa IA solo para casos muy complejos o para validar tu cÃ³digo.

---

### 4. Edge cases con IA (20 min)

**Concepto**: Los edge cases (casos extremos) son situaciones raras pero reales que rompen tu cÃ³digo si no las previenes.

**Ejemplos**:
- Nombre vacÃ­o
- Archivo JSON corrupto
- ID negativo o inexistente
- Concurrencia (dos procesos escriben al mismo tiempo)

**Problema**: Es difÃ­cil pensar en TODOS los edge cases sin experiencia.

**SoluciÃ³n**: El Test Coverage Strategist agent estÃ¡ entrenado para identificarlos.

#### Ejercicio prÃ¡ctico: Edge cases de `completar_tarea`

**Tu tarea**: Pide a la IA edge cases para `completar_tarea(ruta, id)`.

**Prompt**:
```
Rol: Test Coverage Strategist
FunciÃ³n: completar_tarea(ruta_archivo, id_tarea)

CÃ³digo:
[pega la funciÃ³n]

Objetivo: Lista de edge cases que deberÃ­a testear.
Categoriza por criticidad (Alta/Media/Baja).
```

**Resultado esperado**:
```markdown
### Criticidad ALTA:
1. ID inexistente (devuelve False, no crashea)
2. ID negativo (validaciÃ³n de input)
3. Archivo JSON corrupto al leer

### Criticidad MEDIA:
4. ID como string "1" en vez de int 1
5. Completar tarea ya completada (idempotencia)

### Criticidad BAJA:
6. ID = 0 (edge numÃ©rico)
7. ID muy grande (999999)
```

**TU tarea**:
1. âœ… Implementa tests para criticidad ALTA (TÃš MISMO)
2. âœ… Usa IA solo si te atascas en cÃ³mo escribir el assert
3. âœ… Documenta en notes.md cuÃ¡les implementaste y por quÃ©

**Ejemplo de test de criticidad Alta**:
```python
def test_completar_tarea_id_inexistente(archivo_temporal):
    """
    Edge case crÃ­tico: Completar tarea que no existe.
    Debe devolver False sin crashear.
    """
    agregar_tarea(archivo_temporal, "Tarea 1")  # ID = 1

    # Intentar completar ID que no existe
    resultado = completar_tarea(archivo_temporal, 999)

    assert resultado == False

    # Verificar que la tarea 1 no se modificÃ³
    tareas = cargar_tareas(archivo_temporal)
    assert tareas[0]["completada"] == False
```

---

### 3. El ciclo "Refactor con seguridad"

- **Primero** escribes o ajustas un test.
- **Luego** haces un cambio en el cÃ³digo para limpiarlo.
- **DespuÃ©s** corres los tests.
- Si todo sigue verde âœ…, sabes que el refactor no rompiÃ³ nada.

Esto es la semilla de **TDD (Test Driven Development)**, pero de momento con que uses tests para **validar tus refactors** es suficiente.

---

### 5. Ejercicio prÃ¡ctico: Testing con IA (80%+ coverage)

**ğŸ“‹ Consulta el ejercicio completo**: Ver `ejercicio_clase3_ai.md` en esta carpeta.

**Resumen del flujo**:

1. **Manual** (15 min): Escribe `test_agregar_tarea` (happy path) sin copiar
2. **Con IA** (10 min): Pide al Test Coverage Strategist lista de edge cases
3. **Manual** (20 min): Implementa 2-3 edge cases de criticidad ALTA tÃº mismo
4. **Con IA** (10 min): Pide ayuda para UN edge case complejo (ej. JSON corrupto)
5. **ValidaciÃ³n** (10 min): Ejecuta `pytest --cov=. --cov-report=term-missing`

**ğŸ¯ Objetivo**: Alcanzar 80%+ coverage usando pytest y Test Coverage Strategist.

**ğŸ“ Entregables**:
- `test_tareas_pytest.py` con 5+ tests
- `notes.md` documentando: edge cases descubiertos con IA, tests escritos manual vs con ayuda
- Coverage 80%+

**âš ï¸ Regla de oro**:
- Escribe al menos 2-3 tests 100% manual (interiorizar patrÃ³n)
- Usa IA para DESCUBRIR casos, no para copiar cÃ³digo sin entender
- Documenta quÃ© aprendiste en `notes.md`

---

### 6. AplicaciÃ³n con IA (Refactoring con Python Best Practices Coach)

AdemÃ¡s del Test Coverage Strategist para tests, puedes usar el **Python Best Practices Coach** para limpiar tu cÃ³digo.

**Prompt ejemplo**:
```
Rol: Python Best Practices Coach
Contexto: Tengo el cÃ³digo de tareas.py funcionando pero quiero hacerlo mÃ¡s Pythonic.

CÃ³digo:
[pega tareas.py]

Objetivo: Identifica anti-patterns y sugiere mejoras (f-strings, type hints, pathlib, etc.).
No cambies funcionalidad, solo limpia el cÃ³digo.
```

**El agente te dirÃ¡**:
- Usa f-strings en vez de concatenaciÃ³n
- AÃ±ade type hints a funciones
- Usa Pathlib en vez de os.path
- Reemplaza loops manuales por list comprehensions

**Tu tarea**:
1. Lee las sugerencias del coach
2. Aplica las que entiendas
3. Ejecuta tests despuÃ©s de cada cambio (seguridad con tests!)
4. Si alguna sugerencia no la entiendes, pide explicaciÃ³n

**Beneficio del workflow**:
```
Tests escritos âœ… â†’ Refactor cÃ³digo con IA âœ… â†’ Tests siguen pasando âœ…
```

Sin tests, refactorizar da miedo (Â¿rompÃ­ algo?). Con tests, refactorizas confiado.

---

Lo que buscamos aquÃ­ no es que seas un fanÃ¡tico de "Clean Code", sino que empieces a **oler cuÃ¡ndo tu cÃ³digo puede mejorar** y uses los tests como tu red de seguridad, con la IA como asistente educativo (no como copiador automÃ¡tico).

---

Aqui dejo el [tareas.py](http://tareas.py) refactorizado, con capas separadas, argparse, los tests, y todo explicado:

## Meta-objetivo de hoy

1. Mantener el comportamiento. 2) Hacer el cÃ³digo mÃ¡s legible. 3) Prepararlo para testear sin miedo.
    
    Estrategia: pequeÃ±os cambios â†’ ejecutar â†’ comprobar.
    

## Paso 0 â€” PequeÃ±o â€œcambio de mentalidadâ€

- Comentarios que repiten lo obvio (tipo â€œabrimos archivo en lecturaâ€) **se van**. (ya los explicamos en la clase anterior)
- Comentarios que explican una **decisiÃ³n** o **convenciÃ³n** **se quedan**.
- Nombres y estructura deben contar la historia: funciones cortas y con **una sola responsabilidad**.

## Paso 1 â€” Extraer constantes y tipar un poco

La idea: nombres claros + una mÃ­nima pista de tipos (para lector humano y tu editor/IA).

```sql
# tareas.py

from typing import TypedDict, List
import json, os, sys

ARCHIVO_POR_DEFECTO = "tareas.json"

class Tarea(TypedDict):
    id: int
    nombre: str
    completada: bool

```

**Por quÃ©**: `ARCHIVO_POR_DEFECTO` deja claro el contrato del programa. `Tarea` te da estructura mental (y autocompletado).

Si nunca has visto una clase, seguro que te acaba de explotar la cabeza, no te preocupes, vamos a explicarlo:

### QuÃ© es eso de `class Tarea(TypedDict)`

- En Python, una **clase** (`class`) normalmente sirve para definir **objetos** (con atributos y mÃ©todos).

- Ejemplo tÃ­pico:

```python
class Perro:
    def __init__(self, nombre):
        self.nombre = nombre

    def ladrar(self):
        print("Guau!")

```

Pero aquÃ­ **no hemos creado un objeto** nuevo. Lo que usamos es algo mÃ¡s ligero: **TypedDict**.

### QuÃ© es `TypedDict`

- `TypedDict` viene del mÃ³dulo `typing`.
- Sirve solo para **documentar** quÃ© claves y tipos tiene un diccionario.
- No cambia cÃ³mo corre tu programa; es una **pista para humanos y para el editor/IA**.

Ejemplo:

```python
from typing import TypedDict

class Tarea(TypedDict):
    id: int
    nombre: str
    completada: bool
```

Esto equivale a decir: â€œuna tarea es un diccionario con tres claves (`id`, `nombre`, `completada`) y esos tipos de datosâ€.

En tiempo de ejecuciÃ³n sigue siendo un `dict` normal:

```python
t: Tarea = {"id": 1, "nombre": "Estudiar IA", "completada": False}
print(t["nombre"])  # funciona igual que un dict

```

### Â¿Por quÃ© te lo metÃ­ ahÃ­?

Porque en **Clean Code** es Ãºtil que cualquiera que lea tu funciÃ³n vea enseguida quÃ© estructura manejas.

Ejemplo:

```python
def agregar_tarea(ruta: str, nombre: str) -> Tarea:
    ...
```

Ese `-> Tarea` le dice al lector: â€œesto devuelve un diccionario con esas tres clavesâ€.

### Â¿QuÃ© hacer tÃº ahora?

- Si todavÃ­a no quieres liarte con clases ni `TypedDict`, puedes **borrarlo** y quedarte solo con `dict`.
- Ejemplo versiÃ³n sin `class`:

```python
def agregar_tarea(ruta: str, nombre: str) -> dict:
    ...

```

El programa funciona igual.

Cuando te sientas cÃ³modo, puedes aÃ±adir `TypedDict` como un **truco de legibilidad**.

## Paso 2 â€” Capa de acceso a datos limpia (I/O)

Quitamos comentarios redundantes y hacemos funciones cortas. Mantengo un comentario contextual donde aporta.

```python
def cargar_tareas(ruta: str) -> List[Tarea]:
    """Devuelve la lista de tareas. Si no hay archivo o estÃ¡ vacÃ­o/corrupto, lista vacÃ­a."""
    if not os.path.exists(ruta):
        return []
    try:
        with open(ruta, "r", encoding="utf-8") as f:
            contenido = f.read().strip()
            return json.loads(contenido) if contenido else []
    except json.JSONDecodeError:
        # DecisiÃ³n: no romper por JSON corrupto; devolvemos vacÃ­o para que el CLI siga siendo usable.
        return []

def guardar_tareas(ruta: str, tareas: List[Tarea]) -> None:
    with open(ruta, "w", encoding="utf-8") as f:
        json.dump(tareas, f, ensure_ascii=False, indent=2)

```

## Paso 3 â€” LÃ³gica de dominio pequeÃ±a y directa

- **Una funciÃ³n = una acciÃ³n**.
- IDs robustos con `max(id)+1` (por si en el futuro borras).

```python
def nuevo_id(tareas: List[Tarea]) -> int:
    return 1 if not tareas else max(t["id"] for t in tareas) + 1

def agregar_tarea(ruta: str, nombre: str) -> Tarea:
    tareas = cargar_tareas(ruta)
    tarea: Tarea = {"id": nuevo_id(tareas), "nombre": nombre, "completada": False}
    tareas.append(tarea)
    guardar_tareas(ruta, tareas)
    return tarea  # devolver la entidad ayuda en tests

def listar_tareas(ruta: str) -> List[Tarea]:
    return cargar_tareas(ruta)

def completar_tarea(ruta: str, id_tarea: int) -> bool:
    tareas = cargar_tareas(ruta)
    for t in tareas:
        if t["id"] == id_tarea:
            t["completada"] = True
            guardar_tareas(ruta, tareas)
            return True
    return False  # no encontrada

```

FÃ­jate que ahora **no imprimimos** dentro de la lÃ³gica: devolvemos datos. Imprimir es cosa del CLI. Esto facilita los **tests**.

## Paso 4 â€” Interfaz mÃ­nima (CLI) con `sys.argv` (todavÃ­a)

Seguimos con tu estilo de la clase anterior, solo **usamos las funciones nuevas**. Esto mantiene tu aprendizaje y te prepara para `argparse` sin romper nada.

```python
def uso():
    print("Uso: python tareas.py <listar|agregar|completar> [args]")
    sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        uso()

    comando = sys.argv[1]
    archivo = ARCHIVO_POR_DEFECTO  # podrÃ­as aÃ±adir mÃ¡s tarde -f <ruta>

    if comando == "listar":
        for t in listar_tareas(archivo):
            estado = "âœ…" if t["completada"] else "âŒ"
            print(f"{t['id']}. {t['nombre']} {estado}")

    elif comando == "agregar":
        if len(sys.argv) < 3:
            print("Falta el nombre de la tarea"); sys.exit(1)
        tarea = agregar_tarea(archivo, sys.argv[2])
        print(f"Agregada: {tarea['id']}. {tarea['nombre']}")

    elif comando == "completar":
        if len(sys.argv) < 3:
            print("Falta el ID de la tarea"); sys.exit(1)
        ok = completar_tarea(archivo, int(sys.argv[2]))
        print("Completada âœ…" if ok else "No encontrada âŒ")

    else:
        print("Comando no reconocido"); uso()

```

**QuÃ© cambiÃ³ vs tu versiÃ³n**

- La lÃ³gica ya **no imprime** ni lee `sys.argv` â†’ **separaciÃ³n de capas** real.
- Las funciones **devuelven datos** (Ãºtil para tests y para futuras UIs).
- Los comentarios se reducen a **decisiones** y **contratos**.

## Paso 5 â€” Tests iniciales (sin magia)

Creamos `test_tareas.py`. Usamos un **archivo temporal** para no tocar tu `tareas.json`.

```python
# test_tareas.py
import os, tempfile, unittest
from tareas import guardar_tareas, cargar_tareas, agregar_tarea, completar_tarea, listar_tareas

class TestTareas(unittest.TestCase):
    def setUp(self):
        fd, self.tmp = tempfile.mkstemp(prefix="tareas_", suffix=".json")
        os.close(fd)          # cerramos el descriptor; usaremos la ruta
        guardar_tareas(self.tmp, [])  # empezamos limpio

    def tearDown(self):
        os.remove(self.tmp)

    def test_agregar_tarea(self):
        agregar_tarea(self.tmp, "Estudiar IA")
        tareas = cargar_tareas(self.tmp)
        self.assertEqual(len(tareas), 1)
        self.assertEqual(tareas[0]["nombre"], "Estudiar IA")
        self.assertFalse(tareas[0]["completada"])

    def test_completar_tarea(self):
        t = agregar_tarea(self.tmp, "Repasar Git")
        ok = completar_tarea(self.tmp, t["id"])
        self.assertTrue(ok)
        tareas = listar_tareas(self.tmp)
        self.assertTrue(tareas[0]["completada"])

    def test_completar_inexistente(self):
        guardar_tareas(self.tmp, [])
        ok = completar_tarea(self.tmp, 999)
        self.assertFalse(ok)

if __name__ == "__main__":
    unittest.main()

```

Ejecutas:

```bash
python -m unittest -v

```

**QuÃ© aprendes aquÃ­**

- `setUp/tearDown`: cada test empieza/termina limpio.
- Las funciones devuelven valores Ãºtiles â†’ tests simples y claros.
- Nada de mocks exÃ³ticos todavÃ­a: **archivito temporal y a correr**.

## Paso 5.1. - ExplicaciÃ³n del Test

Antes de ver`test_tareas.py` tenemos que explicarlo por si **nunca en tu vida has escrito un test**

y lo que tienes delante es como un ritual extraÃ±o.

Vamos a verlo como un experimento de laboratorio: probamos una funciÃ³n â†’ vemos si hace lo que prometiÃ³.

### Â¿QuÃ© es un test?

Un **test** no es mÃ¡s que un mini-programa que llama a tus funciones y comprueba que el resultado sea correcto.

Es como si dijeras: *â€œoye funciÃ³n, te doy esto y tendrias que devolverme estoâ€¦ al pasar el test comprobamos que el resultado sea correctoâ€*.

Seguramente alguno habreis visto test con pytest, esto lo veremos mÃ¡s adelante, de momento usaremos unittest que es mucho mas simple.

### El archivo `test_tareas.py`

Vamos a destripar el `test_tareas.py` lÃ­nea a lÃ­nea, como si estuviÃ©ramos juntos leyÃ©ndolo y entendiendo quÃ© hace cada trocito. AsÃ­ te quedas con la **plantilla mental** para cualquier test futuro.

### Paso 1 â€“ Importamos cosas

```python
import os, tempfile, unittest
from tareas import guardar_tareas, cargar_tareas, agregar_tarea, completar_tarea, listar_tareas
```

- `unittest` â†’ librerÃ­a de Python para escribir tests (ya viene instalada).
- `tempfile` â†’ nos deja crear un archivo temporal, asÃ­ no tocamos tu `tareas.json` real.
- Importamos las funciones que vamos a probar (`agregar_tarea`, `completar_tarea`, etc.).

### Paso 2 â€“ Creamos una â€œcaja de pruebasâ€

```python
class TestTareas(unittest.TestCase):
```

AquÃ­ juntamos todas las pruebas. Piensa que es como un cuaderno: cada mÃ©todo dentro es una comprobaciÃ³n.

### Paso 3 â€“ Preparar y limpiar

```python
    def setUp(self):
        fd, self.tmp = tempfile.mkstemp(prefix="tareas_", suffix=".json")
        os.close(fd)
        guardar_tareas(self.tmp, [])

    def tearDown(self):
        os.remove(self.tmp)
```

- `setUp` â†’ se ejecuta **antes de cada prueba**. Crea un archivo vacÃ­o.
- `tearDown` â†’ se ejecuta **despuÃ©s de cada prueba**. Borra el archivo.
    
    AsÃ­ cada prueba empieza desde cero, sin basura de pruebas anteriores.
    

### Paso 4 â€“ Una prueba sencilla

```python
    def test_agregar_tarea(self):
        agregar_tarea(self.tmp, "Estudiar IA")
        tareas = cargar_tareas(self.tmp)
        self.assertEqual(len(tareas), 1)
        self.assertEqual(tareas[0]["nombre"], "Estudiar IA")
        self.assertFalse(tareas[0]["completada"])
```

TraducciÃ³n a lenguaje humano:

1. Agrego una tarea llamada â€œEstudiar IAâ€.
2. Vuelvo a leer el archivo.
3. Compruebo tres cosas:
    - La lista tiene 1 tarea.
    - El nombre es el correcto.
    - La tarea no estÃ¡ completada.

### Paso 5 â€“ Otra prueba

```python
    def test_completar_tarea(self):
        t = agregar_tarea(self.tmp, "Repasar Git")
        ok = completar_tarea(self.tmp, t["id"])
        self.assertTrue(ok)
        tareas = listar_tareas(self.tmp)
        self.assertTrue(tareas[0]["completada"])

```

AquÃ­ lo que hacemos es:

1. Crear una tarea.
2. Llamar a `completar_tarea`.
3. Comprobar que la funciÃ³n nos dijo â€œsÃ­, la completÃ©â€ (`ok = True`).
4. Ver que efectivamente en la lista aparece como completada.

### Paso 6 â€“ Caso raro

```python
    def test_completar_inexistente(self):
        guardar_tareas(self.tmp, [])
        ok = completar_tarea(self.tmp, 999)
        self.assertFalse(ok)
```

AquÃ­ probamos quÃ© pasa si intentas completar una tarea que **no existe**.

Esperamos que la funciÃ³n devuelva `False` (fallo controlado).

### Paso 7 â€“ Lanzar los tests

```python
if __name__ == "__main__":
    unittest.main()
```

Esto hace que si ejecutas el archivo, todos los tests se corran automÃ¡ticamente.

En la terminal:

```bash
python -m unittest -v
```

VerÃ¡s algo como:

```
test_agregar_tarea ... ok
test_completar_tarea ... ok
test_completar_inexistente ... ok
```

Si algo falla, Python te dirÃ¡ cuÃ¡l test y por quÃ©.

---

### Resumen mental

- **Cada test = una pequeÃ±a historia**: â€œsi hago X, espero Yâ€.
- `assertEqual`, `assertTrue`, `assertFalse` son formas de decir *â€œcomprueba que esto sea asÃ­â€*.
- Usamos un **archivo temporal** para que no rompas tu app real al probar.

## Paso 8 â€” ComprobaciÃ³n de lista vacia

Vamos a escribir un **test para `listar_tareas`** en el caso de que la lista estÃ© vacÃ­a y tambiÃ©n cuando tenga varias tareas. AsÃ­ te queda claro cÃ³mo comprobar listas. 

No lo copies, intenta hacerlo tu primero, y cuando estes muy atascado y ya no se te ocurra, miras la soluciÃ³n.

### Caso 1: lista vacÃ­a

```python
    def test_listar_vacio(self):
        # Arrancamos con el archivo vacÃ­o
        guardar_tareas(self.tmp, [])

        tareas = listar_tareas(self.tmp)

        # La lista debe estar vacÃ­a
        self.assertEqual(tareas, [])
```

TraducciÃ³n:

- Guardo explÃ­citamente una lista vacÃ­a en el archivo temporal.
- Llamo a `listar_tareas`.
- Compruebo que lo que devuelve es una lista vacÃ­a (`[]`).

### Caso 2: lista con varias tareas

```python
    def test_listar_con_tareas(self):
        # Agrego dos tareas
        agregar_tarea(self.tmp, "Estudiar IA")
        agregar_tarea(self.tmp, "Repasar Git")

        tareas = listar_tareas(self.tmp)

        # Debe haber 2 tareas
        self.assertEqual(len(tareas), 2)

        # Verificamos que los nombres estÃ¡n correctos
        self.assertEqual(tareas[0]["nombre"], "Estudiar IA")
        self.assertEqual(tareas[1]["nombre"], "Repasar Git")

        # Ambas deben empezar como no completadas
        self.assertFalse(tareas[0]["completada"])
        self.assertFalse(tareas[1]["completada"])

```

TraducciÃ³n:

- Meto dos tareas en el archivo temporal.
- Al listar, espero exactamente 2.
- Compruebo que los nombres coinciden y que ninguna estÃ¡ completada.

### Salida esperada

Al correr `python -m unittest -v`, ahora deberÃ­as ver tambiÃ©n:

```
Ran 5 tests in 0.016s

OK
```

---

Esto ya te cubre los **4 escenarios bÃ¡sicos** de tu app:

1. Agregar una tarea.
2. Completar una tarea.
3. Completar algo que no existe.
4. Listar en vacÃ­o y con varias tareas.

Con eso tienes un **mini-escudo** que te avisa si al refactorizar rompes algo sin darte cuenta.

## Paso 9 â€” Check rÃ¡pido de â€œClean Codeâ€

Haz esta mini-checklist en tu `tareas.py`:

- [ ]  Â¿Los nombres cuentan la historia? (`agregar_tarea`, no `agregar` a secas).
- [ ]  Â¿Cada funciÃ³n hace **una cosa**?
- [ ]  Â¿La lÃ³gica no imprime ni lee argumentos?
- [ ]  Â¿Los comentarios que quedan aportan contexto/decisiÃ³n?
- [ ]  Â¿Puedo testear sin tocar el mundo real?

---

## Mini-ejercicio (tu PR de la clase)

1. Crea rama `feature/cli-clean-tests`.
2. Aplica los pasos 1â€“8 a tu repo.
3. AÃ±ade `test_tareas.py` y ejecuta `python -m unittest -v`.
4. Abre PR. En la descripciÃ³n pega:
    - QuÃ© cambiaste (separaciÃ³n de capas, nombres, return de datos).
    - QuÃ© tests aÃ±adiste y por quÃ©.

---

## â€œAplicaciÃ³n con IAâ€ (pero guiada por ti)

Cuando lo tengas verde, pide a la IA un segundo PR **solo** para migrar el CLI a `argparse` sin tocar la lÃ³gica:

```
Rol: Revisor Python.
Contexto: Tengo lÃ³gica de dominio en tareas.py que ya devuelve datos y no imprime. Quiero migrar la capa CLI a argparse sin cambiar firmas de funciones.

Entrega: un diff o archivo CLI con argparse, subcomandos (listar/agregar/completar) y opciÃ³n global -f/--file para ruta JSON. Sin modificar la lÃ³gica ni los tests.
```

AsÃ­ aprendes tÃº, y la IA **se limita** a la capa de interfaz.

Con esto tienes una clase completa, paso a paso, **sin perder la parte didÃ¡ctica**. El siguiente peldaÃ±o, cuando te sientas cÃ³modo, es aÃ±adir **formato de salida** (ej. `--json` en el CLI) y un par de tests mÃ¡s (IDs, vacÃ­os, JSON corrupto). 

## De momento para estas pruebas estamos usando ChatGPT 5 en auto. Puedes probar con diferentes IA para ver el resultado que da cada una.

Mi prompt:

```sql
Tengo esto:

# tareas.py (Indicamos el nombre del archivo)

# ================================
# 0. IMPORTAR LIBRERIAS
# ================================

from typing import (
    TypedDict,
    List,
)  # Esta libreria nos permite crear tipos de datos (para hacer legible el codigo)
import json, os, sys

# ================================
# 1. VARIABLES GLOBALES
# ================================

# Cambiamos a ARCHIVO_POR_DEFECTO porque explica mejor la funcion del archivo
ARCHIVO_POR_DEFECTO = "tareas.json"

# ================================
# 1.1. TIPOS DE DATOS
# ================================
class Tarea(TypedDict):
    id: int
    nombre: str
    completada: bool

# ================================
# 2. FUNCIONES
# ================================

# Vamos a escribir mejor la funcion para cargar tareas

def cargar_tareas(ruta: str) -> List[Tarea]:
    """Devuelve la lista de tareas. Si no hay archivo o estÃ¡ vacÃ­o/corrupto, lista vacÃ­a."""

    # Si el archivo no existe, devolvemos una lista vacÃ­a
    if not os.path.exists(ruta):
        return []

    try:
        with open(ruta, "r", encoding="utf-8") as f:
            contenido = f.read().strip()
            return json.loads(contenido) if contenido else []

    except json.JSONDecodeError:
        # DecisiÃ³n: no romper por JSON corrupto; devolvemos vacÃ­o para que el CLI siga siendo usable.
        return []

# Esta funciÃ³n guarda las tareas en el archivo JSON
def guardar_tareas(ruta: str, tareas: List[Tarea]) -> None:
    """Guarda la lista de tareas en el archivo JSON con indentaciÃ³n legible."""
    with open(ruta, "w", encoding="utf-8") as f:
        json.dump(tareas, f, ensure_ascii=False, indent=2)

def nuevo_id(tareas: List[Tarea]) -> int:
    """Genera un ID incremental robusto."""
    return 1 if not tareas else max(t["id"] for t in tareas) + 1

def agregar_tarea(ruta: str, nombre: str) -> Tarea:
    """Crea una nueva tarea no completada y la persiste."""
    tareas = cargar_tareas(ruta)
    tarea: Tarea = {"id": nuevo_id(tareas), "nombre": nombre, "completada": False}
    tareas.append(tarea)
    guardar_tareas(ruta, tareas)
    return tarea  # devolver la entidad ayuda en tests

# Esta funciÃ³n lista las tareas
def listar_tareas(ruta: str) -> List[Tarea]:
    """Imprime el listado de tareas con su estado."""
    return cargar_tareas(ruta)

# Esta funciÃ³n completa una tarea
def completar_tarea(ruta: str, id_tarea: int) -> bool:
    """Marca como completada la tarea con el ID indicado (si existe)."""
    tareas = cargar_tareas(ruta)
    for tarea in tareas:
        if tarea["id"] == id_tarea:
            tarea["completada"] = True
            guardar_tareas(ruta, tareas)
            return True
    return False  # no encontrada

# FÃ­jate que ahora no imprimimos dentro de la lÃ³gica: devolvemos datos. Imprimir es cosa del CLI. Esto facilita los tests.

def uso():
    """Imprime el mensaje de uso."""

    print("Uso: python tareas.py <listar|agregar|completar> [args]")
    sys.exit(1)

# ================================
# 3. MAIN
# ================================

# Flujo principal

if __name__ == "__main__":
    if len(sys.argv) < 2:
        uso()

    comando = sys.argv[1]
    archivo = ARCHIVO_POR_DEFECTO

    if comando == "listar":
        for t in listar_tareas(archivo):
            estado = "âœ…" if t["completada"] else "âŒ"
            print(f"{t['id']}. {t['nombre']} {estado}")

    elif comando == "agregar":
        if len(sys.argv) < 3:
            print("Falta el nombre de la tarea")
            sys.exit(1)
        tarea = agregar_tarea(archivo, sys.argv[2])
        print(f"Agregada: {tarea['id']}. {tarea['nombre']}")

    elif comando == "completar":
        if len(sys.argv) < 3:
            print("Falta el ID de la tarea")
            sys.exit(1)
        ok = completar_tarea(archivo, int(sys.argv[2]))
        print("Completada âœ…" if ok else "No encontrada âŒ")
    else:
        print("Comando no reconocido")
        uso()

Rol: Revisor Python.
Contexto: Tengo lÃ³gica de dominio en tareas.py que ya devuelve datos y no imprime. Quiero migrar la capa CLI a argparse sin cambiar firmas de funciones.

Entrega: un diff o archivo CLI con argparse, subcomandos (listar/agregar/completar) y opciÃ³n global -f/--file para ruta JSON. Sin modificar la lÃ³gica ni los tests.
```

Respuesta GPT

```sql
Perfecto, Quillo. Mantengo tareas.py tal cual (sin tocar firmas ni lÃ³gica) y te doy un CLI separado con argparse: tareas_cli.py. Usa subcomandos listar | agregar | completar y una opciÃ³n global -f/--file para la ruta del JSON.

tareas_cli.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CLI para el gestor de tareas usando argparse.
No modifica la lÃ³gica de dominio en tareas.py.
"""

import argparse
import sys

# Importa la lÃ³gica existente sin cambiar firmas
from tareas import (
    ARCHIVO_POR_DEFECTO,
    listar_tareas,
    agregar_tarea,
    completar_tarea,
)

def cmd_listar(args: argparse.Namespace) -> int:
    tareas = listar_tareas(args.file)
    for t in tareas:
        estado = "âœ…" if t["completada"] else "âŒ"
        print(f"{t['id']}. {t['nombre']} {estado}")
    return 0

def cmd_agregar(args: argparse.Namespace) -> int:
    # Permitimos nombres con espacios: se recoge como lista y se une
    nombre = " ".join(args.nombre) if isinstance(args.nombre, list) else args.nombre
    tarea = agregar_tarea(args.file, nombre)
    print(f"Agregada: {tarea['id']}. {tarea['nombre']}")
    return 0

def cmd_completar(args: argparse.Namespace) -> int:
    try:
        id_tarea = int(args.id)
    except ValueError:
        print("El ID debe ser un nÃºmero entero.", file=sys.stderr)
        return 2

    ok = completar_tarea(args.file, id_tarea)
    print("Completada âœ…" if ok else "No encontrada âŒ")
    return 0 if ok else 1

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="tareas",
        description="CLI de tareas con subcomandos (listar, agregar, completar).",
    )

    # OpciÃ³n global de archivo
    parser.add_argument(
        "-f", "--file",
        default=ARCHIVO_POR_DEFECTO,
        help=f"Ruta del archivo JSON (por defecto: {ARCHIVO_POR_DEFECTO})",
    )

    subparsers = parser.add_subparsers(
        title="subcomandos",
        dest="subcomando",
        metavar="{listar,agregar,completar}",
        required=True,  # exige un subcomando
    )

    # listar
    p_listar = subparsers.add_parser(
        "listar",
        help="Lista todas las tareas.",
        description="Lista todas las tareas.",
    )
    p_listar.set_defaults(func=cmd_listar)

    # agregar
    p_agregar = subparsers.add_parser(
        "agregar",
        help="Agrega una nueva tarea.",
        description="Agrega una nueva tarea.",
    )
    # Captura nombre con espacios: uno o mÃ¡s tokens
    p_agregar.add_argument(
        "nombre",
        nargs="+",
        help="Nombre de la tarea (se permiten espacios).",
    )
    p_agregar.set_defaults(func=cmd_agregar)

    # completar
    p_completar = subparsers.add_parser(
        "completar",
        help="Marca como completada una tarea por ID.",
        description="Marca como completada una tarea por ID.",
    )
    p_completar.add_argument(
        "id",
        help="ID numÃ©rico de la tarea a completar.",
    )
    p_completar.set_defaults(func=cmd_completar)

    return parser

def main(argv=None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    return args.func(args)

if __name__ == "__main__":
    raise SystemExit(main())
```

Lo que ha devuelto la IA es un nuevo archivo tareas-cli.py

Deja **la lÃ³gica en `tareas.py`** y mueve **la interfaz de lÃ­nea de comandos** a un archivo aparte, `tareas_cli.py`. Eso te permite testear la lÃ³gica sin pelearte con argumentos de terminal. Vamos a leerlo con gafas de aprendiz y a usarlo paso a paso.

## QuÃ© es `tareas_cli.py` (en cristiano)

- **Tu â€œmando a distanciaâ€**: recoge lo que escribes en la terminal, lo interpreta con `argparse` y llama a las funciones de `tareas.py`.
- **Subcomandos**: `listar`, `agregar`, `completar`.
- **OpciÃ³n global `f/--file`**: eliges quÃ© archivo JSON usar (por defecto, `tareas.json`).

Separar â€œmandoâ€ (CLI) de â€œmotorâ€ (lÃ³gica) es la base de un proyecto limpio: el motor se testea fÃ¡cil; el mando se cambia sin tocar el motor.

## CÃ³mo se usa (pruÃ©balo ya)

Desde la carpeta del proyecto:

```bash
# 1) Listar (si no hay archivo o estÃ¡ vacÃ­o, no pasa nada)
python tareas_cli.py listar

# 2) Agregar tareas (nota: permite espacios en el nombre)
python tareas_cli.py agregar "Estudiar IA con Manu"
python tareas_cli.py agregar Repasar Git

# 3) Completar por ID (empiezan en 1, 2, 3â€¦)
python tareas_cli.py completar 1

# 4) Usar otro archivo distinto
python tareas_cli.py -f tareas_demo.json agregar "Probar archivo alternativo"
python tareas_cli.py -f tareas_demo.json listar

```

VerÃ¡s las marquitas âœ…/âŒ y mensajes como â€œAgregada: 1. Estudiar IA con Manuâ€.

## Lectura guiada (quÃ© hace cada parte)

- `build_parser()` define el â€œdiccionario de la terminalâ€:
    - `f/--file` â†’ ruta del JSON (default: `ARCHIVO_POR_DEFECTO` de tu `tareas.py`).
    - Subcomandos:
        - `listar` â†’ llama a `cmd_listar`.
        - `agregar nombre...` â†’ `nargs="+"` permite nombres con espacios; luego los une en el comando.
        - `completar id` â†’ recibe el id como texto y luego lo convierte a `int`.
- `cmd_*` son â€œadaptadoresâ€: traducen argumentos â†’ llaman a tu lÃ³gica â†’ imprimen salida.
- `main()` parsea los argumentos y ejecuta la funciÃ³n del subcomando elegido; devuelve un **cÃ³digo de salida** (0 = ok, 1/2 = error). Esto es Ãºtil si algÃºn dÃ­a automatizas con scripts o CI.

---

## Dos mini-mejoras (para que aprendas afinando)

1. **Parsear el `id` como entero directamente en argparse**. AsÃ­ evitas el `try/except` manual:

```python
p_completar.add_argument("id", type=int, help="ID numÃ©rico de la tarea a completar.")

```

y en `cmd_completar` ya recibes `args.id` como `int`.

1. **Mensaje amable si no hay tareas** (solo en el CLI; la lÃ³gica sigue devolviendo lista vacÃ­a):

```python
def cmd_listar(args):
    tareas = listar_tareas(args.file)
    if not tareas:
        print("No hay tareas aÃºn. Usa 'agregar' para crear la primera.")
        return 0
    for t in tareas:
        estado = "âœ…" if t["completada"] else "âŒ"
        print(f"{t['id']}. {t['nombre']} {estado}")
    return 0

```

Estas mejoras no tocan tu â€œmotorâ€ y hacen la experiencia mÃ¡s clara.

---

## CÃ³mo encaja con tus tests

- Tus tests unitarios siguen apuntando a **`tareas.py`** (motor). Perfecto.
- Si mÃ¡s adelante quieres probar el CLI, lo haces aparte, con **tests de integraciÃ³n** (por ejemplo, ejecutando `python tareas_cli.py ...` con `subprocess` y mirando la salida). De momento no hace falta: **primero consolida la lÃ³gica**, que ya estÃ¡s testando bien. (No te marees con el subprocess, la idea es que te vayan sonando los conceptos)

## QuÃ© subir al repo (y rama sugerida)

1. Crea rama: `feature/cli-argparse-separado`.
2. AÃ±ade `tareas_cli.py` tal como te lo dieron (o con las 2 mini-mejoras).
3. MantÃ©n `tareas.py` y `test_tareas.py` **sin cambios funcionales**.
4. En el PR, explica: â€œSe separa CLI de la lÃ³gica. `tareas_cli.py` usa argparse; `tareas.py` queda testeable y estableâ€.

