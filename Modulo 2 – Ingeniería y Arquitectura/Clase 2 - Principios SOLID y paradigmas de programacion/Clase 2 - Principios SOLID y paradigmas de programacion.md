# ğŸ“ Clase 2 - Principios SOLID y paradigmas de programaciÃ³n
*(MÃ³dulo 2 â€“ IngenierÃ­a y Arquitectura)*

---

## ğŸ¬ Escena 1 â€“ Del taller al laboratorio

En el mÃ³dulo anterior, tu CLI ya funcionaba.

Guardaba tareas, tenÃ­a tests, incluso aplicaste Clean Code y SRP sin darte cuenta.

Ahora empieza una nueva etapa:

> â€œNecesitamos una API para que otros programas usen tu sistema de tareas.â€
> 

Y tÃº piensas: *â€œÂ¿API? Â¿endpoint? Â¿FastAPI? Â¿esto no era un curso de IA?â€*

SÃ­, pero aquÃ­ **no escribimos APIs mÃ¡gicas**: las diseÃ±amos **como ingenieros**.

Y eso significa: entender principios, diseÃ±ar bien, probar antes de tocar.

Por eso hoy haremos **tu primer endpoint con TDD** y aprenderÃ¡s **por quÃ© cada lÃ­nea existe**, no solo quÃ© escribir.

---

## ğŸ§© Escena 2 â€“ Las reglas que te salvarÃ¡n del caos (SOLID en versiÃ³n humana)

No te las aprendas de memoria todavÃ­a. Solo quiero que las **sientas**.

| Principio | TraducciÃ³n humana |
| --- | --- |
| **S**ingle Responsibility | Cada parte del cÃ³digo debe tener un motivo Ãºnico para cambiar. |
| **O**pen/Closed | Puedes extender sin romper. |
| **L**iskov Substitution | Si algo se comporta como un pato, deberÃ­a reemplazar al pato sin romper nada. |
| **I**nterface Segregation | Mejor interfaces pequeÃ±as que una gigante que obligue a implementarlo todo. |
| **D**ependency Inversion | No dependas de detalles, sino de abstracciones. |

Hoy verÃ¡s el primero en acciÃ³n: **SRP**, el principio de una sola responsabilidad.

---

## âš™ï¸ Escena 3 â€“ Entendiendo el problema antes del cÃ³digo

Queremos esta historia de usuario:

> Cuando un usuario envÃ­a POST /tareas con {"nombre": "Estudiar SOLID"},
> 
> 
> el servidor responde **201** (creado) y un JSON con:
> 
> ```json
> { "id": 1, "nombre": "Estudiar SOLID", "completada": false }
> ```
> 
> Si el nombre va vacÃ­o, responde **422** (error de validaciÃ³n).
> 

Esa historia **es nuestro test**.

---

## ğŸ§ª Escena 4 â€“ Escribiendo el test primero (TDD: el contrato)

Crea la carpeta `Clase2/tests/test_crear_tarea.py` y escribe:

```python
from fastapi.testclient import TestClient
from api.api import app  # aÃºn no existe, lo haremos despuÃ©s

cliente_http = TestClient(app)

def test_crear_tarea_minima_devuelve_201_y_cuerpo_esperado():
    cuerpo_peticion = {"nombre": "Estudiar SOLID"}
    respuesta = cliente_http.post("/tareas", json=cuerpo_peticion)
    assert respuesta.status_code == 201

    cuerpo_respuesta = respuesta.json()
    assert cuerpo_respuesta["id"] == 1
    assert cuerpo_respuesta["nombre"] == "Estudiar SOLID"
    assert cuerpo_respuesta["completada"] is False

def test_crear_tarea_con_nombre_vacio_devuelve_422():
    respuesta = cliente_http.post("/tareas", json={"nombre": ""})
    assert respuesta.status_code == 422

```

---

### ğŸ§  Pausa de comprensiÃ³n

**Â¿QuÃ© estÃ¡s haciendo aquÃ­?**

- `TestClient(app)` crea un **servidor fantasma**: no hay nada corriendo aÃºn, pero simula peticiones HTTP.
- `respuesta = cliente_http.post(...)` es como decir *â€œmando una carta al servidor y espero su respuestaâ€*.
- `assert respuesta.status_code == 201` significa *â€œla historia solo se cumple si el servidor contesta â€˜creadoâ€™â€*.

AsÃ­ que este test no prueba el cÃ³digo: **prueba la historia**.

Cuando lo ejecutes con:

```bash
pytest -q
```

fallarÃ¡ con un error tipo:

```
ModuleNotFoundError: No module named 'api'
```

Perfecto. EstÃ¡s en la fase **Red** del ciclo TDD: nada funciona aÃºn.

---

## ğŸ§° Escena 5 â€“ Preparando el entorno

Antes de tocar nada, crea tu entorno limpio.

```bash
python -m venv .venv
.venv\Scripts\activate  # (Windows)
# o
source .venv/bin/activate  # (mac/Linux)
python -m pip install --upgrade pip
python -m pip install fastapi uvicorn pytest httpx
pip freeze > requirements.txt

```

ğŸ§© **TraducciÃ³n mental**

â€œEstoy creando un laboratorio cerrado para mis experimentos.

Todo lo que instale vive aquÃ­, no en mi ordenador global.â€

---

## ğŸ”¨ Escena 6 â€“ Construyendo el cÃ³digo mÃ­nimo

Crea tu estructura:

```
Clase2/
 â”œâ”€ api/
 â”‚   â”œâ”€ __init__.py
 â”‚   â””â”€ api.py
 â””â”€ tests/
     â”œâ”€ conftest.py
     â””â”€ test_crear_tarea.py
```

`__init__.py` puede estar vacÃ­o. Solo dice:

> â€œEsta carpeta contiene cÃ³digo Python importable.â€
> 

Y ahora el esqueleto de la API:

```python
# api/api.py
from fastapi import FastAPI
from pydantic import BaseModel

class CrearTareaRequest(BaseModel):
    nombre: str  # sin validaciÃ³n todavÃ­a

app = FastAPI()

@app.post("/tareas", status_code=201)
def crear_tarea(cuerpo: CrearTareaRequest):
    return {
        "id": 1,
        "nombre": cuerpo.nombre,
        "completada": False,
    }

```

---

### ğŸ§  Pausa de comprensiÃ³n

- `FastAPI()` crea tu servidor.
- `@app.post("/tareas")` significa *â€œcuando alguien haga POST a /tareas, ejecuta esta funciÃ³nâ€*.
- `BaseModel` (de Pydantic) define **quÃ© campos esperas** en el cuerpo de la peticiÃ³n.

Por ahora, el cÃ³digo **siempre devuelve id=1**, da igual cuÃ¡ntas veces lo llames.

Y eso estÃ¡ bien: **TDD no busca belleza, busca cumplir el contrato**.

---

### ğŸ”— Arreglando el import (el error feo de antes)

A veces `pytest` no sabe dÃ³nde estÃ¡ `api/`.

Crea `tests/conftest.py` con esto:

```python
import sys
from pathlib import Path

raiz_proyecto = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(raiz_proyecto))
```

Ahora vuelve a ejecutar:

```bash
pytest -q
```

Uno de tus tests pasarÃ¡ âœ…, el otro fallarÃ¡ âŒ.

### ğŸ§  Pausa de comprensiÃ³n

`conftest.py` solo le dice a Python **â€œoye, sube un nivel y mira ahÃ­â€** para encontrar tu carpeta `api/`.

- `import sys` â†’ te deja tocar la configuraciÃ³n interna de Python.
- `from pathlib import Path` â†’ sirve para manejar rutas sin volverte loco con las barras.
- `Path(__file__).resolve().parents[1]` â†’ coge la ruta del archivo actual (`tests/conftest.py`) y sube una carpeta.
- `sys.path.insert(0, str(raiz_proyecto))` â†’ aÃ±ade esa carpeta al radar de bÃºsqueda de Python.

Sin eso, Python solo mira dentro de `tests/` y te suelta el error â€œNo module named apiâ€.

Con eso, ya sabe mirar hacia la raÃ­z del proyecto y encuentra tu `api/api.py`.

---

## ğŸ§  Escena 7 â€“ Validando el nombre (segunda iteraciÃ³n TDD)

Queremos que si el nombre va vacÃ­o, la API devuelva **422**.

Edita tu modelo:

```python
from pydantic import BaseModel, Field

class CrearTareaRequest(BaseModel):
    nombre: str = Field(..., min_length=1)
```

Ejecuta otra vez:

```bash
pytest -q
```

ğŸ‰ Ambos tests verdes.

---

### ğŸ“– QuÃ© aprendiste aquÃ­ (sin darte cuenta)

| LÃ­nea | QuÃ© significa en lenguaje humano |
| --- | --- |
| `status_code=201` | â€œConfirmo que creÃ© algo nuevo.â€ |
| `Field(..., min_length=1)` | â€œEl nombre no puede ir vacÃ­o.â€ |
| `pytest -q` | â€œComprueba si mis promesas se cumplen.â€ |
| `return {...}` | â€œDevuelvo lo que prometÃ­: id, nombre, completada.â€ |

---

## ğŸ’¬ Escena 8 â€“ Â¿QuÃ© estamos realmente testeando?

No estÃ¡s probando si FastAPI funciona.

EstÃ¡s comprobando **tu contrato con el mundo exterior**:

1. Si alguien envÃ­a un JSON correcto â†’ respondes 201.
2. Si alguien envÃ­a un JSON invÃ¡lido â†’ respondes 422.

Tus tests son como **las reglas de un juego**.

Y tu cÃ³digo solo gana si las cumple todas.

---

## ğŸ§­ Escena 9 â€“ Primer principio SOLID en acciÃ³n (SRP)

- Tu test tiene una **Ãºnica responsabilidad**: verificar el contrato.
- Tu endpoint tiene una **Ãºnica responsabilidad**: crear una tarea.
- Tu modelo tiene una **Ãºnica responsabilidad**: validar datos.

Eso es **SRP**. No hace falta decirlo, lo estÃ¡s aplicando sin darte cuenta.

---

## ğŸ§  Escena 10 â€“ ComprensiÃ³n activa

**Haz este ejercicio:**

1. Lee tu funciÃ³n `crear_tarea`.
2. ExplÃ­cala sin usar la palabra â€œfunciÃ³nâ€.
    
    Ejemplo:
    
    > â€œCuando alguien me manda una tarea, la reviso, le pongo un nÃºmero, y se la devuelvo con sello de recibido.â€
    > 

Eso es entender cÃ³digo de verdad.

---

## ğŸ¤– Escena 11 â€“ AplicaciÃ³n con IA (como compaÃ±ero de laboratorio)

Una vez que entiendes lo que hace tu cÃ³digo, ahora sÃ­ puedes pedir ayuda inteligente.

Prompt:

```
Rol: Arquitecto Python senior.
Contexto: Tengo una API FastAPI con POST /tareas y tests que pasan.
Objetivo: EnsÃ©Ã±ame cÃ³mo separar la lÃ³gica de negocio de la capa API (repositorio simulado), sin romper los tests.
```

La IA te devolverÃ¡ cÃ³digo mÃ¡s modular:

`RepositorioDeTareas`, `servicio.crear_tarea()`, etc.

Pero esta vez tÃº sabrÃ¡s **quÃ© hace cada pieza** y por quÃ© existe.

---

## ğŸ¤– Escena 12 â€“ Workflow TDD + IA: el cÃ­rculo virtuoso

### El ciclo completo: RED â†’ GREEN â†’ REFACTOR con IA

Hasta ahora usaste IA para generar cÃ³digo aislado. Ahora aprenderÃ¡s el **workflow profesional** que combina TDD con IA de forma disciplinada.

Este es el proceso que usarÃ¡s en cada funcionalidad nueva:

---

### ğŸ“ Paso 1: RED â€“ Escribir el test primero (TÃš defines el contrato)

**QuiÃ©n lo hace**: TÃº (o IA bajo tu supervisiÃ³n)
**Por quÃ© importa**: El test **es la especificaciÃ³n**. Define quÃ© debe hacer el cÃ³digo.

**Prompt para IA** (si necesitas ayuda):
```
Rol: QA Engineer Python
Contexto: API FastAPI con endpoint POST /tareas ya funcionando
Objetivo: Genera test pytest para endpoint GET /tareas que devuelva lista vacÃ­a inicialmente
Restricciones:
- Usar TestClient de FastAPI
- Assert status_code 200
- Assert response JSON es lista vacÃ­a []
```

**IA genera**:
```python
def test_listar_tareas_vacia_devuelve_lista_vacia():
    cliente = TestClient(app)
    respuesta = cliente.get("/tareas")
    assert respuesta.status_code == 200
    assert respuesta.json() == []
```

**TÃš validas**:
- âœ… Â¿El test cumple la historia de usuario?
- âœ… Â¿Los asserts son claros y completos?
- âœ… Â¿El test falla por la razÃ³n correcta? (endpoint no existe aÃºn)

**Ejecuta**: `pytest -v`
**Resultado esperado**: âŒ ROJO (test falla, endpoint no existe)

---

### ğŸ“ Paso 2: GREEN â€“ ImplementaciÃ³n mÃ­nima (IA genera, TÃš validas)

**Prompt para IA**:
```
Rol: Backend Developer Python
Contexto: Tengo este test que falla: [pegar test completo]
Objetivo: Implementa el cÃ³digo MÃNIMO para hacer pasar este test
Restricciones:
- Solo agregar endpoint GET /tareas
- Devolver lista vacÃ­a hardcodeada (por ahora)
- No romper cÃ³digo existente
```

**IA genera**:
```python
@app.get("/tareas")
def listar_tareas():
    return []
```

**TÃš validas**:
1. Ejecutar `pytest -v`
2. âœ… Â¿El test pasÃ³ a VERDE?
3. âœ… Â¿No rompiÃ³ otros tests?
4. âœ… Â¿El cÃ³digo es el MÃNIMO necesario?

---

### ğŸ“ Paso 3: REFACTOR â€“ Mejora con agentes educacionales

AquÃ­ es donde separas cÃ³digo que "funciona" de cÃ³digo **profesional**.

#### 3A. Python Best Practices Coach

**QuÃ© detecta**:
- Type hints faltantes
- Oportunidades para comprehensions
- Anti-patterns (concatenaciÃ³n, loops innecesarios)

**CÃ³mo usar**:
1. Lee tu cÃ³digo implementado
2. Busca patterns del agente (`.claude/agents/educational/python-best-practices-coach.md`)
3. Aplica feedback

**Ejemplo de mejora**:
```python
# âŒ Antes (sin type hints)
@app.get("/tareas")
def listar_tareas():
    return []

# âœ… DespuÃ©s (con type hints)
from typing import List
from pydantic import BaseModel

class TareaResponse(BaseModel):
    id: int
    nombre: str
    completada: bool

@app.get("/tareas", response_model=List[TareaResponse])
def listar_tareas() -> List[TareaResponse]:
    return []
```

#### 3B. FastAPI Design Coach

**QuÃ© valida**:
- RESTful design correcto
- Status codes apropiados
- Response models con Pydantic
- Async/await cuando aplica

**Red flags tÃ­picas que detecta**:
```python
# âŒ Anti-pattern: Retornar dict en vez de modelo
@app.get("/tareas")
def listar_tareas():
    return [{"id": 1, "nombre": "Tarea"}]  # âŒ No tipado

# âœ… Correcto: Response model explÃ­cito
@app.get("/tareas", response_model=List[TareaResponse])
def listar_tareas() -> List[TareaResponse]:
    # FastAPI serializa automÃ¡ticamente
    return [TareaResponse(id=1, nombre="Tarea", completada=False)]
```

#### 3C. API Design Reviewer

**QuÃ© revisa**:
- Convenciones REST (GET, POST, PUT, DELETE)
- Nombres de endpoints (`/tareas` vs `/get-tareas`)
- Estructura de respuestas consistente
- Error handling apropiado

**DespuÃ©s del refactor**: Re-ejecuta tests
```bash
pytest -v
```
**Resultado**: âœ… VERDE (mismo comportamiento, mejor cÃ³digo)

---

### ğŸ¯ Prompts efectivos para endpoints CRUD

#### Prompt template para GET (listar)
```
Rol: Backend Developer FastAPI
Contexto: API de tareas con modelo Tarea(id, nombre, completada)
Objetivo: Implementa GET /tareas que devuelva todas las tareas
Requisitos:
- Response model: List[TareaResponse]
- Status code: 200
- Inicialmente devolver lista vacÃ­a (memoria)
Restricciones: Solo cÃ³digo necesario, sin BD aÃºn
```

#### Prompt template para GET (obtener uno)
```
Rol: Backend Developer FastAPI
Contexto: API de tareas ya tiene POST /tareas y GET /tareas
Objetivo: Implementa GET /tareas/{id} que devuelva una tarea especÃ­fica
Requisitos:
- Path parameter: id (int)
- Response: 200 si existe, 404 si no existe
- Usar HTTPException para 404
```

#### Prompt template para PUT (actualizar)
```
Rol: Backend Developer FastAPI
Objetivo: Implementa PUT /tareas/{id}/completar que marque tarea como completada
Requisitos:
- Solo cambiar campo 'completada' a True
- Response: 200 con tarea actualizada, 404 si no existe
- Validar que id existe antes de actualizar
```

#### Prompt template para DELETE
```
Rol: Backend Developer FastAPI
Objetivo: Implementa DELETE /tareas/{id} que elimine una tarea
Requisitos:
- Response: 204 (No Content) si se eliminÃ³
- Response: 404 si no existe
- No devolver body en 204
```

---

### ğŸ” ValidaciÃ³n del cÃ³digo generado por IA

**Checklist antes de aceptar cÃ³digo IA**:

#### Tests
- [ ] Â¿Los tests pasan en verde?
- [ ] Â¿Los tests validan el comportamiento correcto?
- [ ] Â¿No hay tests comentados o skip?

#### Type hints
- [ ] Â¿Funciones tienen type annotations?
- [ ] Â¿Request/Response models estÃ¡n definidos con Pydantic?
- [ ] Â¿mypy pasa sin errores? (`mypy api/api.py`)

#### REST compliance
- [ ] Â¿Status codes son correctos? (200, 201, 204, 404, 422)
- [ ] Â¿Nombres de endpoints siguen convenciÃ³n? (`/tareas` no `/getTareas`)
- [ ] Â¿Response models son consistentes?

#### Clean Code
- [ ] Â¿CÃ³digo es legible? (nombres claros, no magia)
- [ ] Â¿No hay duplicaciÃ³n innecesaria?
- [ ] Â¿Sigue Single Responsibility?

---

### ğŸš¨ Red flags tÃ­picas del cÃ³digo IA

**Red flag #1**: CÃ³digo "mÃ¡gico" sin explicaciÃ³n
```python
# âŒ IA a veces genera esto
@app.get("/tareas")
def get_tasks():  # âŒ Nombre inconsistente
    return db.query(Task).all()  # âŒ Â¿De dÃ³nde sale db?
```

**SoluciÃ³n**: Pide a la IA que explique de dÃ³nde vienen las dependencias.

**Red flag #2**: Tests que no fallan
```python
# âŒ Test que siempre pasa (inÃºtil)
def test_crear_tarea():
    assert True  # âŒ No valida nada real
```

**SoluciÃ³n**: Valida que el test falle cuando DEBE fallar.

**Red flag #3**: CÃ³digo que "funciona" pero no es mantenible
```python
# âŒ Hardcoded values
@app.get("/tareas/{id}")
def get_task(id: int):
    if id == 1:
        return {"id": 1, "nombre": "Tarea 1"}
    elif id == 2:
        return {"id": 2, "nombre": "Tarea 2"}
    # ...
```

**SoluciÃ³n**: Usa repositorio, no hardcodes.

---

### ğŸ“ CÃ³mo usar agentes educacionales (paso a paso)

**Escenario**: Acabas de implementar GET /tareas con IA

**Paso 1**: Lee el cÃ³digo generado lÃ­nea por lÃ­nea
**Paso 2**: Abre `.claude/agents/educational/python-best-practices-coach.md`
**Paso 3**: Compara tu cÃ³digo con los patterns del agente:
- Â¿Falta type hints? â†’ Agrega
- Â¿Usa loops manuales? â†’ Refactoriza a comprehension
- Â¿Concatena strings? â†’ Cambia a f-strings

**Paso 4**: Abre `.claude/agents/educational/fastapi-design-coach.md`
**Paso 5**: Valida diseÃ±o API:
- Â¿Response model explÃ­cito?
- Â¿Status codes correctos?
- Â¿Async donde aplica?

**Paso 6**: Ejecuta tests nuevamente
**Paso 7**: Commit solo si TODO pasa

---

### ğŸ“Š Workflow visual resumido

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE RED (Test primero)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Escribe test (tÃº o IA)           â”‚    â”‚
â”‚  â”‚ 2. Valida que test es correcto      â”‚    â”‚
â”‚  â”‚ 3. pytest â†’ âŒ ROJO (esperado)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE GREEN (ImplementaciÃ³n mÃ­nima)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. IA genera cÃ³digo mÃ­nimo          â”‚    â”‚
â”‚  â”‚ 2. TÃš validas cÃ³digo                â”‚    â”‚
â”‚  â”‚ 3. pytest â†’ âœ… VERDE               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASE REFACTOR (Mejora con agentes)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Python Best Practices Coach      â”‚    â”‚
â”‚  â”‚ 2. FastAPI Design Coach             â”‚    â”‚
â”‚  â”‚ 3. API Design Reviewer              â”‚    â”‚
â”‚  â”‚ 4. pytest â†’ âœ… VERDE (refactored)  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
        COMMIT
```

---

### ğŸ’¡ LecciÃ³n clave

**La IA es tu junior developer**, no tu arquitecto.

- âœ… **IA genera cÃ³digo** â†’ TÃš validas diseÃ±o
- âœ… **IA propone tests** â†’ TÃš validas cobertura
- âœ… **IA sugiere refactors** â†’ TÃš decides si aplicar

**No inviertas la relaciÃ³n**. TÃº eres el arquitecto, la IA es la herramienta.

---

## âœ… Escena final â€“ Checklist

- [x]  Entiendes quÃ© es SOLID (y aplicaste SRP).
- [x]  Sabes quÃ© prueba un test HTTP.
- [x]  Tienes tu entorno virtual limpio.
- [x]  Creaste un endpoint real y lo probaste con TDD.
- [x]  Aprendiste a leer cÃ³digo como historia, no como receta.

---

## ğŸ¯ Resultado del PR

- Rama: `feature/api-tdd-crear-endpoint`.
- Archivos:
    - `api/api.py` (endpoint funcional).
    - `tests/test_crear_tarea.py` (dos tests claros).
    - `tests/conftest.py` (arreglo de import).
- Ambos tests en verde.
- `requirements.txt` generado.

---

Esta clase no te enseÃ±Ã³ a â€œhacer APIsâ€.

Te enseÃ±Ã³ **a pensar como un ingeniero**: cada lÃ­nea tiene un propÃ³sito, cada error es una pista, cada test es una historia.

Lo importante es que ahora sabes, que cuando pidas a la IA un codigo, debes:

1. Pedir las historias a la IA
2. pedirle los test
3. Recordarle usar SOLID y SRP
4. Documentar en [notes.md](http://notes.md) para poder dar contexto en el futuro a la IA o a ti mismo
5. Sabes que tienes que hacer entornos virtuales
6. Sabes escalar de un CLI a una API
7. Aunque no sepas el codigo de memoria, ya puedes pedirle a la IA las cosas con mayor contexto en menos palabras.

No necesitas hacer un curso de prompt engineering, solo debes saber el lenguaje y lo que tienes que pedirle.