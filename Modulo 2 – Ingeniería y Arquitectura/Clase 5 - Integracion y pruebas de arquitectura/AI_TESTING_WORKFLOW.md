# Workflow de IA: Test Coverage Strategist para APIs

## üéØ Objetivo

Aprender a usar IA para **dise√±ar estrategias de testing inteligentes** que maximicen el valor de los tests minimizando el esfuerzo.

**Meta**: Al final, tendr√°s:
- ‚úÖ Estrategia de testing basada en an√°lisis de riesgos
- ‚úÖ Tests parametrizados que validan m√∫ltiples implementaciones
- ‚úÖ Mocking estrat√©gico (solo donde aporta valor)
- ‚úÖ >= 80% coverage con tests de alto valor

---

## üìã Fase 1: An√°lisis de Arquitectura (Test Coverage Strategist)

### Paso 1.1: Mapear la arquitectura con IA

**Objetivo**: Entender QU√â componentes tiene tu sistema y c√≥mo interact√∫an.

**Prompt**:

```
Rol: Software Architect experto en an√°lisis de sistemas

Contexto:
Tengo una API FastAPI con esta estructura de archivos:
[Listar estructura del proyecto]

C√≥digo de componentes principales:
[Pegar api.py, servicio_tareas.py, repositorio_base.py]

Objetivo:
Genera un mapa de arquitectura identificando:
1. Capas (API, Servicio, Repositorio)
2. Flujos de datos entre capas
3. Puntos de integraci√≥n (d√≥nde se comunican las capas)
4. Dependencias externas (archivos, APIs, BD)

Formato:
```
Capa API (api.py)
‚îú‚îÄ‚îÄ POST /tareas ‚Üí ServicioTareas.crear()
‚îî‚îÄ‚îÄ GET /tareas ‚Üí ServicioTareas.listar()

Capa Servicio (servicio_tareas.py)
‚îú‚îÄ‚îÄ crear(nombre) ‚Üí RepositorioTareas.guardar()
‚îî‚îÄ‚îÄ listar() ‚Üí RepositorioTareas.listar()

Capa Repositorio (repositorio_base.py + implementaciones)
‚îú‚îÄ‚îÄ RepositorioMemoria (en RAM)
‚îî‚îÄ‚îÄ RepositorioJSON (archivo JSON)
```

Lista los **puntos de riesgo** (lugares donde pueden aparecer bugs)
```

---

### Paso 1.2: Generar estrategia de testing con Test Coverage Strategist

**Prompt estructurado**:

```
Rol: Test Coverage Strategist experto en pytest y FastAPI

Contexto:
Arquitectura:
[Pegar mapa del paso 1.1]

C√≥digo actual:
[Pegar c√≥digo completo]

Objetivo:
Genera una ESTRATEGIA DE TESTING completa priorizando tests de alto valor

Formato:

## 1. Tests Unitarios (por capa)
| Capa | Componente | Edge cases cr√≠ticos | Valor (1-10) | Esfuerzo (1-10) |
|------|------------|---------------------|--------------|-----------------|

## 2. Tests de Integraci√≥n
| Flujo completo | Capas | Edge cases | Mockear | Valor | Esfuerzo |
|----------------|-------|------------|---------|-------|----------|

## 3. Top 10 Edge Cases por Impacto
[Lista priorizada de bugs m√°s probables]

## 4. Gaps de Cobertura Actuales
[C√≥digo sin tests, ramas sin cubrir]

## 5. Recomendaci√≥n de Mocking
| Componente | Mockear | Testear Real | Justificaci√≥n |
|------------|---------|--------------|---------------|

Restricciones:
- Prioriza por ROI (valor / esfuerzo)
- Identifica tests triviales que NO vale la pena escribir
- Recomienda qu√© testear primero (quick wins)
```

**Output esperado** (ejemplo):

```markdown
## 1. Tests Unitarios

| Capa | Componente | Edge cases | Valor | Esfuerzo |
|------|------------|------------|-------|----------|
| Servicio | crear() | Nombre vac√≠o, IDs negativos | 9/10 | 2/10 |
| Repo | guardar() | IDs √∫nicos, mutaci√≥n de estado | 10/10 | 3/10 |
| API | POST /tareas | Content-Type incorrecto | 6/10 | 2/10 |

## 2. Tests de Integraci√≥n

| Flujo | Capas | Edge cases | Mockear | Valor | Esfuerzo |
|-------|-------|------------|---------|-------|----------|
| POST ‚Üí crear ‚Üí guardar | API+Servicio+Repo | Caracteres especiales (', ", \) | Ninguno | 10/10 | 4/10 |
| GET con repo vac√≠o | API+Servicio+Repo | Devuelve [] no 404 | Ninguno | 8/10 | 2/10 |

## 3. Top 10 Edge Cases por Impacto

1. **IDs duplicados** (Valor: 10, Impacto: CR√çTICO)
   - Qu√©: Dos tareas tienen el mismo ID
   - Por qu√©: Rompe invariante de unicidad, bugs de UPDATE/DELETE

2. **Nombres con SQL injection** (Valor: 9, Impacto: ALTO)
   - Qu√©: `nombre = "'; DROP TABLE tareas;--"`
   - Por qu√©: Si usas BD SQL sin parametrizar, vulnerabilidad cr√≠tica

3. **Archivo JSON corrupto** (Valor: 8, Impacto: MEDIO)
   - Qu√©: JSON inv√°lido en disco
   - Por qu√©: App crashea al arrancar si no maneja excepci√≥n

[... 7 casos m√°s]

## 4. Gaps de Cobertura

‚ùå **RepositorioJSON.listar()**: No maneja FileNotFoundError si alguien borra el archivo
‚ùå **API POST /tareas**: No valida longitud m√°xima (ataque DoS con nombre de 10MB)
‚ùå **ServicioTareas**: No tiene tests para m√∫ltiples tareas con mismo nombre (permitido o no?)

## 5. Recomendaci√≥n de Mocking

| Componente | Mockear | Testear Real | Justificaci√≥n |
|------------|---------|--------------|---------------|
| RepositorioMemoria | ‚ùå No | ‚úÖ S√≠ | Es r√°pido (<1ms), testear real valida comportamiento |
| RepositorioJSON | ‚ùå No | ‚úÖ S√≠ | Usar tmp_path, testear I/O real detecta bugs |
| API externa (Stripe) | ‚úÖ S√≠ | ‚ùå No | Lenta, cara, no controlamos |
```

---

## üî® Fase 2: Implementaci√≥n de Tests Parametrizados

### Paso 2.1: Generar fixture parametrizado

**Prompt**:

```
Rol: Pytest Expert

Contexto:
Tengo 2 repositorios que cumplen el mismo Protocol:
- RepositorioMemoria
- RepositorioJSON

Objetivo:
Genera fixture parametrizado que devuelva AMBOS repositorios

Requisitos:
- Usa @pytest.fixture(params=[...])
- Para JSON, usa tmp_path para archivos temporales
- Cleanup autom√°tico (archivos se borran despu√©s del test)
- Documentaci√≥n clara de por qu√© usar fixture vs pytest.mark.parametrize

Formato:
```python
import pytest
from pathlib import Path
# ... c√≥digo con explicaci√≥n
```
```

**Output esperado**:

```python
import pytest
from pathlib import Path
from api.repositorio_memoria import RepositorioMemoria
from api.repositorio_json import RepositorioJSON


@pytest.fixture(params=["memoria", "json"])
def repositorio(request, tmp_path):
    """Fixture parametrizado que devuelve cada repositorio.

    ¬øPor qu√© fixture parametrizado y no pytest.mark.parametrize?

    1. **Flexibilidad**: Permite setup/teardown espec√≠fico por tipo
    2. **Archivos temporales**: tmp_path solo disponible en fixtures
    3. **Escalabilidad**: Agregar nuevo repo = a√±adir 1 l√≠nea en params

    Args:
        request: Objeto pytest con par√°metro actual
        tmp_path: Directorio temporal de pytest (se limpia autom√°tico)

    Yields:
        RepositorioMemoria o RepositorioJSON
    """
    if request.param == "memoria":
        yield RepositorioMemoria()

    elif request.param == "json":
        # Archivo temporal √öNICO por test (evita colisiones paralelas)
        archivo = tmp_path / f"test_{request.node.name}.json"
        yield RepositorioJSON(str(archivo))
        # Cleanup autom√°tico: pytest borra tmp_path al terminar
```

---

### Paso 2.2: Generar tests para edge cases cr√≠ticos

Usa la lista del Paso 1.2 para generar tests priorizados.

**Prompt para cada edge case**:

```
Rol: Pytest Developer

Contexto:
Edge case a testear: [Describir el edge case del Top 10]
Fixture disponible: repositorio (parametrizado, Memoria + JSON)

Objetivo:
Genera test parametrizado que valide este edge case

Requisitos:
- Nombre descriptivo (test_<componente>_<caso>_<resultado_esperado>)
- Docstring que explique QU√â se testea y POR QU√â es cr√≠tico
- Assertions fuertes (no solo `is not None`)
- Mensajes de error claros en assertions

Ejemplo:
```python
def test_guardar_asigna_ids_unicos_a_multiples_tareas(repositorio):
    \"\"\"Valida que no hay IDs duplicados al guardar m√∫ltiples tareas.

    Edge case cr√≠tico: IDs duplicados rompen DELETE y UPDATE.
    \"\"\"
    tareas = [Tarea(id=0, nombre=f"Tarea {i}") for i in range(5)]

    for tarea in tareas:
        repositorio.guardar(tarea)

    ids = [t.id for t in tareas]
    assert len(set(ids)) == 5, f"IDs duplicados: {ids}"
    assert all(id > 0 for id in ids), f"IDs inv√°lidos: {ids}"
```
```

---

## ‚úÖ Fase 3: Estrategia de Mocking

### Cu√°ndo mockear (regla de oro)

**Mockear S√ç** cuando:
- ‚úÖ Es **lento** (>100ms por test)
- ‚úÖ Es **no determinista** (random, time, red)
- ‚úÖ Es **caro** (llamadas a APIs de pago)
- ‚úÖ Es **no controlable** (servicios externos, prod DB)

**Mockear NO** cuando:
- ‚ùå Es **r√°pido** (<10ms)
- ‚ùå Es **cr√≠tico** (core business logic)
- ‚ùå Es **f√°cil de testear real** (in-memory DB, temp files)

### Tabla de decisi√≥n

| Componente | Velocidad | Control | Mockear | Alternativa |
|------------|-----------|---------|---------|-------------|
| RepositorioMemoria | <1ms | Total | ‚ùå No | Testear real |
| RepositorioJSON (tmp_path) | ~10ms | Total | ‚ùå No | Testear real |
| RepositorioDB (SQLite :memory:) | ~50ms | Total | ‚ùå No | Testear real |
| API externa (Stripe) | ~500ms | Ninguno | ‚úÖ S√≠ | `responses` library |
| Email service (SendGrid) | ~300ms | Ninguno | ‚úÖ S√≠ | Mock SMTP |

---

### Paso 3.1: Validar estrategia de mocking con IA

**Prompt**:

```
Rol: Testing Architect

Revisa esta suite de tests y eval√∫a la estrategia de mocking:

C√≥digo:
[Pegar tests]

Checklist:
1. ¬øHay over-mocking? (componentes r√°pidos que deber√≠an testearse reales)
2. ¬øHay under-mocking? (APIs externas testeadas reales que ralentizan CI)
3. ¬øLos mocks son fr√°giles? (acoplados a detalles de implementaci√≥n)
4. ¬øLos tests con mocks siguen siendo valiosos? (o solo validan que se llama al mock)

Para cada problema, dame:
- L√≠nea exacta del problema
- Por qu√© es un problema (consecuencia)
- Refactorizaci√≥n (c√≥digo antes/despu√©s)
```

**Ejemplo de detecci√≥n**:

```python
# ‚ùå OVER-MOCKING detectado
def test_servicio_llama_repo(mocker):
    mock_repo = mocker.Mock()
    servicio = ServicioTareas(mock_repo)

    servicio.crear("Test")

    mock_repo.guardar.assert_called_once()  # Solo valida llamada, no comportamiento

# IA te alertar√≠a:
> **Problema**: Test solo valida que `guardar()` se llama, no que la tarea se guarda correctamente.
> Si `guardar()` tiene un bug que ignora la tarea, el test sigue pasando.
>
> **Refactorizaci√≥n**: Usa RepositorioMemoria real (es r√°pido)

# ‚úÖ CORRECTO
def test_servicio_persiste_tarea_correctamente():
    repo = RepositorioMemoria()  # Real
    servicio = ServicioTareas(repo)

    tarea = servicio.crear("Test")

    tareas_guardadas = repo.listar()
    assert len(tareas_guardadas) == 1
    assert tareas_guardadas[0].nombre == "Test"
```

---

## üß™ Fase 4: Validaci√≥n de Calidad de Tests

### Paso 4.1: Evaluar calidad con Test Quality Reviewer

**Prompt**:

```
Rol: Test Quality Reviewer

Eval√∫a la calidad de estos tests:

C√≥digo:
[Pegar tests]

Criterios (1-10 cada uno):
1. **Assertions significativas**: ¬øValidan comportamiento o solo existencia?
2. **Independencia**: ¬øPueden correr en cualquier orden?
3. **Nombres descriptivos**: ¬øExplican QU√â testean?
4. **Edge cases**: ¬øCubren casos l√≠mite?
5. **Fragilidad**: ¬øSe rompen solo con bugs reales o con cualquier cambio interno?
6. **Velocidad**: ¬øCorren en <1 segundo?
7. **Legibilidad**: ¬øUn junior entiende qu√© falla leyendo el nombre del test?

Formato:
| Test | Criterio 1 | Criterio 2 | ... | Total | Problemas |
|------|------------|------------|-----|-------|-----------|

Recomendaciones de mejora para tests con score <7
```

---

### Red Flags Comunes

#### 1. Assertions d√©biles

```python
# ‚ùå MAL (score: 2/10)
def test_crear_tarea():
    tarea = servicio.crear("Test")
    assert tarea  # Solo valida que no es None

# ‚úÖ BIEN (score: 9/10)
def test_crear_tarea_asigna_id_positivo_y_nombre_correcto():
    tarea = servicio.crear("Test")
    assert tarea.id > 0, f"ID debe ser positivo, fue {tarea.id}"
    assert tarea.nombre == "Test", "Nombre debe coincidir"
    assert tarea.completada is False, "Nueva tarea debe estar pendiente"
```

#### 2. Tests dependientes (orden importa)

```python
# ‚ùå MAL (score: 1/10) - Tests acoplados
def test_1_crear():
    servicio.crear("Tarea 1")

def test_2_listar():
    tareas = servicio.listar()
    assert len(tareas) == 1  # Falla si test_1 no corri√≥ antes

# ‚úÖ BIEN (score: 10/10) - Tests independientes
def test_crear_y_listar_devuelve_tarea_creada():
    servicio = ServicioTareas(RepositorioMemoria())  # Repo limpio
    servicio.crear("Tarea 1")

    tareas = servicio.listar()
    assert len(tareas) == 1
```

#### 3. Tests fr√°giles (acoplados a implementaci√≥n)

```python
# ‚ùå MAL (score: 3/10) - Testea detalle interno
def test_contador_incrementa():
    repo = RepositorioMemoria()
    repo.guardar(Tarea(id=0, nombre="Test"))

    assert repo._contador == 1  # Acoplado a variable privada

# ‚úÖ BIEN (score: 9/10) - Testea contrato p√∫blico
def test_ids_son_secuenciales():
    repo = RepositorioMemoria()
    t1 = Tarea(id=0, nombre="Test1")
    t2 = Tarea(id=0, nombre="Test2")

    repo.guardar(t1)
    repo.guardar(t2)

    assert t2.id == t1.id + 1  # Testea comportamiento observable
```

---

## üìä Fase 5: Medici√≥n de Coverage

### Paso 5.1: Ejecutar pytest con coverage

```bash
pytest --cov=api --cov-report=term-missing --cov-fail-under=80 -v
```

**Qu√© significa**:
- `--cov=api`: Medir coverage del directorio api/
- `--cov-report=term-missing`: Mostrar l√≠neas NO cubiertas
- `--cov-fail-under=80`: Fallar si <80%
- `-v`: Verbose (ver cada test)

---

### Paso 5.2: Analizar gaps de cobertura con IA

**Prompt**:

```
Rol: Coverage Analyst

Contexto:
Aqu√≠ est√° el reporte de coverage de pytest:

[Pegar output de pytest --cov --cov-report=term-missing]

Objetivo:
Analiza los gaps y recomienda qu√© tests agregar

Formato:
## L√≠neas sin cubrir
| Archivo | L√≠neas | Componente | Criticidad (1-10) | Test sugerido |
|---------|--------|------------|-------------------|---------------|

## Ramas sin cubrir
[if/else, try/except que no se ejecutan]

## Recomendaci√≥n
[Qu√© testear primero para llegar a 80%+ con m√≠nimo esfuerzo]
```

---

## üö® Checklist Final

### Estrategia de Testing Completa

**An√°lisis previo**:
- [ ] Mapa de arquitectura generado (capas, flujos, riesgos)
- [ ] Top 10 edge cases priorizados por impacto
- [ ] Estrategia de mocking definida (qu√© mockear, qu√© no)

**Tests implementados**:
- [ ] Tests parametrizados para TODOS los repositorios
- [ ] Edge cases cr√≠ticos cubiertos (IDs √∫nicos, nombres especiales, repo vac√≠o)
- [ ] Tests de integraci√≥n end-to-end (API ‚Üí Servicio ‚Üí Repo)

**Calidad de tests**:
- [ ] Assertions significativas (no solo `is not None`)
- [ ] Tests independientes (orden no importa)
- [ ] Nombres descriptivos (explican QU√â y CU√ÅNDO)
- [ ] Sin over-mocking (componentes r√°pidos testeados reales)

**Coverage y performance**:
- [ ] >= 80% line coverage
- [ ] 100% de m√©todos p√∫blicos testeados
- [ ] Todos los tests corren en <10 segundos total

---

## üí° Tips para Usar IA Efectivamente

### 1. Prompts iterativos (no esperes perfecci√≥n en primer intento)

```
# Iteraci√≥n 1: Estrategia general
‚Üí IA: "Analiza arquitectura y genera estrategia de testing"

# Iteraci√≥n 2: Validar estrategia
‚Üí IA: "Revisa esta estrategia, ¬øhay over-testing o under-testing?"

# Iteraci√≥n 3: Implementar
‚Üí IA: "Genera tests para los 3 edge cases de mayor valor"

# Iteraci√≥n 4: Validar calidad
‚Üí IA: "Eval√∫a estos tests con criterios de calidad"
```

### 2. Pregunta "¬øPor qu√©?" para aprender

```
"¬øPor qu√© este test necesita mock del repositorio?"
"¬øPor qu√© este edge case es m√°s importante que este otro?"
"¬øPor qu√© usar fixture parametrizado en vez de pytest.mark.parametrize?"
```

### 3. Valida con m√∫ltiples agentes

- **Python Best Practices Coach**: Sintaxis, fixtures, type hints
- **FastAPI Design Coach**: Tests de endpoints, status codes
- **Performance Optimizer**: Tests lentos (>1s)

Cada agente detecta problemas diferentes ‚Üí cobertura completa.

---

**Resumen**: Tests inteligentes > tests en cantidad. IA te ayuda a priorizar los tests que realmente atrapan bugs. üéØ
