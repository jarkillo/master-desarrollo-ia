# Workflow de IA: Test Coverage Strategist para APIs

## ğŸ¯ Objetivo

Aprender a usar IA para **diseÃ±ar estrategias de testing inteligentes** que maximicen el valor de los tests minimizando el esfuerzo.

**Meta**: Al final, tendrÃ¡s:
- âœ… Estrategia de testing basada en anÃ¡lisis de riesgos
- âœ… Tests parametrizados que validan mÃºltiples implementaciones
- âœ… Mocking estratÃ©gico (solo donde aporta valor)
- âœ… >= 80% coverage con tests de alto valor

---

## ğŸ“‹ Fase 1: AnÃ¡lisis de Arquitectura (Test Coverage Strategist)

### Paso 1.1: Mapear la arquitectura con IA

**Objetivo**: Entender QUÃ‰ componentes tiene tu sistema y cÃ³mo interactÃºan.

**Prompt**:

```
Rol: Software Architect experto en anÃ¡lisis de sistemas

Contexto:
Tengo una API FastAPI con esta estructura de archivos:
[Listar estructura del proyecto]

CÃ³digo de componentes principales:
[Pegar api.py, servicio_tareas.py, repositorio_base.py]

Objetivo:
Genera un mapa de arquitectura identificando:
1. Capas (API, Servicio, Repositorio)
2. Flujos de datos entre capas
3. Puntos de integraciÃ³n (dÃ³nde se comunican las capas)
4. Dependencias externas (archivos, APIs, BD)

Formato:
```
Capa API (api.py)
â”œâ”€â”€ POST /tareas â†’ ServicioTareas.crear()
â””â”€â”€ GET /tareas â†’ ServicioTareas.listar()

Capa Servicio (servicio_tareas.py)
â”œâ”€â”€ crear(nombre) â†’ RepositorioTareas.guardar()
â””â”€â”€ listar() â†’ RepositorioTareas.listar()

Capa Repositorio (repositorio_base.py + implementaciones)
â”œâ”€â”€ RepositorioMemoria (en RAM)
â””â”€â”€ RepositorioJSON (archivo JSON)
```

Lista los **puntos de riesgo** (lugares donde pueden aparecer bugs)
```

---

### Paso 1.2: Generar estrategia de testing con Test Coverage Strategist

**Prompt estructurado**:

```
Rol: Test Coverage Strategist experto en pytest y FastAPI

Contexto:
Arquitectura:
[Pegar mapa del paso 1.1]

CÃ³digo actual:
[Pegar cÃ³digo completo]

Objetivo:
Genera una ESTRATEGIA DE TESTING completa priorizando tests de alto valor

Formato:

## 1. Tests Unitarios (por capa)
| Capa | Componente | Edge cases crÃ­ticos | Valor (1-10) | Esfuerzo (1-10) |
|------|------------|---------------------|--------------|-----------------|

## 2. Tests de IntegraciÃ³n
| Flujo completo | Capas | Edge cases | Mockear | Valor | Esfuerzo |
|----------------|-------|------------|---------|-------|----------|

## 3. Top 10 Edge Cases por Impacto
[Lista priorizada de bugs mÃ¡s probables]

## 4. Gaps de Cobertura Actuales
[CÃ³digo sin tests, ramas sin cubrir]

## 5. RecomendaciÃ³n de Mocking
| Componente | Mockear | Testear Real | JustificaciÃ³n |
|------------|---------|--------------|---------------|

Restricciones:
- Prioriza por ROI (valor / esfuerzo)
- Identifica tests triviales que NO vale la pena escribir
- Recomienda quÃ© testear primero (quick wins)
```

**Output esperado** (ejemplo):

```markdown
## 1. Tests Unitarios

| Capa | Componente | Edge cases | Valor | Esfuerzo |
|------|------------|------------|-------|----------|
| Servicio | crear() | Nombre vacÃ­o, IDs negativos | 9/10 | 2/10 |
| Repo | guardar() | IDs Ãºnicos, mutaciÃ³n de estado | 10/10 | 3/10 |
| API | POST /tareas | Content-Type incorrecto | 6/10 | 2/10 |

## 2. Tests de IntegraciÃ³n

| Flujo | Capas | Edge cases | Mockear | Valor | Esfuerzo |
|-------|-------|------------|---------|-------|----------|
| POST â†’ crear â†’ guardar | API+Servicio+Repo | Caracteres especiales (', ", \) | Ninguno | 10/10 | 4/10 |
| GET con repo vacÃ­o | API+Servicio+Repo | Devuelve [] no 404 | Ninguno | 8/10 | 2/10 |

## 3. Top 10 Edge Cases por Impacto

1. **IDs duplicados** (Valor: 10, Impacto: CRÃTICO)
   - QuÃ©: Dos tareas tienen el mismo ID
   - Por quÃ©: Rompe invariante de unicidad, bugs de UPDATE/DELETE

2. **Nombres con SQL injection** (Valor: 9, Impacto: ALTO)
   - QuÃ©: `nombre = "'; DROP TABLE tareas;--"`
   - Por quÃ©: Si usas BD SQL sin parametrizar, vulnerabilidad crÃ­tica

3. **Archivo JSON corrupto** (Valor: 8, Impacto: MEDIO)
   - QuÃ©: JSON invÃ¡lido en disco
   - Por quÃ©: App crashea al arrancar si no maneja excepciÃ³n

[... 7 casos mÃ¡s]

## 4. Gaps de Cobertura

âŒ **RepositorioJSON.listar()**: No maneja FileNotFoundError si alguien borra el archivo
âŒ **API POST /tareas**: No valida longitud mÃ¡xima (ataque DoS con nombre de 10MB)
âŒ **ServicioTareas**: No tiene tests para mÃºltiples tareas con mismo nombre (permitido o no?)

## 5. RecomendaciÃ³n de Mocking

| Componente | Mockear | Testear Real | JustificaciÃ³n |
|------------|---------|--------------|---------------|
| RepositorioMemoria | âŒ No | âœ… SÃ­ | Es rÃ¡pido (<1ms), testear real valida comportamiento |
| RepositorioJSON | âŒ No | âœ… SÃ­ | Usar tmp_path, testear I/O real detecta bugs |
| API externa (Stripe) | âœ… SÃ­ | âŒ No | Lenta, cara, no controlamos |
```

---

## ğŸ”¨ Fase 2: ImplementaciÃ³n de Tests Parametrizados

### Paso 2.1: Generar fixture parametrizado

**Prompt**:

```
Rol: Pytest Expert

Contexto:
Tengo 2 repositorios que cumplen el mismo Protocol:
- RepositorioMemoria
- RepositorioJSON

Objetivo:
Genera fixture parametrizado que devuelva AMBOS repositorios

Requisitos:
- Usa @pytest.fixture(params=[...])
- Para JSON, usa tmp_path para archivos temporales
- Cleanup automÃ¡tico (archivos se borran despuÃ©s del test)
- DocumentaciÃ³n clara de por quÃ© usar fixture vs pytest.mark.parametrize

Formato:
```python
import pytest
from pathlib import Path
# ... cÃ³digo con explicaciÃ³n
```
```

**Output esperado**:

```python
import pytest
from pathlib import Path
from api.repositorio_memoria import RepositorioMemoria
from api.repositorio_json import RepositorioJSON


@pytest.fixture(params=["memoria", "json"])
def repositorio(request, tmp_path):
    """Fixture parametrizado que devuelve cada repositorio.

    Â¿Por quÃ© fixture parametrizado y no pytest.mark.parametrize?

    1. **Flexibilidad**: Permite setup/teardown especÃ­fico por tipo
    2. **Archivos temporales**: tmp_path solo disponible en fixtures
    3. **Escalabilidad**: Agregar nuevo repo = aÃ±adir 1 lÃ­nea en params

    Args:
        request: Objeto pytest con parÃ¡metro actual
        tmp_path: Directorio temporal de pytest (se limpia automÃ¡tico)

    Yields:
        RepositorioMemoria o RepositorioJSON
    """
    if request.param == "memoria":
        yield RepositorioMemoria()

    elif request.param == "json":
        # Archivo temporal ÃšNICO por test (evita colisiones paralelas)
        archivo = tmp_path / f"test_{request.node.name}.json"
        yield RepositorioJSON(str(archivo))
        # Cleanup automÃ¡tico: pytest borra tmp_path al terminar
```

---

### Paso 2.2: Generar tests para edge cases crÃ­ticos

Usa la lista del Paso 1.2 para generar tests priorizados.

**Prompt para cada edge case**:

```
Rol: Pytest Developer

Contexto:
Edge case a testear: [Describir el edge case del Top 10]
Fixture disponible: repositorio (parametrizado, Memoria + JSON)

Objetivo:
Genera test parametrizado que valide este edge case

Requisitos:
- Nombre descriptivo (test_<componente>_<caso>_<resultado_esperado>)
- Docstring que explique QUÃ‰ se testea y POR QUÃ‰ es crÃ­tico
- Assertions fuertes (no solo `is not None`)
- Mensajes de error claros en assertions

Ejemplo:
```python
def test_guardar_asigna_ids_unicos_a_multiples_tareas(repositorio):
    \"\"\"Valida que no hay IDs duplicados al guardar mÃºltiples tareas.

    Edge case crÃ­tico: IDs duplicados rompen DELETE y UPDATE.
    \"\"\"
    tareas = [Tarea(id=0, nombre=f"Tarea {i}") for i in range(5)]

    for tarea in tareas:
        repositorio.guardar(tarea)

    ids = [t.id for t in tareas]
    assert len(set(ids)) == 5, f"IDs duplicados: {ids}"
    assert all(id > 0 for id in ids), f"IDs invÃ¡lidos: {ids}"
```
```

---

## âœ… Fase 3: Estrategia de Mocking

### CuÃ¡ndo mockear (regla de oro)

**Mockear SÃ** cuando:
- âœ… Es **lento** (>100ms por test)
- âœ… Es **no determinista** (random, time, red)
- âœ… Es **caro** (llamadas a APIs de pago)
- âœ… Es **no controlable** (servicios externos, prod DB)

**Mockear NO** cuando:
- âŒ Es **rÃ¡pido** (<10ms)
- âŒ Es **crÃ­tico** (core business logic)
- âŒ Es **fÃ¡cil de testear real** (in-memory DB, temp files)

### Tabla de decisiÃ³n

| Componente | Velocidad | Control | Mockear | Alternativa |
|------------|-----------|---------|---------|-------------|
| RepositorioMemoria | <1ms | Total | âŒ No | Testear real |
| RepositorioJSON (tmp_path) | ~10ms | Total | âŒ No | Testear real |
| RepositorioDB (SQLite :memory:) | ~50ms | Total | âŒ No | Testear real |
| API externa (Stripe) | ~500ms | Ninguno | âœ… SÃ­ | `responses` library |
| Email service (SendGrid) | ~300ms | Ninguno | âœ… SÃ­ | Mock SMTP |

---

### Paso 3.1: Validar estrategia de mocking con IA

**Prompt**:

```
Rol: Testing Architect

Revisa esta suite de tests y evalÃºa la estrategia de mocking:

CÃ³digo:
[Pegar tests]

Checklist:
1. Â¿Hay over-mocking? (componentes rÃ¡pidos que deberÃ­an testearse reales)
2. Â¿Hay under-mocking? (APIs externas testeadas reales que ralentizan CI)
3. Â¿Los mocks son frÃ¡giles? (acoplados a detalles de implementaciÃ³n)
4. Â¿Los tests con mocks siguen siendo valiosos? (o solo validan que se llama al mock)

Para cada problema, dame:
- LÃ­nea exacta del problema
- Por quÃ© es un problema (consecuencia)
- RefactorizaciÃ³n (cÃ³digo antes/despuÃ©s)
```

**Ejemplo de detecciÃ³n**:

```python
# âŒ OVER-MOCKING detectado
def test_servicio_llama_repo(mocker):
    mock_repo = mocker.Mock()
    servicio = ServicioTareas(mock_repo)

    servicio.crear("Test")

    mock_repo.guardar.assert_called_once()  # Solo valida llamada, no comportamiento

# IA te alertarÃ­a:
> **Problema**: Test solo valida que `guardar()` se llama, no que la tarea se guarda correctamente.
> Si `guardar()` tiene un bug que ignora la tarea, el test sigue pasando.
>
> **RefactorizaciÃ³n**: Usa RepositorioMemoria real (es rÃ¡pido)

# âœ… CORRECTO
def test_servicio_persiste_tarea_correctamente():
    repo = RepositorioMemoria()  # Real
    servicio = ServicioTareas(repo)

    tarea = servicio.crear("Test")

    tareas_guardadas = repo.listar()
    assert len(tareas_guardadas) == 1
    assert tareas_guardadas[0].nombre == "Test"
```

---

## ğŸ§ª Fase 4: ValidaciÃ³n de Calidad de Tests

### Paso 4.1: Evaluar calidad con Test Quality Reviewer

**Prompt**:

```
Rol: Test Quality Reviewer

EvalÃºa la calidad de estos tests:

CÃ³digo:
[Pegar tests]

Criterios (1-10 cada uno):
1. **Assertions significativas**: Â¿Validan comportamiento o solo existencia?
2. **Independencia**: Â¿Pueden correr en cualquier orden?
3. **Nombres descriptivos**: Â¿Explican QUÃ‰ testean?
4. **Edge cases**: Â¿Cubren casos lÃ­mite?
5. **Fragilidad**: Â¿Se rompen solo con bugs reales o con cualquier cambio interno?
6. **Velocidad**: Â¿Corren en <1 segundo?
7. **Legibilidad**: Â¿Un junior entiende quÃ© falla leyendo el nombre del test?

Formato:
| Test | Criterio 1 | Criterio 2 | ... | Total | Problemas |
|------|------------|------------|-----|-------|-----------|

Recomendaciones de mejora para tests con score <7
```

---

### Red Flags Comunes

#### 1. Assertions dÃ©biles

```python
# âŒ MAL (score: 2/10)
def test_crear_tarea():
    tarea = servicio.crear("Test")
    assert tarea  # Solo valida que no es None

# âœ… BIEN (score: 9/10)
def test_crear_tarea_asigna_id_positivo_y_nombre_correcto():
    tarea = servicio.crear("Test")
    assert tarea.id > 0, f"ID debe ser positivo, fue {tarea.id}"
    assert tarea.nombre == "Test", "Nombre debe coincidir"
    assert tarea.completada is False, "Nueva tarea debe estar pendiente"
```

#### 2. Tests dependientes (orden importa)

```python
# âŒ MAL (score: 1/10) - Tests acoplados
def test_1_crear():
    servicio.crear("Tarea 1")

def test_2_listar():
    tareas = servicio.listar()
    assert len(tareas) == 1  # Falla si test_1 no corriÃ³ antes

# âœ… BIEN (score: 10/10) - Tests independientes
def test_crear_y_listar_devuelve_tarea_creada():
    servicio = ServicioTareas(RepositorioMemoria())  # Repo limpio
    servicio.crear("Tarea 1")

    tareas = servicio.listar()
    assert len(tareas) == 1
```

#### 3. Tests frÃ¡giles (acoplados a implementaciÃ³n)

```python
# âŒ MAL (score: 3/10) - Testea detalle interno
def test_contador_incrementa():
    repo = RepositorioMemoria()
    repo.guardar(Tarea(id=0, nombre="Test"))

    assert repo._contador == 1  # Acoplado a variable privada

# âœ… BIEN (score: 9/10) - Testea contrato pÃºblico
def test_ids_son_secuenciales():
    repo = RepositorioMemoria()
    t1 = Tarea(id=0, nombre="Test1")
    t2 = Tarea(id=0, nombre="Test2")

    repo.guardar(t1)
    repo.guardar(t2)

    assert t2.id == t1.id + 1  # Testea comportamiento observable
```

---

## ğŸ“Š Fase 5: MediciÃ³n de Coverage

### Paso 5.1: Ejecutar pytest con coverage

```bash
pytest --cov=api --cov-report=term-missing --cov-fail-under=80 -v
```

**QuÃ© significa**:
- `--cov=api`: Medir coverage del directorio api/
- `--cov-report=term-missing`: Mostrar lÃ­neas NO cubiertas
- `--cov-fail-under=80`: Fallar si <80%
- `-v`: Verbose (ver cada test)

---

### Paso 5.2: Analizar gaps de cobertura con IA

**Prompt**:

```
Rol: Coverage Analyst

Contexto:
AquÃ­ estÃ¡ el reporte de coverage de pytest:

[Pegar output de pytest --cov --cov-report=term-missing]

Objetivo:
Analiza los gaps y recomienda quÃ© tests agregar

Formato:
## LÃ­neas sin cubrir
| Archivo | LÃ­neas | Componente | Criticidad (1-10) | Test sugerido |
|---------|--------|------------|-------------------|---------------|

## Ramas sin cubrir
[if/else, try/except que no se ejecutan]

## RecomendaciÃ³n
[QuÃ© testear primero para llegar a 80%+ con mÃ­nimo esfuerzo]
```

---

## ğŸš¨ Checklist Final

### Estrategia de Testing Completa

**AnÃ¡lisis previo**:
- [ ] Mapa de arquitectura generado (capas, flujos, riesgos)
- [ ] Top 10 edge cases priorizados por impacto
- [ ] Estrategia de mocking definida (quÃ© mockear, quÃ© no)

**Tests implementados**:
- [ ] Tests parametrizados para TODOS los repositorios
- [ ] Edge cases crÃ­ticos cubiertos (IDs Ãºnicos, nombres especiales, repo vacÃ­o)
- [ ] Tests de integraciÃ³n end-to-end (API â†’ Servicio â†’ Repo)

**Calidad de tests**:
- [ ] Assertions significativas (no solo `is not None`)
- [ ] Tests independientes (orden no importa)
- [ ] Nombres descriptivos (explican QUÃ‰ y CUÃNDO)
- [ ] Sin over-mocking (componentes rÃ¡pidos testeados reales)

**Coverage y performance**:
- [ ] >= 80% line coverage
- [ ] 100% de mÃ©todos pÃºblicos testeados
- [ ] Todos los tests corren en <10 segundos total

---

## ğŸ’¡ Tips para Usar IA Efectivamente

### 1. Prompts iterativos (no esperes perfecciÃ³n en primer intento)

```
# IteraciÃ³n 1: Estrategia general
â†’ IA: "Analiza arquitectura y genera estrategia de testing"

# IteraciÃ³n 2: Validar estrategia
â†’ IA: "Revisa esta estrategia, Â¿hay over-testing o under-testing?"

# IteraciÃ³n 3: Implementar
â†’ IA: "Genera tests para los 3 edge cases de mayor valor"

# IteraciÃ³n 4: Validar calidad
â†’ IA: "EvalÃºa estos tests con criterios de calidad"
```

### 2. Pregunta "Â¿Por quÃ©?" para aprender

```
"Â¿Por quÃ© este test necesita mock del repositorio?"
"Â¿Por quÃ© este edge case es mÃ¡s importante que este otro?"
"Â¿Por quÃ© usar fixture parametrizado en vez de pytest.mark.parametrize?"
```

### 3. Valida con mÃºltiples agentes

- **Python Best Practices Coach**: Sintaxis, fixtures, type hints
- **FastAPI Design Coach**: Tests de endpoints, status codes
- **Performance Optimizer**: Tests lentos (>1s)

Cada agente detecta problemas diferentes â†’ cobertura completa.

---

**Resumen**: Tests inteligentes > tests en cantidad. IA te ayuda a priorizar los tests que realmente atrapan bugs. ğŸ¯
