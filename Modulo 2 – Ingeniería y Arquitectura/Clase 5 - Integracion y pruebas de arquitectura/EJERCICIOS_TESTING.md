# Ejercicios Pr√°cticos: Testing de APIs con IA (Test Coverage Strategist)

## üéØ Objetivo

Dominar el uso de IA para **dise√±ar estrategias de testing inteligentes** que maximicen la detecci√≥n de bugs minimizando el esfuerzo.

Al completar estos ejercicios, sabr√°s:
- ‚úÖ Usar Test Coverage Strategist para identificar edge cases cr√≠ticos
- ‚úÖ Generar tests parametrizados con fixtures avanzados
- ‚úÖ Decidir cu√°ndo mockear y cu√°ndo testear real
- ‚úÖ Validar calidad de tests con agentes educacionales
- ‚úÖ Alcanzar >=80% coverage con tests de alto valor

---

## Ejercicio 1: Generar Estrategia de Testing con IA

### üìã Contexto

Tienes una API de tareas con 3 capas (API ‚Üí Servicio ‚Üí Repositorio). Necesitas una estrategia de testing completa que priorice los tests de mayor valor.

**Requisitos**:
- Identificar edge cases cr√≠ticos (top 10)
- Priorizar por ROI (valor / esfuerzo)
- Recomendar qu√© testear primero (quick wins)
- Detectar gaps de cobertura en c√≥digo actual

---

### ü§ñ Paso 1: An√°lisis de arquitectura

**Tu tarea**: Dise√±a el prompt para que IA analice la arquitectura y genere un mapa de riesgos.

**Plantilla sugerida**:

```
Rol: [¬øQu√© tipo de experto necesitas?]

Contexto:
[Describe la arquitectura: capas, componentes, dependencias]

C√≥digo actual:
[Pegar c√≥digo de api.py, servicio_tareas.py, repositorio_base.py]

Objetivo:
[Qu√© an√°lisis quieres: mapa de riesgos, puntos de integraci√≥n, dependencias externas]

Formato de respuesta:
```
Capa API
‚îú‚îÄ‚îÄ Endpoints
‚îú‚îÄ‚îÄ Validaciones
‚îî‚îÄ‚îÄ Puntos de riesgo

Capa Servicio
‚îú‚îÄ‚îÄ L√≥gica de negocio
‚îî‚îÄ‚îÄ Puntos de riesgo

Capa Repositorio
‚îú‚îÄ‚îÄ Implementaciones
‚îî‚îÄ‚îÄ Puntos de riesgo
```

Lista los **top 5 puntos de riesgo** (lugares m√°s propensos a bugs)
```

**Criterios de √©xito**:
- [ ] Prompt identifica las 3 capas correctamente
- [ ] Lista al menos 5 puntos de riesgo priorizados
- [ ] Explica POR QU√â cada punto es riesgoso
- [ ] Recomienda tipos de tests para cada riesgo

---

### ü§ñ Paso 2: Generar estrategia de testing con Test Coverage Strategist

**Prompt sugerido**:

```
Rol: Test Coverage Strategist experto en pytest y FastAPI

Contexto:
Arquitectura:
[Pegar el mapa del Paso 1]

C√≥digo actual:
[Pegar c√≥digo completo]

Objetivo:
Genera una ESTRATEGIA DE TESTING completa priorizando tests de alto valor

Formato:

## 1. Tests Unitarios (por capa)
| Componente | Edge cases cr√≠ticos | Valor (1-10) | Esfuerzo (1-10) | ROI |
|------------|---------------------|--------------|-----------------|-----|

## 2. Tests de Integraci√≥n
| Flujo completo | Edge cases | Mockear | Valor | Esfuerzo | ROI |
|----------------|------------|---------|-------|----------|-----|

## 3. Top 10 Edge Cases por Impacto
[Lista priorizada]

## 4. Gaps de Cobertura Actuales
[C√≥digo sin tests]

## 5. Recomendaci√≥n: ¬øQu√© testear primero?
[Quick wins: alto valor, bajo esfuerzo]

Restricciones:
- Prioriza por ROI (valor / esfuerzo)
- Identifica tests triviales que NO vale la pena escribir
- Recomienda orden de implementaci√≥n
```

**Criterios de √©xito**:
- [ ] Lista al menos 10 edge cases priorizados
- [ ] Calcula ROI para cada test
- [ ] Identifica al menos 3 "quick wins" (ROI >5)
- [ ] Detecta gaps de cobertura existentes

---

### üíæ Entregable

**Documento a crear**: `ESTRATEGIA_TESTING.md`

**Contenido m√≠nimo**:

```markdown
# Estrategia de Testing - API de Tareas

## 1. Mapa de Arquitectura y Riesgos
[Diagrama de capas con puntos de riesgo marcados]

## 2. Top 10 Edge Cases Priorizados
| # | Edge Case | Impacto | Probabilidad | Esfuerzo | ROI | Test sugerido |
|---|-----------|---------|--------------|----------|-----|---------------|
| 1 | IDs duplicados | CR√çTICO | MEDIA | BAJO | 10/10 | test_guardar_asigna_ids_unicos |
| 2 | Archivo JSON corrupto | ALTO | ALTA | MEDIO | 8/10 | test_listar_maneja_json_invalido |
| ... |

## 3. Plan de Implementaci√≥n
### Fase 1: Quick Wins (Semana 1)
- [ ] test_ids_unicos (ROI: 10)
- [ ] test_json_corrupto (ROI: 8)
- [ ] test_repo_vacio (ROI: 7)

### Fase 2: Tests de Integraci√≥n (Semana 2)
[... tests end-to-end]

### Fase 3: Edge Cases Avanzados (Semana 3)
[... casos raros pero importantes]

## 4. Gaps de Cobertura Detectados
‚ùå RepositorioJSON.listar() no maneja FileNotFoundError
‚ùå API no valida Content-Type
‚ùå ServicioTareas no valida longitud m√°xima de nombre

## 5. Decisiones de Mocking
| Componente | ¬øMockear? | Justificaci√≥n |
|------------|-----------|---------------|
| RepositorioMemoria | ‚ùå No | R√°pido (<1ms), testear real |
| RepositorioJSON | ‚ùå No | Usar tmp_path, testear I/O |
| API externa | ‚úÖ S√≠ | Lenta, cara, no controlamos |
```

---

## Ejercicio 2: Implementar Tests Parametrizados con Edge Cases

### üìã Contexto

Has identificado 3 edge cases cr√≠ticos:
1. **IDs duplicados**: Dos tareas no pueden tener el mismo ID
2. **Copia defensiva**: listar() no debe exponer referencias mutables
3. **Nombres con caracteres especiales**: ", ', \, <, > no deben romper la app

**Tu tarea**: Implementa tests parametrizados que validen estos edge cases en AMBOS repositorios (Memoria y JSON).

---

### ü§ñ Paso 1: Generar fixture parametrizado

**Prompt**:

```
Rol: Pytest Expert

Contexto:
Tengo 2 repositorios: RepositorioMemoria y RepositorioJSON
Ambos cumplen el Protocol: guardar(), listar()

Objetivo:
Genera fixture parametrizado que devuelva AMBOS repositorios

Requisitos:
- Usa @pytest.fixture(params=["memoria", "json"])
- Para JSON, usa tmp_path para archivos temporales
- Cleanup autom√°tico (archivos se borran despu√©s)
- Documentaci√≥n de por qu√© usar fixture vs pytest.mark.parametrize

Formato:
```python
import pytest
from pathlib import Path
# ... c√≥digo con docstrings completos
```

Explica:
- Ventajas de fixture parametrizado
- C√≥mo funciona tmp_path
- Por qu√© cada test corre 2 veces
```

---

### ü§ñ Paso 2: Implementar tests para cada edge case

Para cada uno de los 3 edge cases, genera un test.

**Ejemplo de prompt para Edge Case #1** (IDs √∫nicos):

```
Rol: Pytest Developer

Contexto:
Edge case: IDs duplicados rompen la arquitectura
Fixture disponible: repositorio (parametrizado, Memoria + JSON)
Modelo: Tarea(id, nombre, completada)

Objetivo:
Genera test que valide que NO hay IDs duplicados al guardar m√∫ltiples tareas

Requisitos:
- Nombre descriptivo: test_guardar_asigna_ids_unicos_a_multiples_tareas
- Docstring que explique POR QU√â este edge case es cr√≠tico
- Guarda al menos 5 tareas con id=0
- Assertions fuertes:
  * Todos los IDs son √∫nicos (usa set())
  * Todos los IDs son positivos
  * Los IDs son secuenciales
- Mensajes de error claros en assertions

Formato:
```python
def test_guardar_asigna_ids_unicos_a_multiples_tareas(repositorio):
    \"\"\"[Docstring explicativo]\"\"\"
    # ... c√≥digo
```
```

**Repite para Edge Case #2** (copia defensiva) y **Edge Case #3** (caracteres especiales).

---

### üíæ Entregable

**Archivo a crear**: `tests_integrations/test_edge_cases_parametrizados.py` (~150 l√≠neas)

**Contenido m√≠nimo**:

```python
import pytest
from pathlib import Path
from api.servicio_tareas import Tarea
from api.repositorio_memoria import RepositorioMemoria
from api.repositorio_json import RepositorioJSON


@pytest.fixture(params=["memoria", "json"])
def repositorio(request, tmp_path):
    """Fixture parametrizado con docstring completo."""
    # ... implementaci√≥n


def test_guardar_asigna_ids_unicos_a_multiples_tareas(repositorio):
    """Valida que no hay IDs duplicados (edge case cr√≠tico)."""
    # ... implementaci√≥n con assertions fuertes


def test_listar_devuelve_copia_defensiva(repositorio):
    """Valida que modificar lista devuelta NO afecta repo interno."""
    # ... implementaci√≥n


def test_guardar_con_nombre_caracteres_especiales(repositorio):
    """Valida que caracteres especiales no rompen persistencia."""
    # ... implementaci√≥n con nombre = "\", ', <, >, &"
```

**Validaci√≥n**:
```bash
pytest tests_integrations/test_edge_cases_parametrizados.py -v
```

**Resultado esperado**: 6 tests pasando (3 edge cases √ó 2 repos)

---

## Ejercicio 3: Validar Calidad de Tests con IA

### üìã Contexto

Has escrito una suite de tests, pero no est√°s seguro si son de alta calidad. Necesitas que IA eval√∫e la calidad y sugiera mejoras.

---

### ü§ñ Paso 1: Evaluar tests con Test Quality Reviewer

**Prompt**:

```
Rol: Test Quality Reviewer

Eval√∫a la calidad de estos tests usando 7 criterios:

C√≥digo de tests:
[Pegar tests completos]

Criterios (punt√∫a 1-10 cada uno):
1. **Assertions significativas**: ¬øValidan comportamiento o solo existencia?
2. **Independencia**: ¬øPueden correr en cualquier orden?
3. **Nombres descriptivos**: ¬øExplican QU√â testean?
4. **Edge cases**: ¬øCubren casos l√≠mite?
5. **Fragilidad**: ¬øSe rompen solo con bugs reales?
6. **Velocidad**: ¬øCorren en <1 segundo?
7. **Legibilidad**: ¬øUn junior entiende qu√© falla?

Formato:
| Test | C1 | C2 | C3 | C4 | C5 | C6 | C7 | Total | Problemas detectados |
|------|----|----|----|----|----|----|----| ------|----------------------|

Para tests con score <7/10:
- Lista problemas espec√≠ficos
- Prop√≥n correcciones (c√≥digo antes/despu√©s)
```

---

### ü§ñ Paso 2: Corregir tests seg√∫n recomendaciones de IA

Toma los tests con score <7 y refactor√≠zalos seg√∫n las recomendaciones.

**Ejemplo**:

```python
# ‚ùå ANTES (score: 4/10)
def test_crear():
    tarea = servicio.crear("Test")
    assert tarea  # Assertion d√©bil

# ‚úÖ DESPU√âS (score: 9/10)
def test_crear_tarea_asigna_id_positivo_y_estado_inicial():
    """Valida que nuevas tareas tienen ID v√°lido y estado pendiente."""
    tarea = servicio.crear("Test")

    assert tarea.id > 0, f"ID debe ser positivo, fue {tarea.id}"
    assert tarea.nombre == "Test", "Nombre debe coincidir con input"
    assert tarea.completada is False, "Nueva tarea debe estar pendiente"
```

---

### üíæ Entregable

**Documento a crear**: `REPORTE_CALIDAD_TESTS.md`

**Contenido**:

```markdown
# Reporte de Calidad de Tests

## Evaluaci√≥n Inicial

| Test | Assertions | Independencia | Nombres | Edge Cases | Fragilidad | Velocidad | Legibilidad | Total |
|------|------------|---------------|---------|------------|------------|-----------|-------------|-------|
| test_crear | 2 | 10 | 5 | 3 | 8 | 10 | 6 | 6.3/10 |
| test_listar | 3 | 10 | 7 | 5 | 9 | 10 | 8 | 7.4/10 |
| ... |

## Problemas Detectados

### Test #1: test_crear (6.3/10)
**Problemas**:
1. Assertion d√©bil (`assert tarea`) - Solo valida que no es None
2. Nombre poco descriptivo - No dice QU√â valida
3. No cubre edge cases (nombre vac√≠o, IDs negativos)

**Correcciones aplicadas**:
[C√≥digo antes/despu√©s]

### Test #2: ...
[... m√°s tests]

## Resultado Post-Correcci√≥n

| Test | Total Antes | Total Despu√©s | Mejora |
|------|-------------|---------------|--------|
| test_crear | 6.3 | 9.2 | +2.9 |
| ... |

**Promedio general**: 7.8/10 ‚Üí 9.1/10 (+1.3)
```

---

## üèÜ Criterios de √âxito General

Para considerar los 3 ejercicios completos:

### Ejercicio 1 (Estrategia de Testing)
- [ ] Identificaste al menos 10 edge cases cr√≠ticos
- [ ] Priorizaste por ROI (valor / esfuerzo)
- [ ] Tienes un plan de implementaci√≥n por fases
- [ ] Detectaste gaps de cobertura en c√≥digo actual

### Ejercicio 2 (Tests Parametrizados)
- [ ] Tests corren contra AMBOS repositorios (6 tests = 3 √ó 2 repos)
- [ ] Cada test valida un edge case cr√≠tico diferente
- [ ] Usa fixture parametrizado con tmp_path
- [ ] Todos los tests pasan en <1 segundo

### Ejercicio 3 (Calidad de Tests)
- [ ] Evaluaste al menos 5 tests con los 7 criterios
- [ ] Identificaste al menos 3 problemas de calidad
- [ ] Refactorizaste tests con score <7
- [ ] Promedio general >= 8/10 despu√©s de correcciones

---

## üìö Recursos Adicionales

**Agentes educacionales recomendados**:
- `.claude/agents/educational/python-best-practices-coach.md` ‚Üí Valida sintaxis, fixtures
- `.claude/agents/educational/fastapi-design-coach.md` ‚Üí Revisa tests de endpoints
- `.claude/agents/educational/performance-optimizer.md` ‚Üí Detecta tests lentos

**Documentaci√≥n**:
- pytest fixtures: https://docs.pytest.org/en/stable/fixture.html
- pytest parametrize: https://docs.pytest.org/en/stable/parametrize.html
- Coverage.py: https://coverage.readthedocs.io/

**Librer√≠as √∫tiles**:
- `pytest-cov`: Coverage con pytest
- `pytest-xdist`: Tests en paralelo
- `pytest-mock`: Mocking simplificado

---

## üí° Tips para Usar IA Efectivamente

### Prompts iterativos (refina hasta obtener calidad)

```
# Iteraci√≥n 1: Estrategia general
"Genera estrategia de testing para mi API"

# Iteraci√≥n 2: Refinar
"La estrategia menciona 'mockear repositorio'. ¬øPor qu√©? Es r√°pido (<1ms)"

# Iteraci√≥n 3: Corregir
"Genera la estrategia SIN mockear repositorios r√°pidos"

# Iteraci√≥n 4: Validar
"Eval√∫a esta estrategia con criterios de ROI y detecta over-testing"
```

### Pregunta "¬øPor qu√©?" para aprender

```
"¬øPor qu√© este edge case es m√°s cr√≠tico que este otro?"
"¬øPor qu√© usar fixture parametrizado en vez de pytest.mark.parametrize?"
"¬øPor qu√© este test tiene score bajo en 'Assertions'?"
```

### Valida con m√∫ltiples agentes

- **Test Coverage Strategist**: Qu√© testear
- **Python Best Practices Coach**: C√≥mo testear
- **Test Quality Reviewer**: Qu√© mejorar

---

**¬°√âxito con los ejercicios! Recuerda: tests inteligentes > tests en cantidad. üéØ**
