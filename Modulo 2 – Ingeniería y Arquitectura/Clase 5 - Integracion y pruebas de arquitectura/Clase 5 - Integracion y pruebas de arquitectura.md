# üé¨ Clase 5 - **Integraci√≥n y pruebas de arquitectura**.

Entramos en **la Clase 5 del M√≥dulo 2**, justo donde todo empieza a sentirse como un proyecto ‚Äúde verdad‚Äù.

Para situarnos:

- En la **Clase 1**, aprendiste c√≥mo planificar el trabajo (backlog, historias, sprints).
- En la **Clase 2**, aplicaste **TDD y SRP** en tu primer endpoint con FastAPI.
- En la **Clase 3**, separaste **API / servicio / repositorio** y diste forma a tu primera arquitectura limpia.
- En la **Clase 4**, aplicaste **Open/Closed y Dependency Inversion**: tu servicio ya no depende de detalles, sino de contratos.

Ahora llega **la Clase 5 ‚Äì Integraci√≥n y pruebas de arquitectura**.

El objetivo: **ver si toda esta estructura realmente se comporta como esper√°bamos**.

Aqu√≠ empieza el oficio de **ingeniero**, no solo programador.

Antes de empezar a meter m√°s contigo, PARATE UN SEGUNDO

Ahora mismo deberias tener un mapa mental de que hace cada archivo del proyecto, cada linea. Esto no significa que seas capaz de escribirla en un papel en blanco. Esto significa que si lees el codigo entiendes lo que esta ocurriendo.

Si esto no es as√≠, no avances mas porque se te va a empezar a hacer bola (si no se te ha hecho ya)

Abre tu IA de confianza (Ej. ChatGPT 5) y pasale el codigo que hicimos en la clase 1 y que te lo explique linea a linea. Preguntale todas las dudas que tengas y no avances hasta tenerlo claro.

Lo mismo con las clases 2, 3, 4. El objetivo es que antes de continuar entiendas todo y tengas el mapa mental. (*Tomate esto como una tutoria con el profesor, solo que este no se frustra al explicarte las cosas 1000 veces*)

Ahora si. Continuamos

## üéØ Concepto

Hasta ahora escrib√≠as tests de comportamiento (que el endpoint devuelva 201, que las tareas se guarden‚Ä¶).

Pero ¬øqu√© pasa si alguien cambia una parte interna ‚Äîpor ejemplo, sustituye el repositorio en memoria por uno JSON‚Äî y la API sigue respondiendo igual?

Esa es la prueba de fuego de la **arquitectura limpia**:

> ‚ÄúCambiar dentro sin romper fuera.‚Äù
> 

Hoy aprender√°s a escribir **tests de integraci√≥n** que crucen capas:

API ‚Üí Servicio ‚Üí Repositorio.

As√≠ comprobamos que los contratos entre capas est√°n bien definidos y que tu sistema es **coherente y extensible**.

---

## üß† Historia para entenderlo

Imagina que eres el jefe t√©cnico y tienes dos devs:

- Ana ha implementado `RepositorioMemoria`.
- Luis crea `RepositorioJSON`.

Ambos prometen que sus clases cumplen el mismo contrato (`guardar()` y `listar()`), pero ¬øc√≥mo lo verificas sin abrir el c√≥digo?

Con un **test de integraci√≥n**, que trate ambos repositorios como cajas negras.

Si ambos pasan los mismos tests, tu arquitectura est√° viva y saludable.

---

Pero antes, vamos a crear **`RepositorioJSON`**

Vamos a hacerlo juntos, sin prisas y sin romper nada.

### 1) ¬øQu√© necesitamos que haga?

Igual que `RepositorioMemoria`, debe cumplir el **contrato** (`guardar`, `listar`). Solo que, en vez de una lista en RAM, **lee/escribe un archivo `.json`**.

### 2) Crea el archivo

Ruta sugerida: `api/repositorio_json.py`

```python
# api/repositorio_json.py
from __future__ import annotations
import json, os
from typing import List
from api.servicio_tareas import Tarea  # usamos el mismo modelo Pydantic

class RepositorioJSON:
    """Repositorio que persiste tareas en un archivo JSON sencillo."""

    def __init__(self, ruta_archivo: str = "tareas.json"):
        self._ruta = ruta_archivo
        # si no existe, lo creamos vac√≠o
        if not os.path.exists(self._ruta):
            with open(self._ruta, "w", encoding="utf-8") as f:
                json.dump([], f, ensure_ascii=False, indent=2)

    def listar(self) -> List[Tarea]:
        with open(self._ruta, "r", encoding="utf-8") as f:
            contenido = f.read().strip()
            datos = json.loads(contenido) if contenido else []
        # devolvemos objetos Tarea (Pydantic), no dicts
        return [Tarea(**d) for d in datos]

    def guardar(self, tarea: Tarea) -> None:
        # leemos lo existente
        tareas = self.listar()
        # generamos ID robusto (max + 1)
        nuevo_id = (max((t.id for t in tareas), default=0) + 1)
        tarea.id = nuevo_id
        tareas.append(tarea)
        # guardamos todo
        with open(self._ruta, "w", encoding="utf-8") as f:
            json.dump([t.model_dump() for t in tareas], f, ensure_ascii=False, indent=2)

```

Puntos clave, en cristiano:

- Si el archivo **no existe**, lo creamos con `[]`.
- `listar()` **lee el JSON** y convierte cada item a `Tarea`.
- `guardar()` **calcula el siguiente id** (`max + 1`), asigna y **reescribe** el archivo completo (simple y suficiente ahora).

### 3) Usarlo en la API

En `api/api.py`, donde eliges repositorio:

```python
from api.repositorio_memoria import RepositorioMemoria
from api.repositorio_json import RepositorioJSON

# repositorio = RepositorioMemoria()
repositorio = RepositorioJSON("data/tareas.json")  # por ejemplo, dentro de /data
servicio = ServicioTareas(repositorio)
```

No cambias nada m√°s. Esa es la gracia de DIP: **una l√≠nea y listo**.

Simplemente cambiamos el repo que le damos al servicio y BOOM, ya lo tenemos funcionando en JSON sin escribir nada mas. Y los test pasando en verde.

### 5) Eliminar estado compartido

Vamos a modificar el test crear_tarea por:

```python
from fastapi.testclient import TestClient
from api import api as api_mod  # accedemos al m√≥dulo, no solo al app
from api.servicio_tareas import ServicioTareas
from api.repositorio_memoria import RepositorioMemoria

def test_crear_tarea_minima_devuelve_201_y_cuerpo_esperado():
    # 1. Resetear el servicio a uno limpio
    api_mod.servicio = ServicioTareas(RepositorioMemoria())
    # 2. Crear el cliente justo despu√©s
    cliente = TestClient(api_mod.app)

    respuesta_con_nombre = cliente.post("/tareas", json={"nombre": "Estudiar SOLID"})
    assert respuesta_con_nombre.status_code == 201
    cuerpo = respuesta_con_nombre.json()
    assert cuerpo["id"] == 1
    assert cuerpo["nombre"] == "Estudiar SOLID"
    assert cuerpo["completada"] is False

def test_crear_tarea_con_nombre_vacio_devuelve_422():
    api_mod.servicio = ServicioTareas(RepositorioMemoria())
    cliente = TestClient(api_mod.app)

    respuesta_con_nombre_vacio = cliente.post("/tareas", json={"nombre": ""})
    assert respuesta_con_nombre_vacio.status_code == 422

```

## üß† Traducci√≥n para humanos

- Ya **no hay `cliente_http` global** que se quede con el repo sucio.
- Cada test **crea un cliente despu√©s de inyectar un repo limpio**.
- El `id` siempre arranca en 1. La API es predecible.

### 5) Test r√°pido de integraci√≥n (para dormir tranquilo)

Crea (o a√±ade) un test muy peque√±o que pruebe memoria y json igual:

```python
# tests/test_crear_tarea_json.py
import tempfile, os
from fastapi.testclient import TestClient
from api import api as api_mod
from api.servicio_tareas import ServicioTareas
from api.repositorio_json import RepositorioJSON

def test_crear_tarea_con_repositorio_json_temporal():
    # Crear archivo temporal vac√≠o
    tmp = tempfile.NamedTemporaryFile(delete=False)
    tmp.close()

    try:
        # Inyectar el servicio con RepositorioJSON usando el archivo temporal
        api_mod.servicio = ServicioTareas(RepositorioJSON(tmp.name))

        # Lanzar cliente HTTP contra la API
        cliente = TestClient(api_mod.app)
        r = cliente.post("/tareas", json={"nombre": "Aprender tests con IA"})

        # Verificar respuesta
        assert r.status_code == 201
        tarea = r.json()
        assert tarea["id"] == 1
        assert tarea["nombre"] == "Aprender tests con IA"
        assert tarea["completada"] is False

    finally:
        # Borrar archivo despu√©s del test
        os.remove(tmp.name)

```

Si esto pasa en verde, est√°s listo: **misma historia, distinto almac√©n**.

Fijate bien en una cosa, porque es un error muy tipico:

- Ahora estamos creando un archivo temporal para el test y eliminandolo al terminar. Hay que tener cuidado de no escribir el json real en los test, y la IA mete mucho la gamba en esto.

---

## ü§ñ Aplicaci√≥n con IA

Prompt ejemplo:

```
Rol: QA Engineer Python.

Contexto: Tengo una API con ServicioTareas y varios repositorios (memoria, JSON).
Objetivo: Genera tests de integraci√≥n que verifiquen que todas las implementaciones del repositorio cumplen el mismo contrato.

Formato: C√≥digo pytest limpio y explicaciones breves.

Restricciones: No modificar el c√≥digo de producci√≥n.
```

La IA puede proponerte una **prueba parametrizada** con `pytest.mark.parametrize`, o incluso generar fixtures autom√°ticas.

Pero el punto es que t√∫ entiendas *qu√© est√° comprobando*: consistencia de comportamiento entre implementaciones.

Es importante que compruebes el flujo que est√° siguiendo la IA, te ir√°s dando cuenta de los t√≠picos errores que comete y estar√°s preparado para evitarlos.

---

## üß© Ejercicio pr√°ctico

1. Rama: `feature/test-integracion-repositorios`.
2. A√±ade los tests gen√©ricos como el ejemplo.
3. Corre toda la suite (`pytest -v`).
4. Documenta en `notes.md`:
    - Qu√© descubriste sobre tu arquitectura.
    - Qu√© romper√≠a si cambiaras el contrato del repositorio.
    - Qu√© significa ‚Äúextensible pero estable‚Äù en tu c√≥digo.

---

## ü§ñ Workflow IA avanzado: Test Coverage Strategist

Hasta ahora has escrito tests manualmente. Ahora vamos a subir el nivel: **usar IA como estratega de cobertura de tests**.

La diferencia entre un junior que usa IA y un senior:

- **Junior**: "IA, genera tests para mi c√≥digo" ‚Üí copia/pega sin saber qu√© se est√° testeando
- **Senior**: "IA, analiza mi arquitectura y lista los edge cases cr√≠ticos que DEBO testear. Genera solo tests de alto valor"

Vamos a ser seniors.

---

### 6.1. Por qu√© IA es poderosa para testing

**Ventajas**:
1. **Identifica edge cases que no viste** - Inputs raros, estados inconsistentes
2. **Genera fixtures parametrizados** - Reduce duplicaci√≥n de tests
3. **Propone estrategia de mocking** - Qu√© mockear y qu√© no
4. **Detecta gaps de cobertura** - M√©todos sin tests, ramas sin cubrir
5. **Valida calidad de tests** - Tests fr√°giles, over-mocking, assertions d√©biles

**Riesgos si no la usas con criterio**:
- ‚ùå Tests que solo validan "happy path" (caso exitoso)
- ‚ùå Over-mocking (mockeas TODO, no testeas integraci√≥n real)
- ‚ùå Tests fr√°giles (cambia 1 l√≠nea de c√≥digo, rompen 50 tests)
- ‚ùå Cobertura alta pero sin valor (100% coverage, 0% de bugs atrapados)

Por eso necesitas **Test Coverage Strategist** + validaci√≥n con agentes.

---

### 6.2. Fase 1: An√°lisis de arquitectura con IA (Test Coverage Strategist)

**Escenario**: Tienes una API con 3 capas (API ‚Üí Servicio ‚Üí Repositorio). ¬øQu√© tests necesitas REALMENTE?

**Prompt estructurado**:

```
Rol: Test Coverage Strategist experto en pytest y FastAPI

Contexto:
Tengo esta arquitectura:
- API (api.py): Endpoints POST /tareas, GET /tareas
- Servicio (servicio_tareas.py): L√≥gica de negocio (crear, listar)
- Repositorio (Protocol con 2 implementaciones: RepositorioMemoria, RepositorioJSON)

C√≥digo actual:
[Pegar api.py, servicio_tareas.py, repositorio_base.py]

Objetivo:
Analiza la arquitectura y genera una ESTRATEGIA DE TESTING completa

Formato de respuesta:
## 1. Tests Unitarios (capa por capa)
| Componente | Qu√© testear | Por qu√© es cr√≠tico | Qu√© NO testear |
|------------|-------------|---------------------|----------------|

## 2. Tests de Integraci√≥n
| Flujo | Capas involucradas | Edge cases | Mocks necesarios |
|-------|-------------------|------------|------------------|

## 3. Edge Cases Cr√≠ticos
[Lista de 10+ edge cases que DEBES cubrir]

## 4. Gaps de Cobertura Detectados
[C√≥digo sin tests, ramas no cubiertas]

Restricciones:
- NO generes c√≥digo todav√≠a, solo la estrategia
- Prioriza tests de alto valor (bugs reales, no triviales)
- Identifica qu√© mockear y qu√© testear end-to-end
```

**Resultado esperado** (parcial):

```markdown
## 1. Tests Unitarios

| Componente | Qu√© testear | Por qu√© es cr√≠tico | Qu√© NO testear |
|------------|-------------|---------------------|----------------|
| ServicioTareas.crear() | IDs autoincrementales, nombre vac√≠o (validaci√≥n Pydantic) | Es el core de la l√≥gica de negocio | La persistencia (eso es responsabilidad del repo) |
| RepositorioMemoria | IDs √∫nicos, listar devuelve copias (no refs mutables) | Bugs comunes: IDs duplicados, mutaci√≥n de estado | La API (separation of concerns) |

## 2. Tests de Integraci√≥n

| Flujo | Capas involucradas | Edge cases | Mocks necesarios |
|-------|-------------------|------------|------------------|
| POST /tareas ‚Üí guardar | API + Servicio + Repo | Nombre con 1000 caracteres, emojis, caracteres especiales (", ', \) | **NINGUNO** (end-to-end real) |
| GET /tareas con repo vac√≠o | API + Servicio + Repo | Lista vac√≠a debe devolver 200 (no 404) | Ninguno |

## 3. Edge Cases Cr√≠ticos

1. **IDs autoincrementales con repositorios vac√≠os**: ¬øQu√© pasa si listar() devuelve []?
2. **Nombres con caracteres especiales**: ", ', \, <, >, SQL injection attempts
3. **Repositorio JSON corrupto**: Archivo existe pero JSON inv√°lido
4. **Archivo JSON con permisos de solo lectura**: ¬øFalla guardar()?
5. **M√∫ltiples tareas con mismo ID**: ¬øC√≥mo reacciona el servicio?
6. **Tarea con id=None vs id=0**: ¬øSe comportan igual?
7. **Cambiar de RepositorioMemoria a RepositorioJSON en runtime**: ¬øLos IDs se mantienen consistentes?
8. **POST /tareas sin body**: ¬øDevuelve 422 o 500?
9. **GET /tareas cuando hay 1000+ tareas**: ¬øHay timeout? ¬øPagination?
10. **Crear tarea, eliminar archivo JSON manualmente, listar**: ¬øDevuelve [] o error?

## 4. Gaps de Cobertura Detectados

‚ùå **RepositorioJSON.listar()**: No tiene tests para archivo corrupto
‚ùå **ServicioTareas.crear()**: No valida longitud m√°xima de nombre
‚ùå **API**: No hay test para Content-Type incorrecto (text/plain en vez de JSON)
```

**C√≥mo usar este an√°lisis**:
1. Prioriza edge cases por impacto (IDs duplicados > nombre largo)
2. Genera tests SOLO para los casos de alto valor
3. Valida con agente que no est√©s over-testing

---

### 6.3. Fase 2: Generar tests parametrizados con IA

**Objetivo**: Crear tests que corran contra M√öLTIPLES repositorios (Memoria, JSON).

**Prompt**:

```
Rol: Pytest Expert

Contexto:
Tengo 2 implementaciones del repositorio: RepositorioMemoria, RepositorioJSON
Ambos cumplen el mismo Protocol: guardar(), listar()

Objetivo:
Genera tests parametrizados con @pytest.fixture(params=...) que validen
que AMBOS repositorios son intercambiables

Requisitos:
- Usa pytest.fixture parametrizado (no pytest.mark.parametrize)
- Cada test debe correr 2 veces (una por repositorio)
- Manejo de cleanup (borrar archivos JSON temporales)
- Type hints completos
- Docstrings que expliquen QU√â se valida y POR QU√â

Tests a generar:
1. test_guardar_asigna_ids_unicos
2. test_listar_devuelve_copia_defensiva (no modifica repo interno)
3. test_guardar_con_nombre_caracteres_especiales
4. test_listar_repositorio_vacio_devuelve_lista_vacia

Restricciones:
- NO uses mocks (son tests de integraci√≥n reales)
- Usa tmp_path de pytest para archivos temporales
- Cada test debe ser independiente (no estado compartido)

Formato:
```python
# tests_integrations/test_repositorios_parametrizados.py
import pytest
# ... c√≥digo
```

Explica:
- Por qu√© usar fixture parametrizado vs pytest.mark.parametrize
- C√≥mo garantizar aislamiento entre tests
```

**Resultado esperado** (simplificado):

```python
# tests_integrations/test_repositorios_parametrizados.py
import pytest
from pathlib import Path
from api.servicio_tareas import Tarea
from api.repositorio_memoria import RepositorioMemoria
from api.repositorio_json import RepositorioJSON


@pytest.fixture(params=["memoria", "json"])
def repositorio(request, tmp_path):
    """Fixture parametrizado que crea cada tipo de repositorio.

    Ventajas de fixture parametrizado vs pytest.mark.parametrize:
    - Permite setup/teardown espec√≠fico por implementaci√≥n
    - M√°s flexible para manejar archivos temporales
    - F√°cil agregar nuevos repositorios (solo a√±adir a params)
    """
    if request.param == "memoria":
        return RepositorioMemoria()
    elif request.param == "json":
        # Archivo temporal √∫nico por test (evita colisiones)
        archivo_test = tmp_path / f"tareas_{request.node.name}.json"
        return RepositorioJSON(str(archivo_test))


def test_guardar_asigna_ids_unicos(repositorio):
    """Valida que cada tarea recibe un ID √∫nico e incremental.

    Edge case cr√≠tico: IDs duplicados rompen la arquitectura.
    """
    tarea1 = Tarea(id=0, nombre="Primera", completada=False)
    tarea2 = Tarea(id=0, nombre="Segunda", completada=False)

    repositorio.guardar(tarea1)
    repositorio.guardar(tarea2)

    assert tarea1.id != tarea2.id, "IDs duplicados detectados"
    assert tarea1.id > 0 and tarea2.id > 0, "IDs deben ser positivos"


def test_listar_devuelve_copia_defensiva(repositorio):
    """Valida que listar() no expone referencias mutables internas.

    Bug com√∫n: modificar lista devuelta afecta el repo interno.
    """
    tarea = Tarea(id=0, nombre="Original", completada=False)
    repositorio.guardar(tarea)

    lista1 = repositorio.listar()
    lista1[0].nombre = "MODIFICADO"  # Intentar mutar

    lista2 = repositorio.listar()
    assert lista2[0].nombre == "Original", "El repo interno fue mutado (falla copia defensiva)"
```

---

### 6.4. Fase 3: Mocking estrat√©gico con IA

**Cu√°ndo mockear vs testear real**:

| Escenario | Mockear | Testear Real |
|-----------|---------|--------------|
| API llama Servicio llama Repo | ‚ùå No (test de integraci√≥n) | ‚úÖ S√≠ (end-to-end) |
| Llamadas a APIs externas (Stripe, AWS) | ‚úÖ S√≠ (lento, caro) | ‚ùå No (en tests unitarios) |
| Base de datos en producci√≥n | ‚úÖ S√≠ (no tocar prod) | ‚ùå No |
| Repositorio con BD SQLite en memoria | ‚ùå No (fast enough) | ‚úÖ S√≠ (integraci√≥n) |

**Prompt para validar estrategia de mocking**:

```
Rol: Testing Architect

Revisa estos tests y valida la estrategia de mocking:

C√≥digo de tests:
[Pegar tests]

Checklist:
1. ¬øHay over-mocking? (mockear cosas que deber√≠an testearse reales)
2. ¬øHay under-mocking? (testear APIs externas reales en CI)
3. ¬øLos mocks son fr√°giles? (acoplados a implementaci√≥n interna)
4. ¬øLos tests siguen siendo valiosos con mocks? (o solo validan el mock)

Para cada problema:
- Explica POR QU√â es un problema
- Prop√≥n la correcci√≥n (m√°s mocking / menos mocking)
- Muestra c√≥digo antes/despu√©s
```

**Ejemplo de violaci√≥n que IA detectar√≠a**:

```python
# ‚ùå OVER-MOCKING: Mock innecesario
def test_crear_tarea_llama_repositorio(mocker):
    mock_repo = mocker.Mock()  # Mock del repositorio
    servicio = ServicioTareas(mock_repo)

    servicio.crear("Test")

    mock_repo.guardar.assert_called_once()  # Solo valida que se llama, no QU√â hace

# ‚úÖ MEJOR: Test de integraci√≥n real
def test_crear_tarea_persiste_correctamente():
    repo = RepositorioMemoria()  # Repo real (es r√°pido)
    servicio = ServicioTareas(repo)

    tarea = servicio.crear("Test")

    tareas = repo.listar()
    assert len(tareas) == 1
    assert tareas[0].nombre == "Test"  # Valida comportamiento real
```

**IA te alertar√≠a**:
> ‚ùå **Over-mocking detectado**
>
> **Problema**: Test solo valida que `guardar()` se llama, NO valida que la tarea se persiste correctamente.
> Si cambias la implementaci√≥n de `guardar()` para que NO haga nada, el test sigue pasando.
>
> **Soluci√≥n**: Usa repositorio real (RepositorioMemoria es r√°pido, no necesita mock)

---

### 6.5. Fase 4: Validar calidad de tests con agentes

**Prompt de validaci√≥n**:

```
Rol: Test Quality Reviewer

Revisa estos tests y eval√∫a su calidad:

C√≥digo:
[Pegar tests]

Checklist de calidad:
1. **Assertions significativas**: ¬øValidan comportamiento o solo existencia?
2. **Independencia**: ¬øCada test puede correr solo sin orden?
3. **Nombres descriptivos**: ¬øEl nombre explica QU√â se testea y el caso?
4. **Edge cases**: ¬øCubre casos l√≠mite o solo happy path?
5. **Fragilidad**: ¬øSe rompen con cambios internos o solo con bugs reales?
6. **Velocidad**: ¬øCorren en <1 segundo?
7. **Legibilidad**: ¬øOtro dev entiende qu√© falla al leer el test?

Eval√∫a cada test con una puntuaci√≥n de 1-10 y justifica.
Lista problemas espec√≠ficos con correcciones.
```

**Red flags comunes**:

```python
# ‚ùå RED FLAG 1: Assertion d√©bil
def test_crear_tarea():
    tarea = servicio.crear("Test")
    assert tarea is not None  # MAL: Solo valida que devuelve algo

# ‚úÖ CORRECTO: Assertion fuerte
def test_crear_tarea_asigna_id_y_estado_inicial():
    tarea = servicio.crear("Test")
    assert tarea.id > 0, "ID debe ser positivo"
    assert tarea.nombre == "Test", "Nombre debe coincidir"
    assert tarea.completada is False, "Nueva tarea debe estar incompleta"


# ‚ùå RED FLAG 2: Tests dependientes (orden importa)
def test_1_crear_tarea():
    servicio.crear("Tarea 1")

def test_2_listar_devuelve_una():
    tareas = servicio.listar()
    assert len(tareas) == 1  # MAL: Depende de test_1

# ‚úÖ CORRECTO: Tests independientes
def test_listar_despues_de_crear():
    servicio = ServicioTareas(RepositorioMemoria())  # Repo limpio
    servicio.crear("Tarea 1")

    tareas = servicio.listar()
    assert len(tareas) == 1  # BIEN: Autocontenido


# ‚ùå RED FLAG 3: Test fr√°gil (acoplado a implementaci√≥n)
def test_crear_incrementa_contador_interno():
    repo = RepositorioMemoria()
    repo.guardar(Tarea(id=0, nombre="Test", completada=False))

    assert repo._contador == 1  # MAL: Testea detalle de implementaci√≥n

# ‚úÖ CORRECTO: Test de comportamiento
def test_guardar_asigna_ids_incrementales():
    repo = RepositorioMemoria()
    tarea1 = Tarea(id=0, nombre="Test1", completada=False)
    tarea2 = Tarea(id=0, nombre="Test2", completada=False)

    repo.guardar(tarea1)
    repo.guardar(tarea2)

    assert tarea2.id == tarea1.id + 1  # BIEN: Testea contrato p√∫blico
```

---

### 6.6. Checklist de cobertura estrat√©gica con IA

**Antes de dar por terminados los tests, valida**:

**Tests unitarios** (capa por capa):
- [ ] ServicioTareas tiene tests para cada m√©todo p√∫blico
- [ ] Edge cases de validaci√≥n (nombre vac√≠o, IDs negativos)
- [ ] Errores de negocio (tarea no encontrada)

**Tests de integraci√≥n** (flujo completo):
- [ ] Cada endpoint tiene test end-to-end (API ‚Üí Servicio ‚Üí Repo)
- [ ] Tests parametrizados para TODOS los repositorios
- [ ] Archivos temporales se limpian despu√©s de cada test

**Edge cases cr√≠ticos**:
- [ ] Repositorio vac√≠o (primera tarea tiene id=1)
- [ ] Nombres con caracteres especiales (", ', \, <, >)
- [ ] Archivo JSON corrupto (listar devuelve [])
- [ ] IDs autoincrementales son √∫nicos

**Calidad de tests**:
- [ ] Assertions significativas (no solo `is not None`)
- [ ] Tests independientes (sin orden requerido)
- [ ] Nombres descriptivos (test_crear_tarea_asigna_id_positivo)
- [ ] Sin over-mocking (usar repos reales cuando sea r√°pido)

**Coverage m√≠nimo**:
- [ ] 80%+ de line coverage (`pytest --cov`)
- [ ] 100% de m√©todos p√∫blicos testeados
- [ ] 0 ramas cr√≠ticas sin cubrir

---

### 6.7. Agentes educacionales recomendados

Para esta clase, usa estos agentes:

1. **Python Best Practices Coach**: Valida sintaxis de tests, fixtures, parametrizaci√≥n
2. **FastAPI Design Coach**: Revisa tests de endpoints (status codes, JSON, validaci√≥n)
3. **Performance Optimizer**: Detecta tests lentos (>1 segundo)

**Workflow recomendado**:
1. Genera estrategia de testing con Test Coverage Strategist
2. Implementa tests parametrizados
3. Valida con Python Best Practices Coach
4. Revisa mocking con Testing Architect
5. Eval√∫a calidad de tests con Test Quality Reviewer

**Ver documento completo**: `AI_TESTING_WORKFLOW.md` (workflow detallado paso a paso)

---

### 6.8. Ejercicio pr√°ctico con IA

**Desaf√≠o**: Genera tests de integraci√≥n completos para la API de tareas

**Paso 1**: Usa Test Coverage Strategist para analizar gaps de cobertura

**Paso 2**: Genera 10+ tests parametrizados que cubran:
- Happy path (crear, listar)
- Edge cases (nombres especiales, repo vac√≠o)
- Error cases (nombre vac√≠o ‚Üí 422)

**Paso 3**: Valida con agentes que:
- No hay over-mocking
- Assertions son fuertes
- Tests son independientes
- Coverage >= 80%

**Ver ejercicios completos**: `EJERCICIOS_TESTING.md`

---

## ‚úÖ Checklist final

- [ ]  Entiendes la diferencia entre test unitario e integraci√≥n.
- [ ]  Has comprobado que distintos repositorios se comportan igual.
- [ ]  Tu arquitectura puede cambiar internamente sin romper los contratos.
- [ ]  Has documentado la reflexi√≥n en `notes.md`.
- [ ]  **[NUEVO]** Usas Test Coverage Strategist para identificar edge cases cr√≠ticos
- [ ]  **[NUEVO]** Generas tests parametrizados con fixtures
- [ ]  **[NUEVO]** Validas estrategia de mocking (cu√°ndo mockear vs testear real)
- [ ]  **[NUEVO]** Tienes >= 80% coverage con tests de alto valor

---

En la pr√≥xima clase daremos un paso m√°s: a√±adiremos **repositorios externos y adaptadores (JSON/DB)** y hablaremos de c√≥mo preparar tu proyecto para **tests de aceptaci√≥n y CI/CD**.

Y recuerda: **cobertura alta sin valor es peor que cobertura baja con tests inteligentes**. La IA te ayuda a priorizar los tests que realmente atrapan bugs. üéØ